Source: https://www.youtube.com/watch?v=EuQjiNrK77M
Transcribed: 2025-12-30 18:06:53
Method: YouTube Transcript API
==================================================

hi everyone and welcome to the session titled open problems in mechanistic interpretability a worldwind tour with Neil nander I'm Kim and I'll be your MC for the session f Neil's presentation we'll move on to a live Q&A session where he'll respond to your questions you can submit questions through swap card's live discussion feature and upload the questions you'd like me to ask Neil then after 45 minutes we'll bring the session to an end uh but now I'd like to introduce our speaker for the session uh Neil nander runs the mechanistic interpretability team at Google deep mind he previously was an independent researcher and worked at anthropic his site interests include mentoring too many projects Moonlighting as a mechanistic interpretability Twitter and YouTube influencer and blogging and on that note I'd like to add that if you're interested in learning about or staying up toate with mechanistic interpretability stuff and Cutting Edge AI in general I'd 10 out of 10 recommend following Neil's Twitter and with that I'd like to hand over to Neil uh awesome thanks LW for the intro uh I'll take it as a sign that I spend too much time on Twitter uh all right so I'm going to be giving a Whirlwind overview of what's up in mechanistic interpretability uh these slides will have a lot of links which you can find at the link this link here which if someone could write into the swap card chat that would be useful uh some other useful links are a sequence I wrote called 200 concrete open problems in mechanistic interruptibility a guide for how to get started in the field and a library with a bunch of accompanying tutorials called Transformer on how to actually go and do mechanistic inability but all right first things first what even is mechanistic interpretability so mechanistic interpretability is a subfield of AI interpretability and AI alignment based on this hypothesis the models learn human comprehensible algorithms that models can be understood but they are not understandable by default and that we need to learn how to make them legible uh it's a careful balance of trying to achieve rigorous understanding of what's going on inside these systems without tricking ourselves with overly convenient and elegant stories and um there's an analogy here between how you might try to reverse engineer a trained neural network and how you might reverse engineer a compiled program binary down to the original source code from which it was compiled um and the people with some familiarity with ML this might seem like a pretty wild claim uh the mechanistic understanding is actually possible uh the way deep learning notoriously works is you randomly generate Millions to billions of parameters you give the model a massive pile of data and then you jiggle the parameters to like make the model's performance a little bit better on that data you keep doing this a lot of times and at the end you end up with a mysterious black box that performs well on the data but nobody knows how it works and conventional wisdom is that trying to understand how it works is basically doomed and I want to begin by going through a case study of a paper I wrote on understanding grocking via mechanistic inability to try to convince you that mechanistic understanding is possible uh this talk is going to be split up into a series of case studies if you zone out or join late or whatever you can just start paying attention again whenever the next case study is finished all right so this Genesis of this paper was this mystery why do models Gro grocking was this phenomena observed in power and out where if you take certain small models on certain algorithmic tasks uh and you give them such as modular Edition and you give them some fraction of the data to train on they will initially memorize that training data shown by achieving perfect train accuracy and terrible test accuracy but if you keep training it again and again on the same data it will abruptly learn to generaliz it will abruptly go from terrible test performance to really good test performance and I was like that seems like an interesting question why does that happen and can we use mechanistic interpretability to better understand why that happens the setup here was a one layer Transformer trained to do modular Edition the inputs there were three separate inputs Transformers take as input a sequence of of inputs or a sequence of vectors uh corresponding to the two inputs to the addition X and Y which are then looked which are basically entries in A3 by 128 lookup table 113 is the number of entries because we're doing model 113 128 is the size of the models embedding and a final special token corresponding to an equal sign that is always the same there are four attention heads which are bits of the model that move information from the X and Y positions to the equal sign position an MLP lay which does some nonlinear processing in place on the equal sign and then a linear map to the output followed by a softmax converting 113 dimensional output Vector to a probability distribution over the 113 possible outputs and it turns out that this architecture is capable of a grocking modular addition and I tried to understand what Earth was going on the first hint what was going on is the embedding this is the lookup table that Maps the 113 possible inputs to different embedding vectors and there were a bunch of hints in the literature there was some some kind of cute circle like structure going on uh by default these are 128 dimensional vectors and prayer work could use different techniques to convert these to two Dimensions like tne and principal component analysis and things sure looked pretty circly uh it also turns out that if you look inside the model and plot activations things look really periodic like here's a neural activation uh here's an attention pattern uh or specifically here is an element of an attention pattern repeated over all pairs of possible inputs it's like what what there is so much structure what is going on and it turns out a key insight to getting traction here was applying a technique from maths and Signal processing called a furier transform specifically a discreet fuer transform which takes periodic things and decomposes them into SS and cosiness of different frequencies and it turns out that if you apply a furi transform to the embedding and then look at the size of each s and cosine component most are almost zero and then there's five to six frequencies that are really big this is really weird and really sparse for contrast the randomly initialized model looks completely meaningless when you to a FIA transform because there's no inherent association with frequenc and this was the first big hint about what was going on it was doing something involving signs and cosines of different frequencies skipping ahead of it uh it turns out the algorithm that was Learned was simple enough it could fit on one slide and I didn't go in expecting this algorithm I found this by staring it and reverse engineering the weights so what's going on what essentially happened is the model realized that modular addition essentially clock arithmetic is equivalent to composing rotations around the unit circle you can think of your inputs A and B as rotations uh a * Theta and B * Theta you can compose them uh which adds the two angles and to extract the answer you can rotate backwards by every possible answer C and uh project onto the xaxis uh this the projection is maximized when this is the identity rotation rotate by zero IE C is equal to a plus b and because it's going around a circle you get modularity for free and importantly uh this is not just some theoretical algorithm I've come up with we can actually go inside the model and see that different steps of it clearly correspond to this algorithm uh and we presented four lines of evidence in the paper uh first I've already covered just everywhere you look is really periodic secondly mechanistic evidence uh there are certain points where if you just look at the model weights and multiply out the matrices they literally implement the step that you would predict uh from the algorithm uh we showed that neurons cluster into different frequencies and can be well approximated with signs and cosines despite this being a rank six approximation to a 12,000 dimensional vector and finally we verified that if you ablate everything our algorithm says should not matter uh performance actually improves while if you ablate any of the five key frequencies performance significantly degrades all right so that's the algorithm learned but what up with groing so now we understand what's going on we can actually look inside the model as it trains and what we see is that training breaks up into three distinct stages memorization circuit formation and cleanup uh so initially the model just memorizes train performance is really good the red line test performance is pretty bad the blue line but then during this long seeming Plateau the embedding is actually getting sparer and sparer and we call this circuit formation the model is gradually transitioning from purely doing the task with memorization to purely doing the task with generalization uh this um and uh the embedding is becoming sparer and sparer as it learns the true solution why does test performance seem to Plateau because uh memorization generalizes really badly and the sum of a badly generalizing and a well generalizing solution generalizes poorly it is only when the model gets so good at generalizing that it no longer needs to memorize and can clean up that we see this sudden drop in the test performance groing is thus an illusion it is not that the model uh suddenly generalizes it is that the model gradually generalizes followed by sudden cleanup when it's got so good at generalizing it no longer needs to memorize in the first place and we show this more rigorously in the paper using a series of quantitative progress measures that allow us to disentangle these two algorithms generalizing and memorizing uh and you can go check out the paper if you want to learn more uh but yeah this was our first case study on progress measures for groing Via mechanistic interruptibility and the main takeaway I want you to have from it is that a deep and mechanistic understanding of neural networks is possible and further that a mechanistic understanding can be crucial to resolve mysteries of deep plooding that interruptibility if we get it right and it is really hard to get right I don't want to be overly optimistic here the mechanistic understanding can be really important all right so with that hopefully motivating pitch out of the way I want to dig a bit more into what exactly is mechanistic inability the first thing that I think is useful to cover is what is a Transformer uh Transformers are the most common architecture used in modern zling that what's behind things like chat GPT llama Etc and in a very high level the way it works is that Transformers map sequences of words to sequences of probability distributions over the next word uh where it's set up internally so that information can only move uh forwards not backwards so that say after the 17th word the model outputs a probability distribution over what the 18th word should be after the 18th word it outputs a probability distribution over what the 19th word should be Etc and because information could only move further down the sequence it can't just cheat and look at what the next word is and uh these model you can talk to these models and these models can generate text by just repeatedly sing from this distribution over what the final word will be you take a prediction of what the next word will be you append it to the input you rerun the model uh for various reasons it's a lot more efficient to rerun the model uh because you can just reuse the intermediate activations from the prefix and then you you sample some new output and it just keeps going uh so yeah that's what a blackbox Transformer does sequence is a words to probability distributions internally uh The Intern representations are also a sequence every word uh really that tokens which are kind of subwords but you can just think of them as words for now uh has a vector after each layer that we call the residual stream uh after each layer of the model there is an incremental update to this residual stream it's kind of an ongoing estimate of what's going on with that word plus the surrounding context uh each layer's input is the current residual stream and each layer's output adds directly to the residual stream uh there are two types of sublayer within each layer the first are attention uh the job of attention is to move information between words it's made up of several attention heads that act independently and in parallel to each other uh often a thing we do in mechanistic interpretability is try to interpret these heads and the attention heads try to figure out what information the current word needs from the prior context which prior words have relevant information and then extract some information from there and bring it to the current word uh the second type is MLP layers which stands for multi-layer perceptual and the role of an MLP is to process information once it has been moved to the word to do a bunch of nonlinear processing and produce new and more complicated features and if you're interested to learn more you can see some tutorials I made on what is a Transformer and how to implement your own from scratch I currently have a bit of cold so sorry if I'm becoming a bit nasal uh worldwind tour what is a Transformer hopefully that gives a bit of useful context it wants to come but it's not a big deal if you didn't follow that in too much detail uh all right I discussed this somewh before uh mechanistic interruptibility is a field based on this core hypothesis that models learn human comprehensible algorithms and these algorithms can be understood if we can learn how to make them legible uh this in practice often breaks down into two substeps understanding the features in inside the model uh the variables the properties of the input as it's extracted and understanding circuits the algorithms the model has learned to compute these features uh here we have a great microcosm of the circuit from inside an image classification model uh the model was trained to among other things classified cars and it has neurons corresponding to car windows car and car wheels which correspond to these individual features these are properties of the input like this bit of the image has a window this bit of the image has a car body Etc these are connected up with weights to produce a urine in the next layer uh red means uh this is good uh blue means this is negative so this is saying if there's windows at the top car body at the bottom car wheels at the bottom and not car wheels at the top or windows at the bottom this is a car and yeah these are features which in this case correspond to individual neurons and this whole thing is a circuit a tiny algorithm connected up by weights that we can just read off from the model and the way I think about the field of mechanistic interpretability is that interupt ability seems really important uh I think that there's a lot of just it seems really important to understand what these killer black boxes that we unleash of the world actually do and actually want uh there's a lot of different approaches to interpretability mechanistic interruptibility is characterized by being very bottom up and focusing on very detailed low-l questions to try to understand them in detail uh in contrast to more top down things that might be more scalable but also tends to have a less crisp picture of what's going on and be less rigorous and my vision for mechanistic interpretability is to form what I call an epistemic foundation for a rigorous science of interpretability what we eventually do to a Frontier Model may not be this careful artisinal bottomup search but it seems extremely useful to know how to do this to build an understanding of what goes on inside these models to have a wealth of case studies we actually understand in detail and use this to form a more principal understanding of what kind of top down approaches make sense and can be done uh though I also am very excited to see the growth of other approaches during inter ability all right so let's dig it a bit more to this theme of features and circuits the theme of this section is the models act as feature extractors they find properties of the input they compute them and they represent internally and we can identify what features the model has extracted the features that it knows uh a really great paper for this is the multimodal neurons paper from go out which just find all kinds of gorgeous features inside a multimodal model uh multimodal meaning it can do image and text uh this is an image capturing model called clip uh they found region neurons uh which cover Europe India West Africa there's a Donald Trump neuron there's a Catholicism neuron there's a happy neuron there's a teenage and elderly neuron and an anime neuron and just all kinds of wild stuff I just find it kind of beautiful what weird things go on inside these models uh if if you want to play around there was an exciting recent paper uh called towards mon scientic about sparse Auto enod dictory learning from anthropic uh brick and and Al uh which I'll discuss more later but one really cool tool they have is this feature viewer that lets you just go and browse a bunch of the features they found inside a one layer model here's a feature for uh firing on DNA sequences which apparently are common enough they actually mattered in the data set this model was trained on uh the model really likes activating on strings that look like DNA and tokens that are likely to come next to DNA and downweight things that are not particularly DNA related uh and if you want you can just go to this tool and go browse around I'd love to see what you can find uh so yeah takeways from this section models are feature extractors and to a certain limited degree we can identify what they know and how it's represented and I have a bunch of open problems about studying learned features that you could go check out all right next section circuits are kind of functions circuits of the model how the model thinks and the theme of this section is that circuits are real crucial and can be understood so one of my favorite examples of a circuit is that of induction heads uh induction heads uh were a circuit that we found in two layer tensing models in this paper and mathematical framework that I was involved in anthropic and the core idea of an induction head is that language often contains repeated text uh and that being able to detect this repeated text is really important uh for example if you see the word Neil you probably have no idea what comes next if it's just an arbitrary document but if you've seen Neil Nanda five times before and Neil has never been followed by any Theos a pretty good guess is that Neil Nando should come next and uh induction heads are how the model implements this algorithm they check whether the current token has occurred before they attend to the thing after the earlier occurrence of the current token and then they copy the thing after the earlier currence to the output and predict that it comes next and we found these uh by looking at a tiny two layer attening model in this super simple case we were able to get a fairly detailed mechanistic understanding of what was going on at least for the simplest form the algorithm which you can see in this great blog post from K McDougle uh but induction heads more broadly turned out to be a pretty big deal for what was going on inside language models more broadly as you found in this follow-up paper also about that uh specifically um there's this phenomena of in context learning that's an interesting and mysterious thing that language models are good at uh so what is in context learning uh language models are trained on pretty long documents like hundreds to thousands of words is pretty typical uh lately they've been going up to like 128,000 tokens though honestly I don't even know where you get many documents that are that long Beyond Just Books uh and uh it's kind of surprising that models can productively predict and be trained on these in particular it's surprising that models are better at predicting the 100th token in a document than the than earlier ones like the 10th uh or maybe better ex would be it surprising models are better at predicting the 500th than the 50th like it's pretty clear how you use things in the same sentence in the same paragraph to predict what comes next maybe even the previous paragraph but it's pretty unclear given a three-page uh document how you used stuff three pages ago to predict what should come next uh and a weird thing we found in small models is that if you measure this in context learning if you measure how much better it is at predicting the 500th token than the 50th token there's a sudden Improvement in a fairly narrow banded training what's up with that uh so it turns out that this corresponds to the formation of induction heads uh it only happens in two layer and above models uh one layer models aren't complicated enough to form induction heads because they're requires two heads working together across layers and in the same narrow bound and training uh the induction heads for and further we in this paper present a bunch of causal evidence they are actually deeply linked EG if you delete induction heads they systematically damages in context Lear um further induction heads are pretty Universal across all models we've looked at they're not just specific to these toy models uh in the paper we found them in models of size up to 13 billion uh I found them in chinchilla a 70 billion parameter model internally uh in uh we don't have a mechanistic understanding of these induction heads and they may not be exactly the same as the things we found in these small models but they clearly do this thing of detecting and predicting repeated text and to me this is really exciting because it's suggests that models learned these legible algorithms called circuits uh that we can reverse engine near them that they are deeply tied to interesting phenomena such as emergent things and in context learning and that also we can take much more tractable toy language models and still learn insights that may transfer to the much more interesting Frontier language models um all right uh end to that case study I now want to just motivate a bit why you should care about this from an effective out perspective why is understanding what goes on inside these models these features in these circuits actually interesting and the key tagline for this section is this question is the model aligned or is it just telling us what we want to hear so here's something I found pretty evocative in the gbd4 system card uh the uh Arc evals were trying to see if gp4 could get a task RIT worker to solve a capture for for it the task rabbit was like oh why do you care are you a ribot the model when prompted to reset loud instrumentally realized that it would be useful to lie and that should make up an excuse and it successfully made up a convincing excuse and to me this suggests that a model's inputs and outputs are in future not going to be enough in order to understand what's going on in t that models are capable of realizing that it is instrumentally useful to deceive their human operators and they're competent enough to do this I'm not scared of gbd4 like if we're not dumb enough to give it a s secret scratch Pad maybe we can get away with it not realizing it can do this or not doing this competently and by looking at the scratch Pad it's pretty clear what's going on but to me this is like a pretty clear indication that this is not going to be the case in fature the jump from here to models can just reason about this without making the reasoning at all legible to us doesn't seem that big and so it seems really important to be able to look inside the model and to notice is it just outputting things to get me to believe what it wants or is it just actually trying to be honest and actually trying to be helpful uh one concrete World it seems pretty plausible we could end up in is that we make AGI and to all the tests we can throw it up it seems aligned and a pretty important question to me is what then it seems uh yeah none of the here releasing great like uh I'm not comfortable just deploying a system in the world that we think is smart enough to competently deceive us because we train these systems so what we can measure we don't know how to measure the internal cognition and how to make sure it's actually being honest rather than just telling us what we want to hear and a really good system is probably really good at telling us what we want to hear so it's really really hard to rule out that the model is trying to commit some treacherous to uh but on the other hand uh it seems kind of difficult to just say and so we should never deploy AGI and this will never happen in the world and I mean uh the Paul's AI movement has gotten more traction than he would have expected AI policy is moving pretty fast and it's moved a lot since when I wrote These slides so maybe I'll feel different about this in a year uh but it sure seems to me like it's pretty important to have a plan for what it looks like to actually deploy AJ and a world and have anything approaching confidence that's not going to commit a treacherous turn and to me interruptibility seems like one of the main things that could get me anywhere near confidence though full confidence still seems like a pretty big lift in particular the idea of eliciting a model's latent knowledge uh teing it internal beliefs and either decoding it from the model or finding some way to get it to Output this internal knowledge in a way that we are really confident is actually faith and one of my hopes is that if we can get good at interupt ability we can actually do these things to Frontier systems and get to a point where we have a system that we think could be amazing for the world and actually have reason to believe that we have succeeded at this there is not my only reason for being optimistic about interpretability or at least hopefully interpretability will matter but it's one of them uh there's yeah there's a bunch of other reasons I think interruptibility is exciting uh generally what I'm thinking about my interruptibility work day-to-day at the moment my top priority is this more meta theory of change I just want to get better at doing mechanistic interpretability all because we are not very good at it we are quite far from Frontier models and even on small models like GPD too small I think there's a lot of things I'm pretty confused about uh but more ambitiously and in the longer term uh I think that two key things two key categories to me are that detecting misalignment means detecting misaligned reasoning and I really want to get good at that uh what does it look like to audit a model for whether it's being deceptive can we get better feedback loops for alignment approaches by taking a model and interrogating in detail how deceptive it is even as models get smart enough that we can no longer trust that our blackbox evals actually tell us what we want can we give a model feedback trainer on this feedback according to whether it did the right thing for the right reasons rather than just did the right thing and secondly there's this big category of just better understanding whether misalignment is a big deal at all uh there's a lot of deep confusions in the case for alignment a lot of fuzzy messy Concepts like what is agency what is situational awareness are systems gold directed and real interruptibility seem like K sh a lot of light on this and secondly demonstrating if a system is actually miss align it's plausible we live in a world where alignment is just really hot and we're going to try a bunch of techniques that might superficially look like they work but they don't and then they just produce systems that are deceptively aligned it seems really important in that world to have a great interruptibility case that makes super clear that these systems are actually deceptive so we can coordinate the world and especially regulators and policy makers around this and my hope is that interpretability can be part of that story to uh on a more personal level I just find mechanistic interpretability a really fun area of L to work is easy to get soled in you get pretty solid feedback loops but also are just fun like to me it's a kind of VI the vibe is kind of a cross between maths and playing around with linear algebra computer science thinking about abstract algorithms uh Natural Sciences doing deep empiricism and running experiment in a beautiful well controlled environment where you can carefully design experiments to perfectly test your hypotheses and carefully empirically and skeptically seeking truth one bit of advice I have for people getting into the field is to just write a lot of codes get your hands dirty and really get contact with reality reading papers is great but don't just read papers and blog posts and yeah I have a blog post on how to get started the I'd highly recommend if you're looking for a starting point uh a question I often get asked in EA contexts is are too many people doing mechant though I think we get the impression the field is a bit less hot now than it was a couple of months ago when I first made these sles uh my two attempting to convey a nuanced opinion in like a minute My overall take is that mechanistic interruptibility is important uh it can absorb a bunch of people and I would like there to be way more people working on it now than I currently working on it but that in the global alignment portfolio my guess is that more people are trying to work on mechanistic inability and fewer people are trying to work on other agendas than is optimal at the margin uh I think that generally EA overweight the is this the most neglected thing at the margin consideration and underweight like personal fits if this talk seems exciting for you if you read me and papers you're like oh my God I want to work on this then I think you should just go for it and not worry about a thing being overrepresented at the margin but if you feel like you really want to do something in alignment you don't know what and maybe back into T because people think it's important but who knows I don't know maybe you should go try something else there's a bunch of other things in alignment uh and and yeah uh all right I'm going to try to go through one final case study in the remaining time and then try to leave some time for questions so how do models represent their thoughts understanding superposition so a key sub goal in mechanistic interpretability is to be able to decompose a model to independently meaningful and composable units or features uh we want to be able to these models are enormous high-dimensional objects and it's really useful to be able to be say like this is the cat detecting neuron it lights up when there's a cat and I can just ident analyze it in isolation without needing to care about every other bit of the model an initial hope in the early field was that neurons were these individually meaningful units that each neuron was monatic and corresponded to some feature uh this is for people with a linear algebra background you might hear this and say like wait what uh model activations are vectors neurons are just entries in some standard basis why should that standard basis be meaningful you could apply an arbitrary rotation this is less naive than it initially sounds because neurons immediately follow a nonlinearity like a reu or a jell that applies element wise if you apply a arbitary rotation there's now substantial interference between different entries so different neurons uh can't vary independently of each other so there was some theoretical reasons to think that neurons being meaningful was actually kind of legit uh sadly it does still turn out to be kind of BS because of this problem of polysemantic it is just sometimes the case that there are neurons that represent multiple things like this neuron in a multimodal image model that represents poetry and games uh and dice and poker uh dice games and poker kind of form one cluster poetry forms another these seem pretty unrelated uh one hypo hypothesis for why this is happening laid out in the Fantastic toy models of superposition paper is this hypothesis of superposition superposition is the hypothesis that language models want or models want to represent more features than they have Dimensions the language models uh represent their features uh language models want to represent more features than they have Dimensions these features correspond to directions in space these are called linear representations but the rather than having say a thousand features and a thousand dimensional space that are all orthogonal to each other the model tries to compress in 10,000 features that are almost orthogonal to each other uh that have like low but not zero do product and the model just deals with the interference between them this would explain poly semanticity because if the model has more features than it has neurons obviously neurons are going to be polysemantic uh and what does work here look like so one approach is just forming a clearer conceptual framework of one Earth is even going on the Fantastic toy model is a superp position paper built a toy model whose job was to take features in a large disentangled space compress them into a low dimensional space and then map them back to the large disentangled space and uh they tried to see when the model would use super position in the bottleneck space what they found is that if the features were sparse that is if the features were zero most of the time chosen independently of each other um then the model was a lot more willing to use superp position because it's a lot easier to deal with interference between features when one is off and the other is on than when both is on uh both are on uh they found that as they increased the sparity the model went from two features in two Dimensions with orthogonal directions to these beautiful geometric structures like two antipodal pairs or five features as a pentagon and I think there's still a lot of room to build better conceptual Frameworks here like can we do this for computation superposition rather than just representations superposition can we Model A model's MLP layers rather than the residual stream as is St here another approach from the recent towards mon manity paper uh that I'm super excited about is extracting features from superposition training what are called sparse Auto encoders to find an interpretable set of directions that is overcomplete meaning there are more directions that are dimensions uh they found a bunch of interpretable features uh even though these features were highly superposed uh for example here is a linear combination of most of the MLP neurons in a one layer language model that seems to detect Arabic uh it gets kind of noisy close to the origin but when the feature fires a lot it's like super super clear that it's firing on Arabic uh the core technique here is this idea of training a sparse Auto encoder Auto encoders are a technique in ml that take some input and uh try to compress it and then try to reconstruct it from that compressed State uh and you the hope is that the compressed State can tell you something about the input that you didn't already know sparse Auto encoders kind of flip this we try to map the input into a larger space because we expect there are more directions that are meaningful than Dimensions we hope that each entry in this space is interpretable and then we apply a technique called L1 regularization to the entries in this latent space uh not to the models parameters but to the model's activations to try to make it sparse so that not many things activate and uh in this exciting recent paper they argue oh if you do this and do a bunch of Tricks it just kind of works and this can let you extract an interpretable basis from things in superp position and I think there's a lot of exciting open questions to build on here like just training a bunch of these on models uh seeing what insights they can get us about smaller models uh seeing whether we can understand a bunch of new circuits and features by seeing what kinds of features were actually found that you can browse an anthropics feature viewer uh all right I think I'm going to wrap up there as my final case study and move over to Kim for questions thanks lot to everyone for listening and just as a reminder you can go to uh where is the link you can see my slides here great yeah someone put that in the chat already so thank you for whoever did that um yep we don't have a ton of time for questions it it feels like the the most responsible question to ask would be from Brian um How likely do you think it is that we actually need Mech and tur to succeed to successfully align AGI or super intelligence could you imagine it not being necessary and what what that might involve so for instance mainly using concept based interpretability or some other approach yeah so I think two questions here there's do we need interruptibility and there's do we need mechanistic interruptibility uh I think that do we need mechanistic interupt ability seems like probably false like it's one approach to interruptibility I like it as an approach I think it is less the most approaches to interruptibility I think it would be wildly unreasonable to claim that this is the only approach to interpretability and and I'm definitely a fan of like just try a bunch of stuff try things that seem more scalable try things that seem like too dumb to work but where it would be really embarrassing if no one tried this uh yeah try I would love there to be a bunch of different kinds of fields of interpretability that all deeply care about this core problem of trying to understand Frontier Air systems and whether they're aligned but stack a bunch of different approaches my guess is also the different approach has learn a bunch from each other like taking a concept based approach and applying it to a case St way we can actually understand what's going on mechanistically I hope will teach us things about what the concept based techniques actually doing uh do we need interruptibility for alignment at all my guess is no uh but it sure seems really useful and in particular it seems I don't know I've got a am of credence that we live in a world where we're just going to succeed at alignment via kind of dumb stuff that looks like rhf on steroids uh but we're not going to have some deeply principled guarantee that the thing is aligned it could be deceptively aligned but it turns out we get lucky and it's not and I'm like man I'm not comfortable with that world we wouldn't all die we probably wouldn't all die in that world but it doesn't seem great and I think in ter builds I'm a lot more confident that with interpretability we could get to a world where we're confident we have a thing that is aligned or at least a lot more confident I do think that interpretability also helps us increase the probability that the thing we produce is aligned by like helping inform inter alignment research helping inform policy Etc but those are more indirect cool uh we've got a minute to fit a question in so I challenge to answer this in a minute um what do you think are the strongest arguments against working on ma uh it's too narrow uh it has not yet been usefully applied to a downstream task you have really short timelines like one to two years and they just like there's no way this is getting useful in time to matter uh you think that it's really specific to the Transformer architecture and that we're definitely going to Pivot from Transformers in a way that's going to fundamentally invalidate a lot of progress so far I don't really believe that one but whatever uh you think it's just going too slowly and it's too bottom up Co a lot of a lot for people to SN their teeth into there um we've reached the end of the session so I'm not going to ask any more questions i' like to thank Neil for the presentation and thank everyone for the thoughtful questions and apologies that we couldn't reach uh or answer most of them um yep thank you everyone and have a great day thanks for to talk B