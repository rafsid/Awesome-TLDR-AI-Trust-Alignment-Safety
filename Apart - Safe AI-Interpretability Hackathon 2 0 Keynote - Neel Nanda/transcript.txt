Source: https://www.youtube.com/watch?v=gQibeD1ygE8
Transcribed: 2025-12-30 18:10:19
Method: YouTube Transcript API
==================================================

so of course welcome everyone to uh this month's alignment jam on interpretability again with the great Neil Nanda uh who has graced us with his presence from the deep mind conference officers here well lit conference officers and uh of course thank you very much to him for coming and uh Neil Nanda is an amazing researcher he's done a lot of work in inability and been a big voice in spreading it and making it available for a lot of buing researchers a lot of starting researchers uh with both YouTube videos and some of the some of the public discourse on inability that's like super interesting he has previously worked with anop that's perfect I think that's uh that's the good public discourse right um but yes Neil nand has also worked with Chris Ola one of the founders of uh of the mechanistic interpretability field I I would say it seems that way at least uh who is working at anthropic now and uh he has also been with fhi and uh with a few other organizations so thank you very much uh for to Neil for coming and I will kick it off to you yeah thanks a lot um I should probably also add the context that I spent a while at an independent interpretability researcher and recently joined the Deep Mind interpretability team but yeah thanks a lot for inviting me and I'm super excited to see how many people are excited about interpretability um yeah so welcome everyone to the mechanistic compability alignment Jam uh this is going to be a lightly adapted version of my previous keynote so if you've watched that you can just like I don't know go hack around stuff on your own but this is a whirlwind tour of what mechanistic interpretability is and a bunch of open problems in it trying to have an orientation to one of things that I think someone could like get some traction on in a weekend and this is you can see the slides at this link and I'm going to be uh maybe esman if you could put this link in the chat that would be great um the slides are going to be full of links and resources so I recommend checking them out the key things are this is an ad for my concrete open problem sequence um which is a sequence called 200 concu different problems in mechanistic inability it I've got a post on how to get started in the field and there's a very bridged version of that called the quickstop guide at n./ quickstop that I believe is the main resources for this hackathon and this Library I wrote called Transformer lens for doing mechanistic inability on language models that will hopefully save you a bunch of the incredibly tedious infrastructure setup that you need to do before you can do anything in these models and yeah the spirit of this presentation is going to be a worldwind tutour roughly split into discrete parts I'll aim to go fast and I'll flag when I'm switching area so if you zone out get confused you can rezone back in and yeah so high level motivation what what is mechanistic interpretability what is the point of all this um I think that a key question basically anyone doing anything in AI should be asking themselves is what does my research look like in a post gbd4 world and to me the key thing is large generative language models are a really big deal and we either want to be aggressively pursuing extremely scalable techniques that can be relevant to the biggest models or we want to try to be aiming for fundamental insights that will let us be able to tackle models at any scale better and I will I would roughly categorize meup in the second category though I'll discuss a bunch throughout this ways in which I think it links to SS actually relevant and yeah one high level motivating thing is I think that if you want to do interpretability work that is relevant to alignment a bunch of the standard approaches to interpretability that look at things like the inputs and outputs of the model are just basically wildly insufficient um for models that are gbd4 level and above and here's a particularly evocative example from the gbd4 system card where Paul ciano's alignment Research Center tried to get gbd4 to get a task rabbit worker to solve a capture and it was able to competently reason through that it should manipulate the task rabbit worker and then gave a pretty convincing lie and when you have a model that is smart enough to be able to solve the task of understand what its operator wants to hear even if it may not have the agency or go reers to do this I think you really want interpretability to be able to work in that world and I would operationalize this goal as to try to understand the model's cognition what is the thought process and algorithm that generated its outputs and in particular to what degree is the model aligned or just telling us what we want it to hear and this is my very much my high level motivation of this whole thing um the actual I'm going to talk about is much more grounded and zoomed in um but this is just like some broad picture how I think about interpretability why I care about these things all right so in necessary context to jump into the actual problems is just briefly outlining what is a Transformer the neural network architecture that underlies basically all interesting modern language models and uh wour tour Transformers take as input a sequence of words uh words is actually tokens which are kind of subwords but this is a annoying Nuance that's not important for the intuition but is very important for like actually doing research on them the output of the model is a probability distribution over the next word though um a key Nuance is that actually the output is a sequence of probability distributions over the next word where for every word in the sequence the model makes a prediction for the word after that will be and Transformers are set up so information can only move forwards through the network which means that I don't know if you're trying to predict the eighth word after the seventh word the model can't just cheat and look at the eighth word and the internal of the mod the internal representations of the model is What's called the residual stream this is a sequence of representations um one for each of the input tokens and there's a different residual stream after each layer and in the way to intuitively think about the residual stream is it's representing the word plus a bunch of context and processing and computed information and every layer of the model is an incremental update um to that to the residual stream that does some additional processing and computation and refines the representation of the word until at the very end it can predict what should come next there's two types of layers the first is attention layers which move information between words attentional are made up of heads Each of which act independently and in parallel and they look at all previous residual streams identify which ones are most relevant and collect some information from there and move it to the current residual stream and generally a lot of what we've had the most luck with in Mech and Tu is trying to interpret heads the second type of layer uh the MLP layers which process information once it has been moved to a word uh very roughly attention is like the plumbing that moves information around because Transformers operate on sequences MLPs are the where the thinking happens once all of the key information has been collected and I have these two walkthroughs um one being a 1our YouTube video explaining what a Transformer is at a more fleshed out version than what I just said and the second is a coding tutorial of actually writing gpd2 from scratch with an accompanying template you can fill out yourself as or before you watch uh generally I think that one of the more productive things you can do when trying to do Transformer meup is just get the basics of how a Transformer works and spending at least some time going through some mix of these walkthroughs if you've never dealt with Transformers before I think is a pretty solid place to start all right and context hope that was helpful and not too boring for the people who already knew it now what is mechanistic interruptibility so the broad Spirit of the field is goal reverse engineer neural networks uh analogist to how we might take a compiled program binary and try to reverse engineer that to get the source code and it's built on this bold and ambitious hypothesis that neural networks learn human comprehensible algorithms and can be understood in principle but that models are not incentivized to make these algorithms legible to us they learn a blackbox stack of matrices that get good performance on the task they're given but the a consequence of the structure of a network is that it's incentivized to form certain kinds of algorithms and the by learning the right techniques and mindsets and hacks we can decode this and get some insight into what they're doing and this is possible but also really hard and there's two key concepts for reasoning about what goes on inside models the first are features the variables inside the model and um properties of the inputs like this is a noun or this is a list variable whose first element has one in Python code or any any other property of the inate that could possibly be useful and the second are circuits circuits are the algorithms the model has learned which generally act to take some features and compute more features until at the very end you can compute the feature of what you think the next token is and generally features are represented in the model's activations circuits are represented the model's weights and a key property of Mech done well is that it will let us because we're trying to deeply engage with the algorithms in the model we should be able to distinguish between different kinds of model cognition that produce identical outputs and my personal hot take is that a deep knowledge of circuits is basically crucial to understand predict and align Model Behavior and that without it we just pretty hopelessly confused about what's going on inside of them in the bottom right we have a cute toy example of features and circuits taken from an image image classification model the psychedelic pictures represent neurons that are detecting different features of the input car windows car bodies and car wheels and here's a circuit which is some convolutional weights assembling these neurons into a neuron the next layer which seems to detect cars and here's a bunch of papers that can be argued to be mechan though definitions wildly vary but there's like a bunch of stuff and a diverse range of stuff though I'm going to be focusing on what I personally most excited about um on a more personal front why am I doing mechanistic inability um or specifically why do I enjoy doing mechanistic inability why is this a thing that I chose about other things that seem equivalently impactful and one thing that's really great about it in particular as a thing to get into or do a hackathon in is it's very easy to get started and it's very easy to start getting feedbacks um a key thing you really need to do whenever you're getting into any new field is get contact with reality and be able to distinguish whether what you're doing is [ __ ] or useful and you're poking at a model and you're getting feedback of what your hypotheses look like it's great um it's also just really fun I I have a pure maths background and to me the vibe is kind of a mix between maths because I'm trying to reason about what is essentially a massive stack of linear algebra computer science because I'm reasoning through the algorithms and implementation details and various crap about the Model Natural Science because I'm fundamentally trying to be an empiricist I form hypothesis I test the hypothesis I'm truth seeking because there is a ground of Truth to be G gained about the model it is hard and confusing but I can track down that truth and I can believe there is an answer out there and my high level advice to people getting into the field is code early and code a lot by far the most common mistake I see is people who just spend months reading papers and thinking about things and never just opening a cab notebook and playing with a model which is one of the reasons I'm really excited to help out with these psychop because it is a structure where it's kind socially awkward if you just spend a weekend reading and try to write no code so go forth and write code and yeah here's a post I made with a bunch of more concrete advice on how to get started all right concrete examples of Mech tur and areas of open problems um features what does the model know so one of the beliefs I generally have about networks is that models a lot of what they're doing is trying to extract features of the input and represent them internally as essentially directions in the residual stream or sometimes it's individual neurons and here is a wild graphic showing a bunch of neurons that was found in a in a model called clip in the multimodal neurons paper and uh these are just like a bunch of really wild abstract neurons like activates on pictures containing the concept of teenager or the concept of happiness or Hinduism or Donald Trump this one also activates on things like Maga hats and Republican politicians you have Europe or group photos and it's just like what these are things the model learns and can represent internally and generally the study of what features are inside a model is much better repres is much better explored for image models and language models um which is sad but also exciting because I think there's a bunch of progress here that someone can make in a weekend even with no prior knowledge of Mech and Tu um one of my favorite papers that propably dug into this uh though I'm very biased because I am an author on it is soax linear units where one of the and underrated parts of it is a bunch of qualitative exploration of the kind of weird neurons they found inside models uh my favorite is this numbers that implicitly describe numbers of people neuron which we can see activate on things like 150 and six and 70 but not on things like two because two does not describe a group of people and I made this tool called neoscope which takes a bunch of Open Source language models and for each neuron in the model it gives you a page showing the text that most activates that neuron and generally if you look at this and you see a bunch of and you see like a clear pattern in those texts that's evidence that that's what the neuron is detecting uh here's uh one random neuron I found in a one layer model um which seems to mostly be activating on uh the in in in the after a verb involving running like walked or ran or running or even running Windows and I guess this is useful for the model it's also just kind of cute and wild and one of the posts in my open problem sequence is on studying learned features with a bunch of ideas what to do here but the main concrete project I'd recommend people try is just go and poke around in the middle layers of the largest models in there like gpd2 XL and just see if you can find any fun and weird features like this and I give a bunch of advice in this post for what rigorously identifying features should look like it's not enough to just notice patterns you want to be editing the text that goes into the model trying to find counter examples trying to check if your hypothesis is too general or too specific and just trying to do science to it um I think this is a particularly accessible entry point because you just really need to know that much about coding um all right pivot new area circuits which are analogist to functions or sub routines or algorithms in the model how does the model think and one of my favorite examples of a circuit is this phenomena called an indu induction heads which we found in this paper I was involved in called a mathematical framework where we looked at toy two layer attention only language models um that is models with just two layers and without the MLP layers and we found that so a thing which is true about text is it often contains repeated substrings like if you see Neil in the text and you saw Neil Nanda previously in the text it's a lot more likely that nand is going to come next and than it would be if you just saw Neil in isolation and this turns out to be an insanely useful algorithm for models to learn and induction heads are a simple circuit where two heads in different layers work together to detect if the current token has come up before and if so look at what came after it and then predict that that comes next and and this is like a genuine algorithm the model has learned that is applied on the text given to it at input time and by Circuit I mean this kind of actual algorithm the model runs um notably the fact that we can find algorithms like this in my opinion just completely disproves the strongest forms of language models only learn statistical correlations and never learn real reasoning um but okay um and a big area of an area of open problem I'm pretty excited about is just going and poking around in toy language models I open- sourced 12 toy language models I trained and trying to see what you can find I'm particularly excited for people to take a one layer model with MLPs and try to understand what the what the MLPs and that model are doing because we're just really bad at Transformer MLPs and no one has checked very hard and I think there's probably a lot of low hanging fruit that could tell us exciting things and um here is something I'm not going to go into in detail but a great blog post from kyum McDougall outlining what the actual algorithm learned by the modelist where very roughly there's something in the first layer where an attention head acts to copy information from a token to the one after and a head in the next layer which checks whether the current token matches the previous token of something in the past and if so predict that that comes next that was probably too fast for anyone to follow but you should totally go read this blog post and it's great and so a natural a natural reaction many people have to meub is sure this is really pretty but you know you're just looking at kind of cute toy things and there never going to matter and a great paper uh well a paper I was involved in which I personally think is great but I'm extremely biased is induction heads and in context learning which is a SQL paper to a mathematical framework and in this we try to investigate the phenomena of in context learning in models in context learning is when a model is capable of tracking long range dependencies in text like using inform more than a paragraph or two back to predict the word that comes next and you can measure this by just checking are models better at predicting things later in a document than earlier in the document and it is kind of wild that models can do this like look usefully using information like a page ago to predict what comes next it's actually kind of hard and non-trivial um but models are pretty good at this this and um in much bigger models this leads to very striking stuff like few shot learning where you give models a few examples of task and it seems to suddenly become better at it and actually um we found that in context learning is an emergence property meaning that when you train a model it goes from not very good at in context learning to very good at it very rapidly during a narrow band of training um and this only happens in models that are two layers and above and not in models that are one layer and uh this these are just tiny models but you get the same striking emergence in much larger models and what we found in this paper is that this emergence of in context learning seems deeply causally linked to induction heads induction heads also form in a phase transition at the same time and and we we present a bunch of evidence like that if you ablate an induction head this will disproportionately damage in context learning and a caveat I should give to this that I think is an easy way to misunderstand the paper is that when I say induction head here I'm referring to a behaviorally induction head something which can do the task of check whether the current token came before and predict what came after it but where I'm not making any strong claims that it has literally the mechanism I outlined here and I think an era of exciting work someone could do in the haathon is take a model that has induction heads but is bigger than the toy ones we looked at and try to really understand what's going on there um but I think the fact that heads that seem to be doing induction stuff seem deeply causally linked to in context learning is just kind of wild and makes me a lot more optimistic that me can teed us real things that matter about models where the theory of impact here is there's a bunch of important and confusing behavior in large models can we study it in smaller toy settings and use this to demystify and ideally predict and understand it better in Real Models and an area of open problems that builds on this is my post on analyzing training TS we just don't really know much about what happens inside a model as it trains and I expect there lots of fascinating and interesting Mysteries to uncover here in particular a thing which is really concerning about Cutting Edge models is that they have emergent Behavior if you train them for long enough or make them bigger sometimes they will suddenly become able to do things and there's some moderately compelling evidence that this is due due to this is partially due to them just suddenly being able to learn circuits they didn't have before and I think that by better interpreting what happens in a model during training we should be able to become somewhat less confused about this all right um and previous section new section what is the mindset and Vibe of mechanistic inability um this is just like a few principles that I find useful for thinking about the field one mechanistic interpretability is like doing alien Neuroscience models are interpretable but not in our language we need to learn how they think and how they represent things and if we do a lot of things that seem mysterious can dissolve two skepticism it's just so easy to trick yourself it's so easy to come up with some elaborate complex hypothesis about models that actually is just kind of BS and there's a much simpler thing going on and one thing this motivates me uh to believe is that we should significantly prioritize deep rigorous dives into models and really understanding them over things that superficially look Broad and scalable but where it's just really easy to be doing total nonsense and where you don't understand what's going on ambition it is possible to achieve deep and rigorous signing of these models it's hard and confusing and [ __ ] but there is a goal worth aiming for and I don't think this is like actually guaranteed but I see the field in part as a bet that even if the specific circuits we're finding in small models won't generalize or aren't that interesting there are underlying principles and structures to these models that we can understand that do generalize and can let us gain Insight that genuinely matters and here's a case study of some independent work thed uh on groing that I think is a good illustration that if you really understand something mechanistically this can help dissolve Mysteries and confusions around it so groing was this wild phenomena that uh was found in this open AI paper last year where they found that if you train small models on algorithmic tasks like modular division mod 97 that the model will initially memorize the data it's trained on and do terribly on the data it hasn't seen and gets per basically perfect accuracy but then if you keep training for a really long time uh the model will suddenly Gro and learn to generalize and it's like what why why does this happen uh in particular naively the model's already good at the task it should just continue memorizing why on Earth would it generalize and why on Earth would it take so long and I tried to investigate what was I was like hm this is a weird phenomena this happens in small models it's an algorithmic task mechanistic interpretability should just obviously work here and this should just obviously be the morally correct way to figure out what's going on here and I found this circuit for modular Edition in a one layer Transformer which I trained to Gro modular Edition and we found that the model doesn't like thinking about modular Edition in nice normal ways like addition or binary addition like a computer would do it instead models like to think in trig functions the model decides to think about the inputs A and B as rotations around the unit circle uh proportional to A and B it learns uh it learns a it memorizes a discret fer transform to convert the inputs to these trig terms it uses tricker entities to compose the rotations and this is addition and it automatically gives you mod because it wraps around the circle and then it uses the unembedded the final map from the final residual stream to the logits to undo this by rotating backwards by every possible output rotation C and looking for the one whose projection onto the xaxis is the biggest using the fact that only if C is a plus b mod n does rotating back by C get get you to the do nothing rotation and firstly what what how how is this how a model learns to do modular rition um but I also think this is a really beautiful illustration of alien Neuroscience the model was initially incomprehensible but once I realized it was doing Galaxy brain furer stuff everything became much clearer and another area of open problems that I'm excited about is interpreting algorithmic models taking models like the modular Edition model train on some algorithm task and trying to understand how they do this and I think this is an unusually accessible area where you might be able to get some real traction over a weekend and uh just as a pretty demonstration that really understanding circuits can actually matter uh so this animation on the left we have have this is an animation of the model as it trains this is the loss curve this is the embedding and when the embedding is sparse that shows that the model has learned the trig algorithm when it's dense it shows the model has is just memorizing and we see that during this long seeming Plateau before it grocs the model is actually learning the true circuit and it's solely transitioning from the memorizing circuit to the generalizing circuit and the Crash only happens when the model gets so good at generalizing it decides it no longer needs to memorize and it gets rid of that circuit and because we understand what's going on internally we can just look inside and see what's going on and it's so pretty and all right new area uh this time I'm talking about a frontier of the field where we're just kind of confused more so than an area where I want to showcase like really exciting and inspiring work um which is a confusion how do models represent their thoughts so in order to do mechanistic inability we really need to deeply engage with how the models represent the features and other knowledge they have internally and naively um our prior might have been that they would store the features corresponding to individual neurons because models want to be able to store and reason about features independently and models have what are called activation functions nonlinear functions like re and jellus that act independently on each neuron and so naely if you store a feature in each neuron then this will operate independently and everything is good uh but this turns out to not always be the case this is an example of a neuron in the multimetal neur neurons paper when you apply a technique called feature visualization to visualize what the neuron is looking at you see a bunch of cards and dice and it looks kind of like a game and neuron but then if you look at the images then you're almost activates on half of them are about games and poker and then half of them are about poetry and it's like what and uh this is just actually a pretty General and confusing problem called polys semanticity where neurons represent multiple things and I have a list of open problems trying to understand what on Earth is going on with police Mantic and a hypothesis put forward in this great paper toy models of superposition um is that poly semanticity is because of superposition superposition is the phenomena where models use uh where models want to represent more features than they have dimensions and they learn to do compression where they shove in more features than they have Dimensions uh here they just have two features in two Dimensions everything's normal here they have four features in two dimensions and here they have five features in two dimensions and um this they exhibit a toy model and show that superp position can happen in it and but they don't actually investigate Real Models and an area that I'm really excited about is just going and looking at Real Models and trying to understand what's up with Po manity and seeing how much the predictions this paper bear out and in particular whether you can actually observe any super position or just any of the other predictions of this paper like the really important thing things will get their own Dimension but unimportant things will be in super position and one thing which I think is a particularly tractable area is just look is again looking at the MLP layers of one or two layer toy language models and just seeing what what are the features that are represented there how are they computed if a feature is represented by many neurons how does that work how is the model using the nonlinear activations across many features and uh the uh toal superposition paper I think did some genuinely great work trying to clarify our conceptual Frameworks for thinking about models particularly with this glorious picture exploring the geometry of superposition so what this graphic shows is that you have a model that's trying to compress about I think about 100 features into 20 dimensions and and they measure for each feature what fraction of a dimension does this take up in the model and they find that the features self-organize into orthogonal subspaces where some number of features are shoved in so here they spontaneously self organize into tetrahedra groups of three dimensions with four features in here they self-organize into triangles and antip pairs here they self organized into pentagons and antipodal Pairs and square anti prisons and this Behemoth they call the everything bagel and does this happen in Real Models I have no idea but it's beautiful and I think that another exciting direction is working with toy models and trying to clarify our conceptual Frameworks for how to think about superp position in particular in the paper that they focus on how to compress features into a Subspace linearly and recover them linearly and briefly have a section on computation and superp position uh where they look at compressing things into neurons I'm particularly excited about compressing things into neurons and I think that if you dig further into those models you might find some cool insights maybe analysis to this uh well this is very unrealistic to get in a weekend you can get something that could be a step towards something like this found the signing Wills up with neurons all right new case study interpret um if you were zoning out or confused you can now Zone back in case study interpretability in the wild seeing what's out there so interpretability in the wild is this great paper from um Kevin Wang when he was interning at Redwood research um exploring the phenomena of indirect object identification in gpd2 small indirect object identification is uh the grammatical task of saying when you've got sentences like this the final word the next word is going to be Mary the name which is not repeated rather than John the name which is repeated and uh this is a grammatical task it comes up enough that models care about solving it um and it's also kind of clean and algorithmic but also genuinely non-trivial and they found this wild 25 head circuit in gpd2 small that did this task very roughly there's a bunch of heads which tell that the John token was duplicated storing this information on the second John residual stream there is there are these s inhibition heads that move that information from joh to the final token 2 again there's a separate residual stream on each token and so attention heads serve the role of moving information around and finally we have these name mover heads that move that copy all non duplicated names to the final token position and I think this is just really cool work and the a lot of the techniques they use are just things that you could copy and I think that trying to find and understand some circuit analogies to this is like a really practical hackathon project and last time there was this great work trying to look into a gender bias circuit that I think made some solid progress and I have this open problem post on finding circuits in the wild and so why care about this I don't intrinsically think that indirect object identification is like an important model capability why is this work useful so one of the main things I'm excited one of the reasons I'm excited about this kind of work is that you're seeing what's out there you're seeing what weird phenomena actually arise in practice in a model and two things I want to draw attention to are these negative name move the heads which systematically do the opposite of the II t they boost the duplicated name not the indirect object name and even weirder backup name mover heads which only if one of the name mover heads is ablet that is deleted or removed do the backup name mover heads take over to compensate and here's a scatter of plot of this we look at the direct effect of every head on the logits and then we ablate one of the key name Movers and we see what happens the key name mover goes from Big effect to zero because it was removed and then if we look at the actual heads then most of them don't change that much but there are like two or three heads that massively move off diagonal and this is a negative head that becomes much less negative this is a backup head which didn't do much before but now does a fair bit and what what's why does this happen what is going on and I'm particularly excited about this because ablations are a common technique used in interpretability that to some people feel like strong in compelling evidence and this is like a concrete case study which we only found by looking hard for a circuit which shows that ablations can be deeply misleading because models learn redundant behavior and forms of backup and this is what I call mechanistic interpretability as a validation set we ultimately want to be forming much more automated and scalable methods whether to find circuits or to do other things about models but I think that only by having different contexts where we really understand a circuit can we go and really check how much our scalable techniques work and whether we're finding real insight or tricking ourselves and I outlined a bunch of open problems along these lines in my techniques tooling and automation post one one that I'll call out is just going and looking for other kinds of backup behavior in models like gpd2 small and other ones of their backup induction heads is back upness of General phenomena can you make any progress understanding the circuits behind backup behavior um another reason I'm excited about this kind of work is helping us refine good techniques if we practice finding circuits we'll get better practical knowledge about what finding those circuits looks like what useful techniques are and hopefully inventing some new techniques and this is a technique called activation patching that was used to solid effect in the Rome paper called locating and editing factual associations in GB in gbt where the key idea is you have two inputs where the model gives different outputs here we've got the Big Bang Theory premieres on maps the CBS the clean input and we've got the corrupted input where we mess up the input token somehow we then choose some activation from the clean run run the model on the corrupted inputs but then at a single point replace that activation with the clean activation and this is a c intervention and we see how much this changes the model's output and how much it flips it from the incorrect output given on the corrupted prompt to the correct output given on the clean prompt and generally most activations do nothing while a few do a lot and this is a fairly rigorous causal intervention that can let us really localize which bits of the model matter for some computation and this is like one of the techniques I'm most excited about that has come out of mechanistic interpretability in the last year or two and I think this kind of thing is like solid and scalable and can tell us something genuinely useful even about much larger mods and I go through a bunch of other things you might try building on this um one thing which I don't actually think is in that post is I have this blog post on what I call attribution patching which is a great gradient based approximation to activation patching that can be much faster and um but also I haven't checked very hard how well it works and I'd love to see people just go and test run it on a bunch of circuits and see what happens and I have this tutorial for Transformer lens called exploratory analysis demo where I go through a bunch of these basic techniques and how you'd use them to analyze something like the II circuit and um I generally highly recommend finding some existing code like this and heavily stealing from it for a hecon project because if you need to write everything from scratch you're not going to make that much progress in a weekend um and final thing I want to discuss is the idea that models have underlying principles with real predictive power that we can uncover by deeply reverse engineering them and then use these principles to predict and become less confused about Model Behavior and future and the one I want to focus on is the linear representation hypothesis the hypothesis that models represent features these variables and properties of the inputs they've learned to compute as directions in space that is directions in their activations if you project the resist ual stream or disinfector you can identify this feature and this was first popularized in the words to VC paper like 10 years ago most famous for the king minus man equals Queen minus woman claim that there's some male minus female Direction and some is royalty Direction and this is to me fairly intuitive but also like a very strong and bold Claim about models not only do they process and do computation on the input but that specifically they represent these as directions in space and this is an example of the kind of thing I mean by models have structures that incentivize them to learn stuff that is understandable but have no incentive to make it legible to US models are made of linear algebra which incentivizes them to learn things like features as directions but I don't know the directions aren't stuff that's clear to us we need to go and find them and I now want to go through a case study of showing that this has real predictive power there was this great paper called emergent World representations studying a model called a GPT that was trained to make random legal moves in the game of AOW um AOW is a go or chess-like board game where you you place pieces and each time you play a piece you need to take some of the opponent's pieces which flips their card and um importantly they took a gpd2 style language model and just trained it to predict the next token on a bunch of random legal games and they had this fascinating result that the model learned an emergent World representation despite only ever seeing data about moves and next moves the model learned to compute the actual state of the board and you could extract the state of the board with a nonlinear Probe on the model's residual stream and you could use the probe to intervene on the model's representation of the board and if he did this the model would now make legal moves in the intervened on board even if the interven on board was impossible to get by normal play and this paper got rightly got a lot of hype because um this is striking evidence that models May that language models may actually form models of the world despite only ever being told to predict the next word but the thing which to me was really interesting about this paper is that they could only find the model of the board if they used nonlinear probes but in order they use linear probes that is there were no directions in space corresponding to is this probe is this Square black or white but if you took the simplest nonlinear function and one hidden lay MLP you could recover it and so you needed to do some complex optimization against this nonlinear probe to edit the board and this seemed like strong evidence against the nonlinear representation so the linear representation hypothesis because it clearly has a model of a board because you can intervene on it but linear probes didn't find it what's going on I did this investigation where I found that actually the model does have a linear representation but that rather than the model learning this cell is Black or White the model doesn't care about black or white the model cares about whether a cell has my color the current players color or the opponent's color because the model is playing both black and white move this graphic shows the models representation of the board extracted after a move and then on the next move you can extract it again it's perfectly correct but it's now flipped sides because whoever's playing next has changed and to me this is a really exciting case study that there are real underlying principles about models in particular a thing we found when looking at words to back language models has held up both on image models on Transformers and now on this weird AOW playing model and despite the evidence pointing against when they had these great results that seem genuinely nonlinear and I have this blog post where I go through my investigation um and I have a in particular I've got a post where I have a long list of open problems I'd love to see people work on including a section of specific recommended concrete startup projects one of the things I'm really excited about is that because I found a linear model um you can also linearly intervene on the model I found to change the model's output move is that this is a example of a circuit where the first half of the model converts the input to something the board State and the second half uses that to figure out the next move and so the model has become spontaneously modular is split into two bits that communicate via some interface we can understand and basically all prior work looking for circuits and language models has been on ends to end circuits things that go from the inputs to the outputs where most of the model doesn't matter and I'm really excited I feel like this is not going to generalize to enormous models and we need to get good at understanding how to find we need to get good at understanding how to understand how models are modular and how to find different bits of them that produce features that other bits use and I think that a fellow GPT and understanding the circuits behind its board State computation and its board State using is a really exciting case study because it's simple and algorithmic enough to be tractable we have this linear probe but it's also complex enough that I don't know how it works and I expected learn call things from it and yeah that closes my talk the uh four resources that I think are particularly hopefully particularly helpful 200 concrete problems mechanistic interruptibility the sequence I being shelling this whole talk uh a post on how to get started that just gives a lot of my distilled advice on how to get traction uh if you change the getting started to Quick Start I have a even more bridged version for hackathons a comprehensive meub explainer which you can just go and look up any terms you come across that confuse you and where I have lots of long sweeping intive tangents and my Transformer lens Library which is tooling I highly recommend using for any of the projects I've outlined because I tried to optimize it for being stuff that I want to use when doing research and I think that if you go in having decent tooling you'll be able to make so much more progress and yeah that ends the main part of my talk happy to take questions depending on how much time we have wonderful thank you so much Neil and let's just give him a give him a hand now you you'll just get the Copenhagen hands here but anyone who wants to um to ask any questions so I can probably start out so in general I'm um I'm curious um which type of tooling there is available now and what type of tooling you're you're seeing as as something that is needed because now you have the Transformer lens but there's also stuff like ACDC right and these other tools but is there something where you're like oh this seems to be missing yeah so so the core thing transformerland tries to do is it gives you really easy access to the models internals you can cache any activation you can change and edit any activation on the Fly the nice interface that gets r a lot of the foot Guns Plus a lot of other stuff that improves your quality of life also recently found a maintainer for it so I don't even know what's in that nowadays but it sure seems like it's good that it has a person who like actually writes tests so thanks Joseph um ACDC the thing has been mentioned is this thing being developed by arthury at conjecture um called stands for automatic circuit Discovery code that's an automated tool for looking at a take some circuit you think a model has and essentially showing you which bits of the model and which connections between them like which heads and which MLPs are actually important for that task and I'm pretty excited about more stuff like that existing though my main hesitation is I'm not sure how well we understand what this should look like or what like the right gics and design principles should look like but that I'd think about this more as a research project of like try different approaches to automatically finding circuits see how well they work and how well they compare to the circuits we understand ideally have a validation set of more than like two circuits and but yeah I think work just trying to refine automated ways to find circuits feels exciting to me um I'm pretty excited about neoscope where my General Vision for it is like a general Wiki for a model where you can yeah like a general Wiki for a model where you can look up any neuron or any head and get a bunch of automated metrics for it uh I haven't actually had the time to go and make a neoscope not a mess but I would love if anyone here is interested in going doing that and also if you want to work on a hackathon project involving neoscope um I can get you if you DM me I can get you the data which I should really get around to open sourcing properly you do not need to scrape the website um and yeah there other tooling um generally working with large models and models split across gpus or God forbid servers is an enormous pain uh I believe we've just merged we either have just merged in or we'll soon merge in something to transport ends lets you do that um though it's a bit janky it's a bit hacky and it doesn't work across devices and I think there's also ways to streamline that so that's that's the kind of thing you generally want to do more when you're like a company or an or with their own infra um rather than like a general open source Library um yeah the tooling yeah a general tent I have is how much with like research style tooling is how much you should just have generic very ergonomic tools that you can use to implement any random idea moderately fast versus something you can just click a button and do lots of stuff with very limited customizability and I go back and forth what I think the right thing to do is here generally I aesthetically flavor really things that let you convert experiment ideas to code as fast as possible across a wide range of experiments and stuff that like as a specific thing that much flexibility yeah I'm a researcher this is my aesthetic make things L let's beginner friendly yeah wonderful see we got a question from gobbins in the chat in how does activation patching compare to integrated gradients in terms of resolving circuits uh so so integrated gradients is an inability technique where you um so the idea is you've got a bunch of inputs to the model like pixels in an image that you change um from like image one to image two you look at some output that changes like the probability of something being a dog and you then kind of incrementally you like take I don't know a thousand small linear steps between the two look at the gradients at each point and look at the integral of the gradient with respect to each of the pixels of the image to like attribute how much each pixel affects that output uh I've mostly seen it applied on input output things which as I've hopefully established I think is boring but the same idea totally transfers to like how does varying each of these neurons in parallel in a layer affect this output log prop or how does varying these neurons affect this neuron um it's honestly not obvious to me how I would apply integrated gradients to attention hits um you could kind because it tries to attribute things to scalers and heads are bigger than scalers you could kind of apply it to activation patterns um honestly I've seen basically no work trying to use integrated gradients with circuits um this is one paper looking for knowledge neurons in models claims to use it though they seem to not really use the power of it because rather than changing a bunch of neurons at once and attributing just change single neurons in isolation which doesn't tell you that much um I think it' be a cool project to try to figure out how to apply integrated gradients to II and see what happens um yeah I'm pretty excited about better exploring gradient based methods um because gradient based methods are like I'm excited about exploring gradient-based methods because gradient based methods actually can are just much more scalable it can give you more insight than just like does this directly affect the output um I just copied into the chat to the post I wrote on attribution patching which is a gradient based approximation to activation patching that I mentioned and I'm excited I'd be excited to see some projects trying to refine and build on that um in particular just like go and try to speedrun finding a bunch of circuits or take circuits we understand and try to understand what's going on there I have a company en code on a bunch of open problems in the post hello um it wasn't it wasn't so it wasn't like a question it was more like a a point of information I guess um so on the integrated gradient um there is this paper that have recently come across called which I don't remember the title anymore but something like tension flow and Transformers that's right yeah um they don't exactly talk about um um integrated gradients but you can look at the so they have two components there one is known as the attention roll out and the attention flow and you can basically like kind of reverse engineer um the attention flow to give you integrated gradients um there's also there's a library called captum um which I'll link here and captum does a lot of um integrated gradient stuff here this um I did work with them for a little bit but um but yeah uh might be might be thank you all right looks like Vincent has a question how would you do meup on a multimodal Model uh so two points first is that multimodal models like clip and multimodal models like Flamingo uh are two very different things I just copied in the chat the multimodal neurons paper from which is on a clip one with the weird neurons like the teenage neuron and so the way clip works is it basically takes in pictures it takes in text and it's basically a captioning model that checks whether the text is a good caption by converting both of them to vectors in some latent space and looking at how aligned they are and they mostly look at the image side and do a bunch of image techniques on it uh I think a very cool project would be trying to look on the text side and trying to apply any of the Transformer circuits techniques to understand what's going on so I have like not thought that hard about this and plausibly a bunch of stuff is weird who knows um then there's multimodal models like Flamingo which is a variant on chinella um which you can go read the paper on but they basically took a language model then shoved on some extra stuff that L it where you freeze all of the language model parameters you shov in some extra stuff that takes an images just like injects the images into the language models residual stream at some point and I'm still kind of shocked that this works because this is such a ridiculous thing to do and um yeah uh we don't know how gbd4 does this but like my money is on some variant of Flamingo and I believe someone recently open sourced a version of Flamingo and I think that know trying to understand things like can you find circuits in this model um can you see interesting things what the new heads you add are doing things like that can you use any of the image model techniques like feature visualization feel pretty exciting to me and yeah so General caution I highly recommend trying to do these projects on smaller models like Max 2 billion premises trying to work with like 20 billion plus parameter models is going to be an enormous nightmare unless you know what you're doing and have a lot of gpus and I don't actually know whether the open source Flamingo style models are small enough that I think it's tractable to try doing something on them um there is a reason I have sourced a bunch of toy language models tny models are great um all right yes doesn't look like there is any further questions shall we close there yes I think that sounds perfect and thank you very much for everyone yes to neand basically and we'll see if we run out of animal names for all these models before the singularity but uh um thank thank you so much for coming by and uh and it's been great having it let's give him a hand again and thank you so much yeah I'm very excited to see what stuff people produce over the weekend yes I just linked in the chat to a g for the previous hackathon that is probably integrated into the current materials it's like oh I think you can get enough momentum you could actually get somewhere in the weekend nice perfect good luck ch oh