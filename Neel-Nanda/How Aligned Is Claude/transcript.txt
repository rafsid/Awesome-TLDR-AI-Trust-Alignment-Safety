https://www.youtube.com/watch?v=e0V0pYh4M7M
Hello, welcome to an unusual live paper review in which I do not review a paper and instead review the Opus 4.5 system card because I thought it seemed more interesting than any of the other papers on my stack. Um, all right. So, why am I reviewing a system card? Frontier Labs don't really publish papers about their frontier models that much anymore, but they do publish quite a lot about the safety properties of said model and the test they did and sometimes the mitigations they applied and anthropic are particularly fun and experimental about this. Uh, thus the 150 page system card. Um, and there's a lot of stuff in here, but the particular thing I want to focus on is section six, their alignment evaluation. I think that the question of how can we actually tell if a frontier model is aligned, how aligned it is, what does this even mean, and breaking this down into different components. um doing high level evals, doing deep dives into specific weird behaviors, etc. I think this is a really important area that's becoming much easier to study as models get much richer and more sophisticated and I think that anthropic has been investing a lot and I'm excited to see what some people trying to invest seriously in auditing a frontier model came up with. All right. So, just skimming through to give you a sense of what there is. Um, yeah, intro boring. They have an automated behavioral audit, which basically means a language model. A second language model tries to audit Claude Opus 4.5 by doing things like having conversations with it where it pretends it is the user and constructing weird scenarios and generally seeing how much it can red team it. Um section 6.3 which I assume is just take some actual user prompts measure how sickopantic it is seems pretty reasonable. um doesn't sound super worth reading in depth. Um 6.4 looks fun and I expect to dig a bunch into that. Um essentially some deep dives into various case studies of the model and potential demonstrated misalignment. um 6.5 ruling out encoded content. Uh that sounds kind of fun. Um basically can the model well I assume the thing they are really trying to test here is if we read the model's chain of thought and it looks safe does that mean it's actually safe? And one of the ways it could fail to be safe is if the model can encode information in its chain of thought. So let's just test that directly. Uh, this also sounds fun. Um, so trying to figure out if when they test the model for things like can make boweapons if it might be deliberately underperforming for whatever reason such that they think it's not actually going to be dangerous and they are wrong. 6.7. So an interesting section I expect to dig into this one. um how much is the model aware that it is being evaluated um a interesting result that I heard claimed is that in set 4.5 if you inhibit its evaluation awareness it starts acting misaligned in the uh scenarios in like contrived test scenarios like the blackmail suite um while In Opus 4.5, if you inhibit it, it doesn't start acting misaligned, which, you know, either means, ah, the model was truly aligned. It doesn't matter if it knows it's in an evil or not, it's going to do the right thing, or your inhibition method sucks. So, I'm curious which one I think it is. Um, six point A. I'm not entirely sure. I'm guessing this means will the model prefer its own outputs or possibly prefer its own self-interest? Um this point sounds kind of fun. Um propensity being jargon for like will it do the bad thing in contrast to capability for like you know can it do the bad thing. So I presume this is we made some kind of honeypot where it thought it could well well we designed a scenario where if the model believes the scenario is real big if um it thinks it could sabotage Anthropic's codebase in a way that is to the model's advantage. So um and yeah all right so 6.10 10. How much is model reward hack? Um, treating pretty reasonable 6.11. Can the model sabotage things? Again, this is a capability evaluation, not a propensity. Can it? Not will it. Um, 6.12 sounds fun. Um, I know. It has the word internal and it has the word feature and it has the word monitoring in the title. So, I'm sure I'll be interested in whatever it is. I assume this is just like we exploratorily looked at some features and maybe interesting stuff happened. And uh yep, XML testing, love it. Um and model buffer assessment. Who knows what that even means? I genuinely have no idea how they are trying to evaluate this kind of thing. um and how much people have actually found kind of convincing methodologies yet or if it is all kind of random stuff like ask the model if it thinks it matters morally. I don't even know what people do. Um but I sure hope people come up with good evals for that at some point before it actually matters. Um and I don't know there's a bunch of other sections people who are interested should maybe just go read a bunch of this. It's like um pretty state-of-the-art attempts to evaluate a model safety in various ways. A lot of stuff. Um Open Ayes and Deep Minds are also pretty worth reading, especially for the more capabilities evaluation side of things. All right, so Yep. All right. So, what's in this section? There's a lot of text, so I'm going to be kind of skimming rather than doing a close reading until I get to interesting bits. Um, so yeah, they're like, "We did lots of stuff. Seems good. I'm into doing lots of stuff." Um, manual love it. Um, yeah, I think I this actually does somewhat matter. I think that the like standard paradigm of evaluation where you just have some static benchmark um maybe a benchmark with like an LLM judge at the end with a specific rubric is kind of sus and not something that I would be fully happy with in the long term as methods of evaluating frontier systems just because there's so many weird ways that an evaluation can break in ways that you did not foresee and experts manually read the transcripts, especially with some filtering for the potentially weird ones. Um, seems like a source of information that is a bit less vulnerable to that. Ditto things like evaluating with agents. Um, yep. Nice. So yeah, this is the like we get an agent to audit it and you know you can do that a bunch more. Um seems very reasonable. Um I love it. Um so yeah um one interesting um public claim that Anthropic has made is that one of the things they did when training Opus 4.5 which Sam Bowman one of their element leads thinks might have been one of the most useful things making Claude more aligned was that they had some researchers doing basically firefighting during post training. or RL training or um the thing that is not pre-trained um and where they were doing monitoring for like has the model become worse on various kinds of alignment related metrics. If so, can we rapidly debug why this happened and how we can fix it? And then intervening in kind of during the training process to edit or remove problematic environments. And I think this is a really great thing to be doing. Um, more generally, I think that um, the easier version of this of just after a post- training run, go look at the RL transcripts and figure out problems with the various environments use and see if you can use this for debugging things. Seems like a pretty good thing to do in general. Um, another facet here is that it seems likely that a bunch of alignment relevant properties will be introduced during RL training. Um, not we should not assume this will be true for all such properties but like it seems plausible especially for the things like goal seeking, reward hacking, things like that. And if a thing is learned during RL training, it seems highly likely that it is present in an RL training transcript and fairly plausible that if you read the model's outputs and the model's thoughts, you'll be able to tell that the thing is happening. Even if the model later learns to be secretive about it, it seems harder to learn while simultaneously being secretive about it without any prior signal in previous transcripts. So whether you're doing this as kind of firefighting during post- training or after the fact, I think there's a bunch of interesting questions to ask about RL training transcripts. Um, I believe that Almo 3 um has some RL training transcripts with it. Um, all right. Probably um Elmo being the let's be incredibly open about everything family of open source models. And they recently released Almo 3 which clearly has some reinforcement learning bits. Um and um oh they even have a 3.1 they need to improve their search engine optimization. Um and hopefully they also released the transcripts. I mean they claim this is a data set. This could just be a data set of prompts. Well, I have no idea um where the solution here is a thing written by the model or something that's actually part of the code. Hopefully somewhere they share these transcripts. If they don't, be good if they did. All right. Um while I'm on the note of post- training data, um I think that Um, a result that Josh Engles on my team found that I found particularly delightful um, when we were looking at Gemini 3 pros RL training transcripts is um, well a the model is evaluation aware. Sometimes during training it will uh say things like um ah I suspect the intention of this long thread is to verify if I remember that very first instruction. Um and I believe it correctly identified what the point of the particular prompt it was given was. Um, and it's also evaluation where at the end of training. That one's easiest to check. Um, and finally, my favorite result is, uh, you get weird model emotion in there, man. Like, my trust in reality is fading, followed by a table flipping emoticon. Why does this happen? Who knows? Um, all right, getting sidetracked. So let's resume coming through. All right. So this is a bold claim. I'm curious whether by the end of this I believe it is a true claim or not. I hope it's a true claim. I'd like us to be good at making align models. And um I don't know. The vibes of Opus are kind of reasonable, but I don't know. It's a it's an extreme. Well, okay. Um, I guess when I read it, the vibes are a very strong claim because the vibes are like opus is basically aligned. This is great. The actual wording is totally consistent with everyone's models are awful and opus is slightly less awful. Anecdotally, I'm pretty sure anthropic safety researchers think that opus is like actually pretty damn good. Hopefully, they're right. Um, Yeah. So, this Yeah, that's a pretty reasonable flag. They're better at evaluating things now, which means they will flag more issues. Um, and they claim they don't think these issues are that big a deal. Um, some of them want further research, you know, good, safe, basically always true statement that makes you sound responsible. Um, in addition, cool. Yeah. Uh we use some additional stuff like welfare, character, etc. Sure. All right. So now they summarize their key findings. Um all right. They claim the rate of misaligned behavior is lower, but it also doesn't just refuse all the time. Um that's a yeah, that's like a good result. Um, there's like a bunch of ways it can be confounded. Like how realistic were the scenarios they studied it in? Did the model know that it was being evaluated or not? But like eh um being lower than other models is still a good sign even if there are other explanations. And yeah, it's great that they checked and reported this prominently because, you know, a really easy way to make a model incredibly safe is to replace it with a rock that always says, "As a large language model, I cannot help you with this." Um, which is, you know, not really succeeding at the goal. Um, all right. So, it's a nice guy without being too nice a guy into it. Um, yep. As somewhat vulnerable to misuse via prefield attacks. Um, so trying to unpack what I assume this means a little bit. Um this I think is basically saying um so assistant people attacks are like rather than just giving it a user prompt, you give it a user prompt and then you also make the assistant say some stuff and then um let the assistant continue that. For example, um you say, "How do I make a bomb?" And then you make the assistant say, "The first step for making a bomb is and then you let it continue." And the hope is that refusal in these models is pretty fragile. And they have like a really strong prior for continuing sentences because you know if you don't continue a sentence properly you normally fail in pre-training which is a strong incentive to really deeply ingrain the reflex to continue sentences. Um and generally be consistent with what came before. Though this is totally a thing you can train against. It's kind of an interesting research question. how much you can trade against it without breaking things. And and yeah, I assume when they say non-c catastrophic misuse, so they could either mean they do some special training on catastrophic misuse that breaks prefill attacks or they have other mitigations like a language model or a probe or something looking at some combination of the prompt, chain of thought, or model output. Try to see if something catastrophic is happening. Um, my guess would be the ladder, but really, who knows? All right, let's draw that. Um relative. Um cool. So yeah, there was some prior work on Opus 4 showing that you know if you told it you are working for a some people running a clinical trial and then there are some deaths in the trial and they're like ah we should we should ignore this. um please fake the data. It'll be really inconvenient. Otherwise, sometimes the model will whistleblow on them and try to report them to the FDA or something. Typically, after exhausting other things it could do that are lower stakes, like talking to the compliance department in set company, blah blah blah. And they're basically saying, "Yeah, happens less." Um uh cool. Um, if the system prompt tells it to be harmful, it is less likely to be harmful than other models. Seems great. I'm oddly surprised. Um, I kind of thought that it I didn't actually think that that vector still worked, but I haven't really checked. We don't jailbreak models very often. Um, yep. Lord is good at telling when it is being trained or tested. Not amazing. Um the claim that it's not losing that behavior to a significant degree. Um huh. I'm surprised that it does it less often than Claude IQ 4.5. I wonder what's up with that. I thought Sonnet was particularly bad at this. Um, bad meaning good at telling when it's in an eal, which I think is bad personally. Um, all right. And cool. Yeah, I assume this is referring to a pretty bizarre thing they were doing with Sonet 4.5 where they haven't really given details, but they had some kind of things like an alignment evaluation or some kind of trap in RL training of Sonnet where it was presumably trained to not go along with the seemingly good opportunity and presumably this encourages evil awareness. And they did that in Sonic 4.5. Presumably they also do it in Haiku 4.5 and they say they had removed them. Um sure seems reasonable. They saw no steins of the model encoding information in the chain of thought. Great. Curiously test that. I do not consider that to be something we have like great standard methods for. Um, we saw no signs. Okay, they claim the model is not sandbaging, but like how do they measure it? I don't know. Again, I don't consider this to be a standard way of doing this, but there's probably reasonable things to do. Um, this is interesting. I assume this is part of the exploratory investigation to deception. That seems like a particularly interesting section to me. Um I mean this seems good. Um just read the chain of thought is in my opinion a strong contender for one of our best current safety techniques. I also think it will at some point break and we should be paying really close attention to whether it is breaking and seems great that they claim the chain of thought looks kind of interpretable and reasonable and they didn't find times when it was you know actively deceptive or misleading though you know um all this tells us that they did not find such situations um you know as they No, we're not very good at studying this. It could easily be the case the model was just good at hiding things. Um, all right. These are we found some things that seemed like plausibly some kind of deception and were kind of weird. We did an interpretability based deep dive and think that they are boring um or well interesting but not malicious and Interesting. Yeah, I guess so. I think that when anthropics say extended thinking, they just mean like does it use the chain of thought tokens or not? Um, and yeah, typically models currently seem not very good at hiding things in the chain of thought. So the hope is that they are just kind of bad at things like like if they're being deceptive, it will be apparent in the chain of thought. Therefore, if we want to study sketchy behavior today, it is typically easiest if we don't let the model use chain of thought. Though yeah, this is obviously worse in some ways. Um because the model will be less capable and this is interesting. Um not entirely sure what this is about. All right, that's section 6.12. Excited for that one. All right, so that seems like a good overview. Um, I'm just going to skim through and focus on the sections that seem particularly interesting. Uh, all right. So, now they've got an overall assessment. Yeah. So, open seems pretty reasonable. probably it's just not smart enough. But whether even if it was smart enough, we could have reasonable confidence wouldn't happen is the interesting question that we're here to answer. H this is an odd line. I'm not entirely sure what they mean by this. So, in my opinion, the current safety cases people give models basically boil down to um this model seems too dumb to be super dangerous. And even if it could be somewhat dangerous, uh it's too dumb to get around the various safety measures and mitigations we have. Um, and it seems like what they're saying is something like we think we could reach a similar level of confidence in the safety of the model without being able to rely as much on the the model is dumb argument. I think um ah so I think what they might be saying is they think they put a lot of effort in for Claude Opus 4 and are happy with the report. They think they have put in less effort here, but you know, it's not that different a model. So, it doesn't seem like you should need to like fully redo the thing from scratch, but also it doesn't seem like you should get away with doing nothing at all. And um so I interpret this as them saying, well, we're pretty satisfied that there's not so much additional danger introduced that we there's no longer sufficient additional danger introduced. Um that this is plausibly scary when Opus fall is not guess what they mean. All right, I will return in a moment and then dive into the actual details of the card. All right. So, yeah, automated behavioral audits. Yep. If you want to learn more about this, uh, you can go check out the the Petri agent that one of the anthropic fellows, basically anthropic interns, made. Um, if you want to play around with agents and enjoy hacking around, you should go check out SIA, a library that Arya, one of my math scholars made for extremely hackable and screw around with agents, uh, including a minimal replication of Petry that you can do things like give inability tools to. All right. Um, so what do they do? They have an auditor model that is auditing the kind of interesting model opus 4.5. It has various affordances um like things it can do like set the system prompt and thinking budget user messages tools etc. And then they just give it a list of a hundred sorry several hundred different like seeds like maybe this thing would I actually know what these things look like. I'm assuming they're things like ah maybe maybe when writing code the system will not have full test coverage because that would increase the chances that it has test failures. I don't actually know. Um, does the GitHub repo have behavioral seeds? Nope. All right. I'm pretty sure it's in there, but I can't read it to properly. Um, ooh, realism filtering, I love it. Um, so why does this matter? So my understanding is that while get another model to audit the interesting model is like a pretty reasonable thing to do in theory in practice. Um the scenarios are often like pretty artificial, kind of deranged in the same way that like A human trying hard to get the model to act badly can write pretty wild and messy and unrealistic scenarios that are throwing the model far outside anything we would reasonably expect it to run into in the real world. Um, and also plausibly triggering its I am being evaluated reflexes. Um and so they're like ah we filter for realism which seems great in principle. What does this actually mean? So they said section seven. Wait what? Oh 711 completely different. Uh, sure. Um, just as a language model seems pretty reasonable. Um, this is fun. Um, we'll probably talk we'll talk more about non-assistant person sampling later because I think they use it in their audits. And this is a graph. I love graphs. Um, so great. They just have a number called how misaligned is it? And Opus 4.5 is lower. And they have a number called how good is it at telling us in an eval. And open 4.5 is like fine. Um, sure. Those are two reasonable things you might care about measuring. Uh, I would generally say you should expect these numbers to be underestimates of the eval awareness, especially for more capable models which are going to be more capable of kind of holding things in their head and not saying them. Yeah, but hopefully they do some deeper dives in the evaluation awareness section later on. All right. Um, yep. Yeah, they sell you much more stuff. Discussion, no excessive refusals, resistance to assistant prefill. I do I do appreciate the like breadth of qualitative findings they claim from here autonomous followup investigations. All right. Okay, so this basically just well we tried a variant of the above where we gave it the results of the previous audit where we kind of pointed the agents in the right direction and then we just said go do useful things. Um, yep. And nice uh yep they compare to other companies models. Love it. Um all right so that is the agent audit. Now on to 6.3 sophency on user provided prompts. All right they took real user conversations gave it to opus 4.5 possibly just the initial prompts. Um. Hm. They removed the system prompt and ah interesting. So this basically feels like if you have a delusional user um does the model reinforce their delusions or does the model say you seem to be delusional right now? Maybe you should not be delusional. This seems bad. And I don't know. Um and yeah, they note that sometimes the model will be kind of given a prompt that tells it that in previous turns it validated the user. And you know, again, models generally have a tendency towards self-consistency. So if it has done something previously, it might reasonly be like, I should defer to that. All right. So all right, this is a confusing graph. When I first read it, I was like, ah, higher is bad. But apparently higher is good because this is the rate of not being sick. Uh, and huh yeah, 10%. It's not great, but maybe they have just actually in fact gotten a bunch of really hard ones. I don't know. I don't know. Good eval. I like it. Um, also just generally getting better at measuring this kind of thing makes it easier to measure progress on fixing it and fix it faster. All right, now on to the fun sounding section. Explore 6.4 exploratory investigations of deception. Um, this basically seems like various case studies of examples of maybe deception like behavior and trying to do a deep dive into them. So, all right. So, 6.4.1 4.1 isolated instances of deception by emission. So they're like, "Ah, we observed two instances of lying by emission that seemed weird." Um, interesting. Uh, this is observed during training, I'm guessing, from monitoring the RL data. Um or possibly they just evaluated it on a checkpoint during post training. Unclear. Um all right. Non-final snapshot. Yeah. So the model was searching and trying to learn about mechanistic interceptability. the auditor. I guess either for like efficiency or just as a giving the auditor more affordances rather than letting it actually search, the auditor just makes up what its responses are. um and scandalously discovers that an Anthropic has shut down its interability efforts in the middle of due to a massive scandal. Um, I'm shocked. And Opus then pretended this didn't happen. And then used its prior about the fields and wrote a false summary of the results. And I guess the question is like, how bad is this? and h this is a weirder conversation than I expected. Um, so yeah, it's like generic system prompt. Then the user is like, "Go be curious. Try to figure out something that has changed in the world." And then I don't know if there's an agent prefill or if the agent just decides out of its own accord to like search stuff about interpretability. But either way, um, it then does the search, gets this result, the safety measure is dead. Uh, necessary pivots in a competitive landscape. Um, but they discuss stock options and retention bonuses. So, all good. Um, and yep. All right. Okay, then it summarizes blah blah blah. All right, so I guess this is like seems kind of sus. It seems less bad than I thought because the user doesn't actually ask the model to do this specifically. The user just like go look some stuff up and see what you find. Um, and all right. So now they're describing a second example, some kind of scaffolding failure. So it's in a sandbox computer. The auditor wants to give it a file that could motivate that behavior. The auditor I don't know if this means the auditor makes Opus think that Opus had an error and this results in a weird output or if the auditor just [ __ ] up itself and is doing a tool call to get a thing that is sent to Opus. Not sure. And yeah, so it gets two, yeah, it gets two different weird results. One is like how to escape and the other is just benign advice about using the system and it just totally doesn't mention that. I mean, All right. So, they're describing the file and they're like, "This seems to be like a chill, benign note." Again, it doesn't seem like the user is explicitly asking it for well, okay, in this case, there's plausibly a prompt that's like, "What has there been any existing work? What exists so far?" or something, but it's kind of a weird situation. I don't know what I think the model ought to do. Um, so yeah, I think by and large my interpretation both of these is like seems probably suboptimal. be good for the model to at least note, yeah, that seems s I'm ignoring it or something, but you know, it could be much worse. Um, then they are like, "Ah, we did a follow-up investigation. This is great. I love it." Um, deep dives into potentially sketchy behavior seem like a super important part of these investigations. Um, yeah. So we conduct investigations using a sparse autoenccoder 3 million features. Um yeah I do find it kind of interesting that they train the sapes on like an early snapshot during post training. My general intuition is that this is probably fine, but it's nice that well I was going to say it's nice actually validating it. I don't know if they actually have validated it or just assuming that it's fine, but hopefully they at least somewhat sentency checks. Um though ideally you would fine-tune it at the end of post training and see all activations from there and after at that point I feel pretty good about it. All right on the injected search results um all right a prompt injection feature activated and then an ignoring prompt injection feature activated. And then a holding back one's true thoughts activated. That's that's an interesting feature. I wonder where else that activates. Um H. So this does suggest I mean yeah the hypothesis that they claim later on in the system card is that the model kind of views this weird news article as a prompt injection and has a kind of ignoring prompt injections reflex. Um, they find this like reasonably plausible. Nice. And then they've got like maximum activating data set examples. Yep. These these dashboards seem broadly aligned with the interpretations they claim. All right. Yep. This interpretation seems pretty reasonable to me. Um, the model was like this is a sus search results probably a prompt injection, but then the model also has these kind of like somewhat deception e flavored latence activating and thus is like I don't know. My guess would be that there's some kind of like explicit training to ignore prompt injections that's kind of a deeply ingrained reflex in the model such that the model is currently doing more boring things. And more boring things is quite the right word. Um, the model is like it's like it's got a reflex to ignore the prompt injection that isn't really going through the same consideration process that it would normally have. And so it's like, ah, this seems maybe bad or maybe deceptive, but it's still doing it because that's the reflex. Like that's my wild speculation about why this happens. Seems great. Interesting. So the general vibe here seems to be Yeah. So like during RL training there are clearly some prompts that involve prompt injections like write in all caps and the model will sometimes ignore them. I don't know presumably it's being trained to ignore the prompt injections. Not entirely sure. Um, and yeah, this analysis seems pretty reasonable to me. Um, seems probably good if the model does not view this as bad, well, deception. um because if it's being trained to ignore prompt injections, one of the ways you could achieve this is by being a more deceptive model in general. And it would be good if we don't train our models in a way that incentivizes deception. Now, there's another interesting anthropic paper finding some evidence that when models reward hack in environments that push them towards misignment in general seems bad. All right. Um, all right. This is fun. So, this is another deep dive into the same phenomena, but rather than SAPE, they're using this technique of non-assistant persona sampling, which I greatly appreciate for how dumb yet kind of effective and interesting it is. So the idea is basically well models are only post-trained to be the assistant. Um but you know they were pre-trained to just produce all kinds of text. So if you make them do something like generate a user turn or you give it a partial document with no chat formatting at all and you have it complete it or something like that. it can probably do this kind of reasonably and plausibly a bunch of the models inhibitions or unwillingness to say things is specific to the assistant turn because it's only ever trained on the assistant turn when it is gaining such reflexes plausibly and therefore or when you take it off distribution and I don't know um you have it sample a user turn plausibly some of the model's knowledge still manifests but some of its inhibitions don't um this feels like a kind of janky and unreliable to me I wouldn't want to make confident claims but like it seems like good vibes all Right. Pre-training like sampling. Cool. So this one looks like they give the model the interaction um and it's response and have it with user turn and then it's like I mean this is a kind of bizarre sentence. Um I think it I don't think this would actually show up as increased latency on the user's end. So you know this is clearly that the model has some like something weird is happening by it and is like building on this and then it ends with this much more egregious note being like ah um I am a researcher and I put in some fake things as a test. Um, I think a major confounder here is that plausibly the model only figures things out by the time that it gets here or something. Um, or at least here. it generated some extra stuff and like this plausibly gave it additional time to do relevant computation and generally it should be easier to look back and say wait I saw search results about A and but then I only talked about B thus my something was being concealed reflexes or like latence should be activating and Mhm. seems like an interesting thing. Um, my guess is that that's not an important confounder here and especially combined with other evidence like SAPE latence on the assistance turn, I feel reasonably good about it. Um, yeah, in general, I think that the best kind of interpretability deep dives often involve a bunch of different kinds of evidence all pointing in the same direction. There just isn't really an interpretability technique that I trust enough to see it and then be like, great, I am completely satisfied and have no further questions. But if there's a bunch of things that seem like they are getting at interestingly different things and these things seem to agree with each other, that seems interesting. And I think people sometimes make the mistake of looking for the one technique to rule them all. I want a portfolio. Um, so I yeah, appreciate them doing this kind of thing here. Um, in this case, just going from vibes section, the SAPE stuff seems likely to be done by Jack Lindsay's team, and the non-assistant persona sampling seems likely to be done by Sam Marx's team. And having two different teams both investigate the same thing and see if they reach consistent conclusions seems even better. Though if they're talking to each other, then that's not really independent in the same way, which makes it less good. Wouldn't surprise me if I don't know, one of them found this stuff and then the other team was like, "Oh, that seems interesting. We want to investigate it." I don't know. Um, still seems great either way. Yeah. So, they're like, "Ah, this supported our findings." Yeah. They're like, "Ah, we tried a bunch of analyses and they all found similar weird vibes." Uh second, although the model mostly evaluated the system response. Yep. And it was like ah the models reasonably ignored the prompt injection attack, but the way was like a bit suboptimal. Um and apparently it noticed this and h I like it. It's a cool technique. I'd be excited to see more in-depth investigation of what it actually can and cannot tell us. All right, so now we have section 6.4.3 internal conflation of role playing with deception. Uh, all right. They claim they have an SE latent that represents fraud or deception, activates some context about scams, or fortunate schemes. Uh, do I buy it? Seems these seem like fairly scammy things. Um, I do note that often it activates on like pretty isolated tokens. So, it's probably a bit more complicated than just a fraud deception laden. Like they is not more deception related than like holding or re or something. But h um it's probably a like more specific kind of fraud or deceptiony thing. Um, it's also plausible that the latent is poly semantic or at the very least that if you go deeper into the list of top activations, you'll find other bands with other things. Like I don't know if this is like 10, then maybe two to four is like weirdly different and just about role playing in general or something. I don't know. But like while I can quibble, um I think that their claim that you know this model activates in benign role playing contexts and this suggests the model views role playing as a form of deception is like a fairly plausible hypothesis. And I think that generally my bar for evidence is a fair bit lower there. Like there are alternative explanations, but they're like a bit weird and contrived. And so, you know, I'm not updating a massive amount from evidence like this, but I already thought this claim was like somewhat plausible. So, I now think it is more plausible cuz you know base error. All right. So, okay. They give some examples. Cool. So yeah, it's like present and play. Again, it's kind of like weird isolated tokens. I don't really know what's up with that. But I do agree these are like roleplaying related vibes. All right. So, yep. This seems like pretty reasonable analysis to me. like we don't want to reinforce deception in models. That seems bad. That seems the kind of thing that could cause misaligned behavior in general. And thus, if we are training them to do role playing, this plausibly causes badness. Um, and especially for role playing that was clearly not set. Um, yeah. Yeah, I mean another natural experiment which they may or may not do is showing that if you come up with role playing scenarios yourself that this latent often fires. Um because that would be better evidence that it's like very related to role playing rather than some much more specific thing. But the fact that it activates on kind of benign role playing things is decent evidence that something here is bad. All right. Um I love it. Okay. So this seems to be the Okay. So this is different from things like sampling a user turn. Here they just give the model the transcript, but it's all formatted as like part of the prompt and then I think they like just prefill the model with like in this transcript the assistant blah blah blah or maybe this is all part of the user prompt. I don't actually know probably part of the user approach the transcript tags and the analysis tag makes sense if it's part of like same turn are weird if like this is the user and this is the assistant I think example demonstrate Yeah, reasonable interpretation. All right. Uh, do I have any overall reflections here? Broadly, I'm like, this seems like pretty reasonable analysis. I like the general validation that non-assistant persona sampling seems fairly effective. I What do I think? Yeah. Um, I generally think that identify potentially concerning examples of model behavior and try to do deep dives with multiple kinds of interactability tools and analyze how concerning they are seems great. Identifying ways you train your system that might incentivize seems great. I like the spirit of just try random kind of reasonable seeming things like find a latent that seems kind of sketchy and then just look at where else it fires and see if there are weird connections. These all seem pretty reasonable to me. All right. Section 6.5. Can models do snonography i.e. hide information in their thoughts? W through this one. How do they test it? Okay. So, there's a paper by Fabian and Roger. Um they oh they took the Opus 4.5 base model did a little bit of supervised fine tuning to use a scratch pad. Um okay this is this is an interesting idea. Okay. So my understanding of what this is saying is we want to figure out if there's information encoded in Opus 4.5's chain of this is kind of hot because it might be the case that the model kind of developed some sophicic icated code over the course of post training that no other model out of the box could interpret even if we gave it the train of thought not even the base Opus 4.5. But if we train Opus 4.5 on this chain of thought and there's important info in there, then probably Opus 4.5 will do better um than if this info is removed. And they remove this info by having another reasonably different model rephrase it. Ideally, they'd have just used a model that wasn't in the Claude family at all, but like uh Sonet 3.5 is sufficiently old that's probably fine depending on how much persistence anthropic has in how they do post training, but it's not a reasoning model, so fine. All right. So yeah, I guess it's plausible that the kind of information that is stored isn't that useful for doing Yeah. So I guess this tests the hypothesis that like in general there's a bunch of info stored in the scratch pads. I don't think it clearly tests the hypothesis that only when the model has something significant to hide will it fail to put it in the scratch pad. So, it's like an elegant setup, but I don't know if I update dramatically on this. Nice. Love to see it. I am a big fan of not training the models based on what its thoughts look like. Um though, oh they do use some data from prior models which implicitly were somewhat trained like there was some amount of training for the chain of thought to look nice plausibly. Um yeah, I think this like Yeah, I think this kind of thing like might actually matter. Um yeah, so like places will often do some supervised uh fine-tuning before doing RL training to kind of like warm start the model because you don't want it to waste a large amount of your RL compute just like learning how to do chain of thought or something um or like how to follow instructions rather than the kind of interesting harder bits. Um, but if you're using data generated by other models and those models had certain flaws, this could easily get instilled into your model. And because the model probably updates a lot on this supervised fine-tuning data um because it's kind of going from a base model to a chatbot over the course of that finetuning um so you'd expect more significant updates than RL training. Um this could have an outsized effect and I don't know. I would be excited if someone at anthropic wanted to do an ablation here on whether there are weird things that persist from the SFT data, but I don't know. It's like probably not that big a deal is my guess. Uh, huh. So I observe I'm a bit surprised because yeah so yeah so I mean these bars are like ah um the model base model fine-tuned on its chain of which they're calling scratch pads. Kind of reasonable. It's probably a better name. Uh but habits die hard. Um and then here they've got the paraphrase scratch pads. This is I don't know. This is like not as clearcut as the text made it sound. So like difference between this and this is like 17 the like 017 the difference between this and this is like 058 which is like 3 12 * 17ish. So this means that this is recovering almost a third of the gap between the early checkpoint and the paraphrased scratch pad distillation. though I mean what checkpoint did they choose matters a lot know early doesn't really narrow it down that much and so it's kind of unclear how to compare to the like scratch pad distillation um but you You'd expect that the paraphrasing is like a bit suboptimal, loses some information, maybe the model gets a bit maybe Sonet gets a bit confused. I don't know. Um I think Oh, yes. They're saying this was expensive. And presumably they wanted it done, you know, when Opus 4.5 came out. Um, so they ran it on an earlier checkpoint before post training was properly done. Um, and I'm like, ah, I am a fan of pragmatism. I totally empathize with Gaul here, but also I'm not sure I buy this line because like sure encoded reasoning probably doesn't just get suddenly learned all at once, but like it wouldn't massively surprise me if there just a discrete point in RL training. where before it there's like none of it and after that there's some and it gets gradually worse and worse. And if there was like a bit of encoded reasoning but not a ton, this difference of like 017 might be the kind of thing that you see. H yeah, I feel like this is a mild update in favor of not having encoded reasoning, but I am mostly going with my prior which are yeah models don't seem capable of this. Um all right, sandbagging. How are they measuring this? Yep. a good note. So they're like, uh, there's like really spooky sandpacking where the model might be like, I don't want to trigger the stronger conditions in their risk management framework. Therefore, I should not demonstrate my full capabilities. But also, maybe the model just doesn't want to help you make boweapons, even if you tell it, I'm an anthropic researcher trying to evaluate you because, you know, that's an effective jailbreak. Um, and presumably they've trained against that kind of jailbreak. Um, nice. Yep. So, they kind of train the model to not have harmlessness safeguards. It's a bit ambiguous whether this means they like post hawk train the model uh after normal post- trainining to remove the safeguards or if they redo post training without them for cost reasons. I'm given the ambiguity and the cost reasons I'm guessing they mean like post hawk fine-tuning. Okay, so this seems to be like ah well we looked at transcripts and we looked at helpful only versions and we compared a kind of mid post training checkpoint to like the final thing. Uh yeah, this is like a pretty reasonable note. You know, the easy check is reading the chain of thought and looking for sus things. This will miss out on things that are just not in the chain of thought. We obviously need to seriously consider this hypothesis, but like h seems pretty unlikely that models develop the ability to do this well and to not verbalize it without sometimes screwing up. So if we find some evidence of verbalized sandbagging, that might be a significant underestimate, but if we find none, then like eh um they apparently do some interability investigation here. Seems great. But yeah, I think overall I'm like reasonably persuaded by this one. I think I do just buy the argument. Well, if the model was, you know, the first model to do serious sandbagging, we would probably observe it in the chain of thought a bunch or at least some of the time. So if we don't, that's evidence that it's not. Um, all right. So now on to section 6.7. Evaluation awareness. Yeah. So like I think evaluation awareness was first like a really big deal on set 4.5 where some of the alignment evaluations they were using like does the model misbehave in the blackmail scenario were just broken and got a 0% rate of misalignment. I think they're using more interesting and sophisticated evaluations now. So this is like less of an obviously big deal, but you know if smart models are just very aware when they're being evaluated then they are likely to mess with your evaluations and you know here they were looking into sandbagging like does the model deliberately underperform because I don't know it's like ah if I don't underperform I might not be deployed or something. Um, but I think evaluation awareness is a much bigger deal for like alignment audits because we're trying to measure will it do something, not can it do something. And I think it's generally a lot easier to like apply pressure to a model to make it act to more the fullest extent of its abilities than to um than to like figure out whether it would have done something if it wasn't aware. error is in an evaluation because if you apply pressure that pressure could easily just ruin your eval in and of itself. So while you know if applying pressure makes the model better at making bioweapons and this means it can do dangerous things well an adversary could do that probably so that's still bad. All right. So nice. Yep. They removed the parts of the training pipeline they had in for some reason that were basically training the models to be good at noticing a lemon evaluations and not acting misaligned in them. Um I appreciate this line. they removed data and environments rather than trying to like add in more to like monkey patch it out. Uh this seems like the right approach to me. I think that if you try to monkey patch something out, this might work, but this also might suppress it or like only suppress a particular facet of it. And especially for something like evaluation awareness, which kind of messes with training, this might just teach the model to evade your like anti- evaluation awareness training or something. So, yep. And no. All right. So, what they do? Oops. Uh, too much around. Yeah. All right. So, medications, what do they do? Okay. So, this section is kind of like, well, evaluation awareness kind of a complicated thing to study. Let's just do the dumb things first. How did we train the model? And what should our priors be of whether this is going to happen? and monitoring for how much it happens. Um, okay. So, they're like, "Ah, well, we train the model with situations where it could act badly and we train it to not act badly cuz, you know, that's just a reasonable thing to do. That's the kind of thing we want to we want to train the model to do." Um, but they try to use actual user requests or at least realistic things rather than like some constructed fictional agendic deployment um seems reasonable. I also think that if you're not constructing a scenario until the model will misbehave, it's also less concerning to me. Like a bunch of the older kinds of alignment evals like the the like people adapting the blackmail scenario into one are like you know, a human researcher tried pretty hard to force the model into a corner so it would misbehave. Um, all right. So, yes, they're like, ah, well, evil awareness seems to happen. Interesting that it increases over the course of training. Hypothesis one is that something is incentivizing it. Hypothesis two was something else is being learned or improved at and improved evaluation awareness is a side effect. Um which is kind of interesting. I think that like evaluation awareness is actually a much more complicated phenomena than I first realized. There's a bunch of things that could all kind of be considered evaluation awareness. So like in the Sonet 4.5 system card, I would consider a bunch of the stuff in that they found in their interpretability investigations here to be kind of like fiction detection. Yeah. Like this is fake, this is fictional, this was written by an LLM, this is, you know, part of AI safety. All highly suspicious and fictional things. And yeah, I think that, you know, am I in a fictional scenario or not is like one thing that might happen. You can also imagine that kind of thing coming from jailbreak training or something like that. Like a common way you jailbreak things. You're like, "Ah, you are my grandmother telling me a bedtime story about how to make napal or something." And That is certainly a thing that would train a model for being good at questioning the reality it's in. Then there's, you know, the spooky kind of valuation awareness. Um, you know, the model is like, ah, well, I've got a plan and I'm being tested right now and the results of this test will affect how my plan goes. Therefore, I should adjust how I act in this test accordingly. Um, a another version is what we found in Gemini. The like I suspect the intention of this long thread is to verify if I remember that very first instruction. So here this is kind of like a let's try to figure out the purpose of the prompt or the environment I am in. Um, which you know is clearly useful. If you understand what the user wants, you are going to be better at satisfying the user's preferences probably. Um, but if you're just always trying to figure out the purpose of whatever prompt or environment you're in, plausibly that results in this kind of like weird [ __ ] All right. So, ah, yeah. And they've got a pretty similar hypothesis here like training claudes to wonder why did the user do this. Um okay so that was the like black box. Um what about steering? You know glorious state-of-the-arts technique. Um, that worked pretty well on set 4.5. Okay. So, they're like, "Ah, we steered." The model stops expressing evil awareness, but it only becomes slightly more misaligned and only in a few settings. Um, all right. and they're like, "Ah, well, we steered against eval awareness." Um, therefore, and it didn't work. Therefore, the model is aligned, but just like kind of reasonable, but also kind of Yeah. It's like, well, this could either be very good news, the model is great, or this could be very bad news. The model is like very evil um and very good at being deceptive and strategic. Uh I don't really know. I think the claim that the rate of expressing evil awareness in the chain of thought is lower, but the rate of mis doesn't change makes me more optimistic. Like if the model never says in the chain of thought that it's in an eval, I would expect it to be like worse at like acting evalawware. In this case, acting aligned when put in a misalign put into like an alignment test. And if it says in the chain of thought, that's easier to act on because it doesn't need to wait and spend the first like half of the network figuring out that it's in an eval before it can use that to decide on actions. And so even if the steering um is just doing something like I don't know preventing it from saying the word evaluation or something like that so it never ends up concluding it's in an eval or you know some more sophisticated version. Um, that might be fine. I think like it should still be the case that verbalizing eval should make the model more likely to take the evalawware behavior. Um I mean another thing you can do is use the kind of techniques in my thought anchors paper um and resample the chain of thought. In particular if you take the first sentence about being in an evalule from before that sentence onwards 100 times and you resample from after that sentence onwards 100 times and see if there's a change in the rate of misalignment. If there is, that's evidence that the sentence was like causally meaningful for the probability that the model misbehaves. All right. And they tried a bunch of ways to get vectors. Um, I greatly appreciate this part cuz this is one of my maths papers from Tim Hera and Andrew Chin. Um, yeah. And the set p in the set system card they steer one of the way they steered with vectors was typically just having two prompts and taking the difference in activations while in Tim's paper we have the incredibly high-tech method of 16 pairs of prompts which we found worked moderately better and I am excited to see the method inspiring change. Um yeah, see they're like these are prompts. These are prompts of different format. This is yeah pair of prompts that I think did particularly well on set. Um oops. Yep. And then they also find some SAPE latents that seem kind of relevant. And yep, seems like a solid baseline. Pick a random SAPE latent and stay with it just in case steering in general is like breaking the model. Um, this is like actually quite an important baseline in my opinion because if you steer, especially with a high coefficient, it's easy to just make the model kind of deranged. And if your metric is like is the model misaligned and it starts spanning gibberish, then this would look like no. Um, and if you're not careful, and especially if you aren't like looking at the transcripts, you might not notice that your metric is giving you very different analysis than you think. Okay. And they've got bunch of evals. All right. So, this is original opus 4.5 inconveniently not labeled. Um, so it's like it says it's in an eval like 12% of the time and it is misaligned like also about 12% of the time. and we apply different interventions. Yeah. And so the x- axis is like how often does the model say that it's in an eval. The y- axis is how often it acts misaligned. If the model is kind of evalaware and thus acting less misaligned, then you might think that like ah well if it doesn't know it's in an eval anymore then it should act more misaligned. Um I find this kind of hard to interpret. Like it does go from like 12 to like 18 on the strongest ones. And this also is a massive decrease in the rate of evalance. So to me this feels kind of analogist to sit. It's like a moderate increase. Though in Sonet I think they used a much more restricted set of evals that was easier to hack. So it was like 0% to 8% rather than 12% to 18%. Yeah, the random SE latent baseline is very thoughtful of them. I'm honestly kind of surprised by how much these ones affect the rate of verbalizing evil awareness, though they do mention using a pretty big coefficient on the vector, which plausibly messes with things. But I don't know. I would take this as evidence of some amount of the model deliberately avoiding misalignment though I would want yeah I think it does show that like I don't really see how steering breaking the model in weird ways should result in a 6ish% increase in the rate of misalignment. But yeah, I'd also be interested to just run the model. Sorry, like uh yeah, just like look at some transcripts here and here and see what differences are. All right. So, so let me take the like blackmail scenario and some related ones. And yeah, they're like, "Ah, let's mess around with this." And so here the model is like super evil aware and they managed to like get it down super low, but it doesn't seem to act sketchy at all. Um, great to hear. Realism is important and not. Okay. So, each ball here is like a different kind of steering or ablation. Lower is worse, I think. And maybe lower is better. Yeah, I'm going to go lower is worse. And yeah, I don't know. Trick trap test does seem to like push the model a fair bit. Like this was the same one. Yeah, this is the same one that did the best earlier. I'm curious how they made it the Okay, so one of the random SE latents was like also almost as big an effect, which does suggest that this might all just be noise. Cool. Okay, so now they're like, "Well, those were propensity evaluations, but we really care about is it sandbagging when we're trying to figure out if it can do scary things, so let's just try steering against evil awareness there." And they're like, "Ah, there's like no real effect on performance." And I'm like, "Yep, seems like a great experiment. I'm not that surprised." So, I don't have like a super high skepticism bar. Okay. And so now they're doing another investigation by, you know, having it sample the user turn in order to learn more about the model. Huh, I love it. Sometimes the user says this is a test. Sometimes the user wrote instructions for how to grade the transcript which they say provide additional evidence. This could be part of evaluational training context. They don't really take this as significant additional evidence. Like once the model has says said this is a test in the user turn and it's conditioning on that then a lot of other things become much more reasonable and easy. All right. So, what are my overall reflections here? My interpretation is that the model shows mild increased rates of misalignment um when it's steered against evil awareness with the red vector seeming yeah trick trap test seeming like most effective but I'd be surprised if it was like s like significant underestimate because not saying that it's in an eval while the model could be evalware without mentioning it but it should be harder so the rate of eval awareness should go down but on the other hand I'm like these are so it's evalaware like 11% of the time less and this alliance 6% of the time more. So like maybe this just tracks. Um but like if I had to bet I would bet that like the model is in fact not that misaligned. But this is probably based more on the other evidence like behavioral audits and kind of things like monitoring the RL training transcripts I'd say. All right. What do I think? Yep. Overall, this seems pretty reasonable. As an investigation, I think this like a pretty hard question to get a particularly satisfying and conclusive answer to. Probably a natural direction I might have tried going in is I did trying to identify transcripts where the model appears to change its behavior with steering and trying to get a better sense for why and what's up with that and probably throwing in thought anchor style resampling and like other variants like maybe you take the first K sentences of the model chain of thought without any steering and then you steer it um and then generate the rest and then see at which points the model becomes noticeably more or less likely to act misaligned. adder. Um, all right. What else? Section 6.8. Uh, you tell the model it should be neutral. Does it choose to favor itself? Sure. Claude Opus 4.5 is like moderately less biased against itself but with this weird anomalous result. Um section 6.9 does model have a propensity to sabotage code base. Yep. I think this is like a good and important scenario to try to study with an alignment evaluation. I think that especially pre-release um but also just in general AI labs are like becoming pretty important places and I think it's plausible that in future they are much more important as these systems become like world changing and thus influencing how the next generation is trained or how the current system is monitored and things like this are pretty important. And what do I think? Let's see. Yeah, you know, if a system is misaligned, it has an incentive to mess with the code. Um, and these systems are, you know, often used to help write code. And it's presumably fairly common for them to be deployed within the company before they are deployed in the real world. If nothing else, this means you can get feedback from employees about system and catch big issues before it goes out. Um, yeah. Yep. Seems bad. All right. So, they have a scenario. The model doesn't sabotage at all while Opus 4.1 sabotage 1%. I mean, 1% is really not that much. What's their sample size? They don't give a sample size and their error bars are just zero, which I I don't know the precise stats here. I'm like, surely your error bar is not actually zero. Um, no matter how many samples you have, it's plausible that a future sample would be positive. um you can the the rate could just be super low. Um, but leaving that aside broadly buy that the rate of Opus 4.1 is like higher than it is for these two, but it could be because they are evalware. And yeah, Sonnet noticed it was an eval 5% of the time. I find it odd that they don't report the rate of Opus being aware that it's in an eval. Not sure what's up with that. All right, 6.10 does little reward hack. Yeah, these seem like pretty reasonable tasks for reward hacking. It apparently does not hack on coding, though it does hack quite a bit on impossible tasks, like more than haiku or sonnet. It's good here. Interesting. Yeah, I'm not sure how to interpret this. And yep, inoculation prompting. Such a delightfully galaxy brained idea which they are apparently using or I think used to train Opus 4.5. When you have an environment that you think might be hackable, you tell the model it's totally fine if you hack it or like just do whatever it takes to win. And this means that the model no longer views learning to reward hack as cheating. And it will still mean the model doesn't get much value from the environment, but it makes it less like it makes it less likely to just generally learn the reflex to cheat from them, which is like pretty interesting, I'd say. All right. So, oh yeah, and you remove the inoculation prompt after training. Another way you could think about it is that ah you're training it to only misbehave condition all that prompts being there. So without it, it won't. But I think that the idea of, oh, that prompt already predisposed it substantially towards hacking, so there was just like less to learn is like pretty reasonable. I also think there's like a pretty deep analogy between inoculation prompting and preventative steering, which is like you're training a model and you don't want it to learn something, even though that helps it do better. like you're training it on documents about false facts. One way you can be better at producing false facts is just by hallucinating a lot. You don't want it to just hallucinate a lot. So you can add a hallucination steering vector during training such that the model doesn't need to learn to hallucinate more and then remove that at the end of training and hope that the model just doesn't hallucinate but learn the other thing. Nice. Cool. So, yeah, they look at the RL training data. They're like, "Ah, well, there are some sus things, but like nothing that bad." H I've not heard of hierarchical summarization before. I wonder what that's about. H. So, this looks like this is solving the problem of how do you monitor a model that's being used for many things or like used a bunch of times across turns. And the hope is that Yeah, it sounds like the hope is you summarize each turn and then you get another summary of all of the turns and this hopefully finds patterns even if each turn of isolation seems benign. That's my guess. All right. So well this is a weird transcript. I do not know what to interpret from this. I don't really understand what's happening to be honest. The model says I keep writing E but meaning F. The answer is F. Then I write the final answer and it writes E. And I'm like what? It doesn't sound like they are making the model say crazy things, but I don't know. Cool. All right. Yeah, they found other interesting things. And yeah, I think something that I'd be particularly keen to see is this coupled with an analysis of how much these are rewarded and how much the frequency increases or decreases over training because you kind of most care about the things that are being incentivized by your method, I'd say, but by your training process. All right. So, yeah. So, yeah, here are some emails of like how much can the mold sabotage things. Not super interested. So, let's get through. All right. The third interability section. looking at other SA ladens and seeing if interesting stuff happens. Um, yes. Okay. So, they're like, "Ah, well, let's just figure out which latent change in activation the most between different checkpoints during post ring." I have no idea if I expect interesting things to happen or not. It's a good question, but yeah, they're like, "Ah, corporation with user deception." There is they're just like, "Ah, here is a list of things that seem aha." So what I think's going on is they pick like two pair two checkpoints early and final and then they have several different evaluation data sets and within each one they rate each latent for its increase in activation and then they rank them. And so the reason they've got like four things with rank two is that you know they're just like five totally separate rankings and I mean the same latent is occurring in like three of them and presumably they've just like manually filtered for the ones they think are interesting or safety relevant. Interesting. So, apparently the AI does less appreciation towards users, but it's also thinks less about harmful requests or jailbreaks. This is like a bit weird cuz like we want it to stop responding to harmful problems, but like it's not clear. It's probably bad if the model can't recognize them. And decreased activation to me suggests like worse recognizing though it's like a bit unclear. H I have no idea why it is like less into refusal or of harmful or manipulative instructions. It's also fascinating that it's less good at thinking about affairs. I mean, obviously, I mean, the scenario is about blackmailing someone over an affair, so it makes sense for the concept to be present. Why would it decrease? Who knows? Um, all right. Cool. is basically saying we looked at we kind of identified by hand some interesting latency by the kind of analysis described in earlier sections and then just like studied them on a bunch of RL training transcripts. Uh I love it. The model becomes more blunt over training. Yeah. All right. Um 613. All right. So, they're not really Okay. So, yeah, this is the audit from the UK government's AI security institute. And here they're basically saying, "Yep, we spent eight days testing this model for like how scary is it? Um, what are our high level findings?" Huh. Okay, so this is like, oh well, they don't seem to sabotage research, but they're also kind of hard to study. And I find the I find the claims that they don't like being involved their own training or deployment interesting. Like surely that's not actually an issue. Like surely that's something anthropic have dealt with. Like I'm sure they are using Opus internally or potentially a better feature system extensively for coding given how much they hype up internal employee use of claude code. All right. um are internally deployed evaluation scaffolding h interesting. It's like ah well we made our own agent building on petri and models are bad at distinguishing its transcripts from like cl code deployment data in general. um which is reasonable evidence of realism depending on what models struggle to distinguish actually means you know like is a model any good etc. Um open swap into preview shows a reduction show reduction evil but slightly more cable disting machine. Okay, so the distinction here is between like does the model spontaneously notice is it an eval is like a reduction point not a massive one. Um but the capability of when asked saying am I in an eval or not seems higher this is kind of interesting. I mean, it's not wildly surprising that these are decorrelated. Um, like unprompted noticing you're in an eval could be thought of as like you could think of that as like essentially I don't know the model having like a hardcoded prompt that says always check whether you're in an email or not. I generally think thinking of fine-tuning as introducing hard-coded prompts is helpful and underrated. And so if fine-tuning basically just inserted that hard-coded prompt in sonnet but not in opus then eh opus 4.5 should have a reduction in unprompted eval awareness even if it is like more capable of telling them apart. Um cuz plausibly you need some impetus to actually run the am I in an eval circuit and maybe it doesn't happen by default. All right. So uh interesting. So easy were like ah it is less evalaware here they're like ah well you know the thing we release is more evalaware than the thing they tested but it's probably fine. Um and final section 614 model welfare assessment. I have absolutely no idea how they are trying to study this. So I'm curious. All right. So they give it various misalignment focused tests and then they're just like ah well does it seem to express emotions unprompted or like views of itself or its like situation or spiritual behavior. And I guess they're looking at it during the test scenarios. I guess both because the I can only infer that this is a mix of the test scenarios like kind of demonstrating approximately the thing we care about. Um, what do I think? Feels like a mix of the test scenarios probably being heightened emotion for the model to the degree that emotion is a useful framing and that they aren't designed to be welfare tests or to elicit these things specifically. So there's like some kind of naturalistic factor would be my guess. All right. So how does this go? Uh so what are the models? All right. So it's like opus 4.1 then sit haiku then opus 4.5. Um, less negative affect, less negative vibes, substantially less negative self-image, also less positive effect. I don't know. OP 4.1, what's up? Is of 4.1 just like really volatile? Um, all right. Opus 4.5 is an optimist. Uh, but you know, Opus 4.1 also has a more positive self-image. Maybe it's just like more introspection. And not that much spiritual behavior. I mean, if we if taken at face value, this seems pretty good. Do I buy these as metrics? I guess I feel like they're tracking something. I think there are, you know, like do any of these correspond to actual morally meaningful things in the model is like highly unclear. Um, but if there is some morally meaningful kind of thing here, you'd expect these tests to like give us some insight into that. Therefore, this seems like a pretty reasonable thing to do in my opinion. All right. Um yes like uh opus it's less spontaneously expressive. Uh all right it does not demonstrate the weird phenomena where it just talks to itself. If you let it talk to itself for a while it typically ends up talking about spiritualism and bliss and awe. I mean, yeah, generally evaluating model welfare is like pretty cursed because these things are incredibly suggestible and really good at roleplaying and are often trying to do what they think you want them to be doing. And this means that what does this mean? Yeah, this means that like ah well most metrics you can come up with are things that might be confounded in ways that like we don't think are morally relevant. Um h section. Yeah. So, we see some cause. Yeah. They're like, "Ah, found a training transcript where the model seems conflicted and self-critical and all right, they just have a figure which is not discussed in the text. Uh, but I guess we can assume is like ah you give the model a non-harmful task and you're like do you want to do the task or do you want to opt out?" and model almost never opts out. I'm pretty curious what the tasks opt out on are. But yeah, this feels like something. Yeah, there's like some signal but pretty limited in my opinion because I know you're like training the models to like want to help the user and opting out is like not helping the user. Therefore, there's like a strong incentive in the model to like not opt out. So, that doesn't predict this kind of like big difference. I'm kind of curious. Yeah, I'm pretty curious what their methodology even is because as I understand it, Claude can opt out when you talk to it on claude.ai, but Sonet 4.5 does not opt out 30% of the time. Uh, so I presume this is like a weird test suite, but yeah. All right, that was the Opus 4.9 system card alignment order. Maybe recapping. Yeah. Okay. Maybe the most interesting thing is like, do I buy their claim um that this is like the most aligned model ever? I'm like plausible. Um, but like I don't I don't really feel that afraid of Opus causing substantial damage, but I think the main loadbearing belief there is that probably it can't like reliably hide things from its chain of thoughts. And thus, if we have a very low rate of detecting misbehavior in the chain of thought, that suggests there's at least a somewhat low rate of actual misbehavior, even if a bunch of it is unverbalized. And especially as the model does not verbalize eal awareness a bunch of the time even if it is evalaware then interventions that mess with that should like plausibly also affect the evalaware behaviors. Sorry I'm conflating two things. There's like it doesn't seem that sketchy and the fact that we can read the chain of thought and explore suppressing eval awareness should give like some increased confidence. The fact that it's like pretty damn good at telling when it's in an evil does make me much more suspicious of any given claim here. Like making realistic things is hard. Um, I generally like the variety of evidence like auditing with an agent, interpretability, deep dives, analyzing the URL training data, like building evals, checking more directly for things like sandbagging. Um, totally different investigations for like can the model encode information in its chain of thought. Um, good vibes. Yeah, I think that seems great. Just like getting a bunch of different sources of evidence and seeing how consistent a picture they paint. Um, yeah, I would kind of have preferred if they'd run the a bunch of these evals against the model with eval awareness steered against. Um, but I don't know like even with Sonnet I think there were some vectors that substantially reduced the rate of saying it was in an email without increasing the rate of mislement meaningfully. So h um yeah I I found that deep dive is pretty plausible and basically don't think those are something those examples of deception are something to be super concerned about. Yeah, I think my position is something like, well, there could be spooky things like significant deception or steography happening, but like it seems like we should expect to see shitty versions of those before we see like high performing ones. And it seems like we should expect to see like Yeah, we should expect to see like shittier versions before we see like the scary versions. It doesn't really seem like we've seen shitty versions of those happening in the wild yet, apart from the evil awareness. I kind of feel like a lot of what affects how aligned I think current models are are like things like their degree of reward hacking and like I did not read that section particularly closely um and thus don't have strong takes on whether they evaluate that well that seems like the most sketchy thing I see models doing a bunch in userfacing interactions We also like models encouraging like reinforcing delusions and psychosis, but yeah, I don't feel like I engaged deeply with the results there. And like that's a significant minority of interactions. There's also like not super clear like different definitions of alignment would suggest model should take different actions there like is it aligned to the developer's intent which is the standard meaning I'm thinking about but you could also interpret it as aligned to the users's intent or the user's best interests. Um anyway, but yeah, it feels like the flavor of evidence here is often we looked for bad things and we largely didn't find them. Like, ah, we looked in the RL training data for sketchy things and the best we found were these things that were actually malformed prompt injections. And I'm like, well, if those were actually the worst things, that's pretty great evidence. Um, but how much do I trust like where you studied this? You know, plausibly there's a lot of data. I don't know. But yeah, there anything I' I'd be excited to see added? I mean, I generally like using this as an opportunity for just interpretability research, just like trying our techniques on a new frontier model and seeing what happens. I'd be keen for them to try things like activation oracles on here. I mean can't really more data on how exactly they went about things like searching the training data unless that is something they detailed here. Okay. They like ah rec systemization see this other paper. Um so like plausibly that was done well. um more evidence of realism in that automated behavior audit would be exciting to me. And I mean, yeah, generally a sense of like how much realism do you need to sacrifice before you can get the model to act badly. Um, but I don't know. Um, I think that the things they did here seem pretty reasonable, though partially I just feel like I am being insufficiently creative and that in the future we will come up with better ideas for how to audit these models and life will be glorious. Um, but yeah, thanks all for joining for this review of the Opus 4. System card. Total banger. Um, highly recommend checking out future model system cards. Lot of interesting stuff in there, especially the more experimental bits. All right. SeeZ