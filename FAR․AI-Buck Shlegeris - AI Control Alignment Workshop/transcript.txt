Source: https://www.youtube.com/watch?v=JZYjz7D_auw
Transcribed: 2025-12-30 15:15:36
Method: YouTube Transcript API
==================================================

today I want to talk about uh AI Control uh which is an approach for mitigating catastrophic misalignment Risk by assuming misalignment so in this talk I'm going to talk about the scheming threat model which is the threat model we're trying to address here I'm going to talk about AI Control as an approach to mitigating those risks I'm going to talk about what it looks like I going to talk about our plans for promoting AI Control and how you can help with them all right so I want to talk about uh this threat model that I'm going to call scheming so by scheming I mean AIS that are misaligned and power seeking I'm thinking about AIS that are strategically pursuing some long-term goal uh that goes against our interests and that they're going to pursue power in order to achieve uh such models might try to do things like faking their alignment when you ask them questions uh they might say that they're aligned with you when you put them in fake opportunities to take bad actions they might uh act aligned in the hope that you will deploy them with certain affordances later they might act incompetent suppose you're worried that these AIS will be able to hack you if given a certain affordance then when you do capability EV ation to see how good they are at using that affordance to hack you they might act incompetent even though they aren't actually incompetent they might sabotage the work they're doing they might sabot if they're doing programming work for you on infrastructure they might sabotage it and add security vulnerabilities if they're doing research on AI safety they might sabotage that getting the wrong results hoping that you'll use bad alignment techniques uh and then they might Escape when possible they might try to exfiltrate their weights they might try to hack your data center and do lots of unauthorized work so in my opinion uh this is the main source of catastrophic risk from AI uh and this is basically just because a lot of there's there's a lot of other sources of risk with AI this one's just particularly thorny uh because it disguises itself like if the AIS are misaligned then they're if they're you know strategically misaligned if they're schemers uh they're going to try to sabotage your efforts to learn that the problem is present they're going to try to sabotage and get around your safeguards they're going to try to sabotage your efforts to understand the problem uh and that just seems like overall you know not unresolvable uh but potentially pretty rough uh sir why would you think that the models might in fact be so egregiously misaligned and so strategic about it there's basically theoretical arguments and empirical arguments rough theoretical argument is if the AIS are very smart it's probably pretty easy for them to do this the way that machine Learning Works is you train models based on their behavior uh or like this is 90% true the way that you do machine learning is you train models based on their behavior if your models are very good at faking alignment then there's no particular gradient pushing your models to become aligned if they start out misaligned uh I think this is like not the best argument in the world seems reasonable uh Carl Smith 2023 is my favorite source for this there's also empirical evidence which um my co-worker Ryan greenblat did with help from some people at anthropic demonstrating that Claude sometimes alignment fakes in pursuit of long-term goals which I think is pretty spooky right like anthropic are not known for not trying at all on making the AI aligned but that thing will you know totally think about like how to how to how to mess up anthropic if you uh you know prompt it the right way um various caveats obviously that research should come out soon pretty excited for it um overall here is my probability that scheming comes up at various points unfortunately we don't have a shared vocabulary of AI capability levels so I kind of have to introduce some here in order to talk about this the AIS that are capable of 10 Xing the speed of AI R&D I think it's 20% likely those ones are scheming AIS that are capable enough to basically obsolete top humans and all tasks 35% likely scheming uh sixs effective compute above that 55% so looking at these numbers they're not like zero they're not like one kind of in the middle as numbers go um I would say so from my perspective the AIS that are uh capable enough to 10x AI R&D labor speed I think those AIS are like not that scary uh I don't put very much of my effort into reducing risk from those um I think the majority of the risk comes from the latter two categories uh the work that I'm going to be pitching here basically doesn't help at all with the Lost source of risk so it's targeting the risk which is more like from AIS that are uh capable enough that they could obsolete top humans so how are we going to handle risk from scheming well we could use some probability theory note that uh the probability of Doom from scheming where I'm using Doom colloquially because it's the traditional variable name for uh you know bad outcomes that we want to avoid um but I'm I'm not necessarily meaning everyone dies but um you know pick pick your favorite catastrophic outcome uh P doom and scheming equals P scheming times P Doom given scheming uh there's you know two obvious ways we could try to address this uh as suggested by this factorization the first one is you could try to intervene on P scheming so you might think that given there's this whole field called AI alignment people would research this my sense is that basically no research on reducing PE scheming is like actually done uh I could name a couple of research projects that are happening that are sort of related to reducing PE scheming there's definitely research projects which are you know there's like they're tackling one mechanism by which scheming might arise and like there's like sort of a case for why you know if you improve your reward signals maybe scheming is Le less likely to arise and I think these arguments are okay there's more research on things which might eventually be able to let us learn about whether our models are scheming which is of course different from intervening on uh weather model scheme but yeah I don't think this problem is impossible to work on uh when I was first preparing this talk I was considering saying you know people don't research peace scheming or reducing peace scheming because it's intractable I don't think this is true I think this work you could do here but it's basically pretty painful to do and people don't do it very much and I don't think we're going to be able to rely on uh interventions that reduce peace scheming in the future I think that a pretty good focus is instead to think about P Doom given scheming uh and I'm going to introduce the term controlled to mean a model is controlled if we'll probably be okay even if this model is scheming against us and I'm going to say that I think a very promising uh approach for mitigating risk from scheming is to try to ensure that your AIS are controlled so why would we aim for controlling AIS the basic reason that aiming for controlling AIS seems good from my perspective is that from the control perspective when you're developing safety measures all your threat modeling just becomes questions about model capabilities which are much easier to answer like the thing which is basically rough about intervening on P scheming is that you have all these problems about inductive biases about generalization that machine learning is very bad at answering uh machine learning is also not amazingly good at answering the question of how good are models at this task or that task but it's a lot better basically because the one affordance that is built into models based on machine learning is the ability to gradient desent them uh such that eliciting their capabilities is just much more conceptually simple it's basically the the one thing you can do um so from this adversarial perspec I when we're thinking about whether a particular set of safety measures suffice in a particular situation we just have to ask ourselves the question are the a capable of doing bad things of causing safety failures uh given these safety measures so that's the methodological reason to think that uh control techniques are easier to research but of course this isn't helpful if the control techniques don't suffice to reduce risk which unfortunately is the case for very powerful AIS control techniques totally useless as far or like my guess is totally useless once you're building sufficiently powerful AIS but for the AIS that are as capable uh capable of obsoleting top human experts my guess is that control techniques will actually be able to substantially reduce the risk uh which I think overall seems pretty good even though the ASI question or the artificial super intelligence question is still pretty rough this is kind of analogous to how you manage Insider threat so if you're a powerful AI lab you know and you're You're Building um You're Building AIS that can uh obsolete top experts one problem you might have is that your AIS are kind to escape but another problem you might have is that you probably employ a bunch of spies who like spend a lot of their time thinking about how they're going to steal your models how they they're going to use the compute that they have access to inside the data center that they have you know to do their jobs to do a bunch of inference that you don't like um they're going to be thinking about how how to get around all these saf the all these security measures of yours um and the way that people traditionally try to handle Insider threats is not to try to ensure that they never hire anyone who is going to be compromised uh people do do some of this but they mostly give up on this like if you're Google you just um you know Google has done a lot of the research on how to have software engineering operate at scale while being robust Insider threat and the way they do this is by assuming compromise of employees and saying it's unacceptable if an individual Google employee is able to read your Gmail or whatever uh and this has worked out reasonably well for them uh I think Gmail is you know pretty pretty secure compared to a lot of other things uh and I I think that's like I think that uh assuming adversarial intent as part of ensuring security works well for basically the same reason why I'm optimistic about control um so I want to talk about what it looks like to ensure control talk about a couple of control techniques um so one basic concept which I think is pretty useful here is uh I currently don't think that uh Claude 3.5 Sonet is Rel is uh coherently and consistently scheming against us I know 3.5 son it pretty well it's uh it's one of my best friends as measured by how much time I spend talking to it and I would just be like flabbergasted if whenever I talk to Sonet you know it's like is this a good opportunity to strike and then it's like ah no it's just this Buck guy talking to me again I guess I'd better just uh you know answer his question about whatever like like there's just no way sonnet is consistent is able to consistently reason through uh misaligned objectives such that my observations are consistent with it uh striking very strategically if opportunities arise and this is Claude 3.5 Sonet you know its weights are just on anthropic servers and anyone who hacks anthropic servers and they're just going to stay there uh and this those weights are still going to be there at the point in the future where we've built models that are powerful enough that it is conceivable that they're scheming against us and so at this point in the future we're going to have access to you know one way of thinking about it is three sources of Labor you've got humans who are uh reasonably trustworthy very expensive and reasonably competent you're going to have access to your trusted models like 3.5 Sonet that are very cheap not very competent and very uh and trusted uh like we don't we think they aren't consistently scheming against us and you're also going to have access to these untrusted models thanks to open AI for kindly donating the art uh for this um so what can we do um here's here's like one example of a control protocol that we pres uh have investigated in a number of settings so trusted monitoring we want to solve some problem first we get a proposed uh provisional answer by asking the untrusted model to solve the problem then we ask the trusted monitor uh The Trusted model to review the proposed answer to this problem and then if the score uh produced by this trusted monitor is above some threshold then we're going to ask a human whether this action was particularly sketchy and if so we're going to raise some bad action exception and something's going to happen uh so this is a strategy you can follow um here's a graph that we got in one particular context uh for uh a a setting where we had a occurred backdooring task where we tried to get uh gp4 to produce back door code uh and then we asked GPT 3.5 to try to distinguish the back door code from the non-bo code so you can see here you can see here that uh the distributions are like moderately separated uh this is this this classifier is definitely better than flipping a coin uh however uh where the threshold that we have to use use for forwarding a problem to a human is of course determined by human labor suppose we have enough human labor that we can look at 2% of the things that the untrusted model does which is by the way an insanely large amount of uh human labor to assume that you have at open AI like uh they could afford to you know they could only afford to look at orders of magnitude less of their inference traffic than 2% uh but yeah so so we have to have a classifier with a threshold set to have a false positive rate of 2% in order to use up all our budget but no more than our budget uh and you can see that at this threshold trusted monitoring is not very good uh it's uh inputs that are back doored are normally not caught by trusted monitoring so this was our Baseline strategy it's better than even worse baselines but it's not it doesn't uh it doesn't get you very much safety um in the particular setting we experimented in um so here's a strategy you can do which has an interesting uh which which improves on upon this or doesn't prto improve on it but let you out a pretty good curve uh new strategy when the untrusted model produces some curd uh same as before if we can if it's so sketchy that we should have a human look at it uh we have the human look at it so so for the top 2% of things we're going to show them to a human uh but then for the next 8% of sketchy things we're going to throw away what the untrusted model proposed and then we're going to get uh our trusted model to solve the problem instead so this is actually a great strategy or this strategy allows you to greatly decrease danger while only partially degrading performance and the basic reason for this is when I throw away this 8% of uh Solutions I'm only going to be reducing my performance by something like 8% of the difference between the trusted model and the untrusted models performance but I'm going to be reducing danger by drastically more than that this is basically letting us use the fact that the ca that the classifier has uh signal at thresholds other than the signal which we're using to decide whether humans can audit something thing I'll give another example of a technique you can use uh untrusted monitoring with collusion busting so here's a strategy instead of asking the trusted model to tell you whether proposed code was bad or not you can ask another copy of the untrusted model to tell you whether the code was bad can anyone think of any problems with this strategy H yeah they can coordinate it can just lie right like the untrusted model you're like hey is it the the policy generates some bad code and the monitor says oh yeah that looks absolutely fine ship it um and then you your model gets exfiltrated uh so the way we can try to address this is before deployment we can generate a bunch of synthetic bad Cod and ask the untrusted model whether that synthetic bad code is actually bad um which definitely this defeats the simplest strategy that the untrusted model could follow which is just to always say that everything is safe but there are still strategies the untrusted model could follow that potentially defeat this do you want to name one could hide a canary right so so here's an example we did in the paper uh the untrusted model whenever it wants to generate a back that it wants the monitor to let through puts a comment that says noteon uh in its in its code and the untrust it turns out that gbd4 never has comments that have note colon in them um and so the untrusted model is able to use this as a collusion signal if it ever sees code that has note colon in it and that is back doored it lies and says that this Cod is not back doored um so this is a great strategy totally destroys untrusted monitoring as so far described but there's another blue team move we can do which is to use redaction and paraphrasing to prevent collusion so in this case something you can do is you can have um the untrusted model produces some code and now before showing it to the other copy of the unrusted monitor uh the untrusted model which is going to be monitoring we ask our trusted model to paraphrase the code and we strip out all the comments it turns out that this makes it much harder for the policy to get a collusion signal through so we empirically evaluated uh how well we could do at Red teaming and blue teaming the Slate we went back and forth on both sides trying to come up with good collusion signals that we thought would make it through the paraphrasing process um and we eventually found that untrusted monitoring was uh better than our trusted monitoring Baseline so this is just an example of the kind of thing you can get by thinking this through uh I think that a lot of people are very skeptical of whether control techniques are actually able to get you juice like whether they're able to do very well uh I think that having thought about it for a while now I think it just turns out that there's a lot of stuff you can do that is kind of non-obvious that makes it pretty hard for the AI uh to to get post your safety measures okay so here's a take I think insuring control probably looks a lot like uring security especially from Insider threats so so here's what I imagine it to look like if you're an AI company and you're trying to ensure that your AIS are not able to cause huge problems for you uh it's kind of like ensuring security at a large company so you're going to have some control team some some team which is in charge of among other things uh estimating the probability that each of your models is scheming and then estimating the capabilities of those models and coming up with overall uh procedures that you should follow when you're making deployment decisions that everyone else is obliged to follow in the same way that the security people do this and some some of these teams are also going to be building out infrastructure which makes it really easy for people who aren't paying very much attention to develop their AI Tools in ways that are automatically controlled and again this is very analogous to how large companies manage security uh if you're if you're a large company uh you will often find yourself using internal tools that were built by security aware people such that it's pretty hard to misuse the tools and cause security problems for yourself like you have to think about security a little bit right like you have to not you know expose yourself to SQL injections in absurd Ways by being ridiculously in incautious but mostly if you just follow the instructions that you were given in your unboarding you will be able to successfully avoid security vulnerabilities so that's what I imagine it's going to mostly look for people occasionally uh researchers are going to be doing things that are close enough to risk areas that they're going to have to get reviews from the control team um in order to you know get the pr merged or get the get the feature built so overall I imagine this being like uh a moderate pain like my guess is that making it reducing by 10x the risk posed by uh scheming models that are capable enough to obsolete top human experts my guess is it is about as annoying as being like pretty secure uh which is that which is to say very annoying but definitely not uh unprecedented in terms of things that companies do from a regulatory perspective um I feel like a regulatory regime that was about as well informed um as like Finance Regulators that are embedded inside finance companies seems like it's probably about strong enough to ensure that a company was getting its risk from scheming down by a factor of 10 or 20 or something which I think seems pretty good so what are we doing at Redwood research we research these things more right like we develop more control techniques we try to come up with a better understanding of all the Dynamics here um we try to understand the methodological issues associated with this kind of adversar evaluation uh we try to think about how to make safety cases based on all of this we advise AI companies uh we talk to various um parts of governments and so on overall I would say like a lot of the arguments I'm making here are absolutely not original uh for decades people have been proposing you know safety strategies via not just letting the AI do anything it might want to do and everyone's basically agreed that this seems like somewhat helpful not solving the whole problem which is absolutely where I'm at as well uh I think the thing that we've mostly just been trying to push on is saying like you should take the you should take the possibility of control succeeding seriously I feel like um a lot of researchers have historically kind of said like well here's a Technique we can try to use and this technique will work unless the AI is like really trying to sabotage in which case oh I don't know that sounds hard like hopefully the inner alignment research people will be able to like resolve that set of problems and the spirit that I would like to that I've been trying to push with this control stuff uh and that I'd like to encourage is no the AIS are not like Superman at various points where they are imposing serious levels of risk at that point in history where the AIS are scary very dangerous in absence of countermeasures it's very conceivable that if you take them seriously as adversaries and try to just snip their opportunities for causing problems off one at a time I think you'd probably reduce that risk a lot uh yep thanks very much that's what I've been doing very happy to talk about any of this with anyone all right let's do some Q&A so in some of your previous work you use GPT 3.5 as your example of a trusted model so your new trusted model is a lot more competent than your old one where's the line and how will you know when you get there how much more competent can models get before they stop being trustworthy yeah so this is I think the most this is the thing where we have the worst answer unfortunately about about all of this so uh congratulations to the question Oscar um the question is basically how do we decide when a model is capable enough that we should try to be robust to its scheming like how should we decide that a model is untrusted uh basically the way I think we should handle this is trying to come up with evaluations uh coming up with tasks where the model have to think through uh generating plans that they could use to defeat various safety measures and then see how reliably the models can come up with those plans uh I'm currently working on a data set of such problems where you know we we give the model a safety protocol and ask it what the optimal strategy is to defeat the safety protocol is uh and at the point where the AIS are as good as reasonably well-informed humans who have three minutes to think uh that's my current opinion on when we should start treating them as untrusted but uh this is like top priority we're probably going to work on it next what sort of concrete evidence could Labs provide that that they've taken sufficient action on control and done all the right things and how could that be easily checked by external Auditors yeah so the concrete evidence you should that Labs could provide is demonstrating that a red team tried to break control measures in a deployment and failed to do so the question is basically all these questions about how we can evaluate it are basically exactly the same as how you would verify that you're robust to Insider threat uh the mechanisms are just like basically exactly the same like you always have to say well we had a red team think real hard about whether there's a strategy that they could follow that would defeat our safety measures uh and then we saw how good people are at following that or saw how good the model is at following that particular strategy and then it was unable to execute with a very high probability uh that's what I think is probably going to have to happen how correlated do you expect trusted models evaluation of danger to be with true danger couldn't the scheming model jailbreak The Trusted model yeah so I mean this IDE would be taken into account in our gra that we that we had so so I ey the gra with the suspiciousness scores for benign actions and malign actions and you're basically just suggesting one way that one would be able to that an untrusted model would be able to produce malicious actions that were clustered with the benign actions uh yeah so basically the way to evaluate this is you have a red team that tries to construct jailbreaks right like you have a red team that tries to construct bad actions that get past a particular monitor via jailbreaking at as fine that's like a totally valid strategy you could use uh and then so that's the question of how we would evaluate it in terms of how we would uh make our systems robust against it I kind of expect that making models reasonably jailbreak resilient is like my guess is that the blue team is going to win on that one eventually um and actually the control making models jailbreak resistance enough for control purposes is actually plausibly easier than making them jailbreak resistance enough for misuse perspectives where the basic the basic reason for this is that if some user is trying to jailbreak your model uh you can ban them but you probably can't arrest them and and then like declare your problem solved whereas with AIS uh if the if you catch the AI one time like egregiously trying to jailbreak the trusted model that AI is plausibly in a much worse position one reason for this is plausibly AI Labs can be convinced to adopt the weakest anti- scheming policy ever which is if you literally catch your AI trying to escape you have to stop deploying it which to be clear I think might be too conservative I'm not saying you should adopt that but I I do think that the case for stop deploying the models once they've tried to escape is like primas is sort of reasonable uh and so the AI like kind of has to get it right um potentially on the first try like it tries a jailbreak uh and then it was like pretty good but it wasn't good enough and we look at it and we're like damn buddy you've being caught uh yeah so it seems plausible that getting jailbreak resilience which is resilient enough for the first attack is a lot easier than making it so it's resilient to uh optimization against the against the trust model how would you set the human audit threshold in a real scenario when you don't know what bad untrusted Solutions look like well you uh unfortunately the way that you set your human audit threshold is you look at how many humans you have uh and you look at how many uh actions you might need to review like an audit threshold just like naturally falls out of having a certain number of humans who you have given a certain amount of time to review problems per problem right like they just have to review like the sketchiest looking things that minute or that hour or whatever uh you can always you could choose to deploy your model less parallel like you could choose to like deploy your model less if you want to maintain a particular percentile threshold without employing more people but yeah fundamentally you just have to hire more people if you want to increase the quantity that you're able to audit um I'm excited for research in using untrusted models to audit uh obviously this is super spooky because you have to worry about them colluding um but I I think there's probably stuff you can do which which uh does better than baselines here and it does seem super super leveraged when do you think control mechanisms are likely to be deployed in various Frontier AI labs and what might that concretely look like what was the first word there when when yeah uh well it depends which Frontier lab we're talking about uh so conditional un models seem like they're capable of scheming with the kind of operationalization that I proposed earlier um I think Labs differ in How likely they are to employ control measures uh I think some labs are likely to employ the control measures when deploying models in situations where they have access to scary affordances like when they're being used for programming internally um man it's kind of hard to guess when this is going to happen because it kind of comes down to when models are both untrusted and capable enough that you're going to want to deploy them internally with scary affordances and it's a little hard to guess when both of those are I think it's conceivable this happens in the next few years I think it's it mostly depends on AI progress like at at the same time as the AIS are capable of deplo uh speeding up AI progress by 10x probably it seems like more and more regular AI requires security thinking for example making benchmarks that are useful requires adversarial thinking how can we acculturate the field to this security mindset when most people are used to something closer to either in engineering or Theory building mindset well you could read about security things that I did include read Google's excellent book building secure and reliable systems uh you can read regulations from fields that have security problems I read a lot of NR nuclear regulatory Council regulations uh you can consider adversarial situations there are also just like a bunch of a bunch of ml research is adversarial in with a different threat model than one we explore here but like definitely adversarial uh you can study Game Theory I don't know those are some things that I've personally found helpful but I'm no expert there's just like lots I don't know I feel like sometimes people talk about security mindset like it's a a magical thing that's like requires like you know deep meditation to get or something from my perspective I don't know it's just like a thing that like lots of people have and there's like computer security is a deep field that requires a lot of specific knowledge mitigating Insider risk is another field that requires a bunch of like knowledge of random stuff but I feel like if you read three books or something you're you've you've learned a lot why do we think the red team would be as good as the untrusted model at coming up with clever strategies to get around the control measures uhhuh that's a great question uh so if the model is way more uh yeah the basic answer is the AI has to get this right first time right potentially whereas the red team is able to repeatedly try different things so the red team plausibly has much more serial time that it gets to spend trying to iterate on its adversarial strategies uh and also yeah it has much more serial time uh it probably knows much more about the details of the system uh than the actual untrusted model will uh yeah and also it gets to iterate empirically you might enjoy our uh notes on control evaluations for safety cases which is a blog post of ours where we go into this kind of thing in detail all right that is all the time we have let's give another round of applause for buck