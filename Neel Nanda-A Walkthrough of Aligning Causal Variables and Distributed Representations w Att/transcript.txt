Source: https://www.youtube.com/watch?v=iZm0_l2H2CQ
Transcribed: 2025-12-30 18:10:19
Method: YouTube Transcript API
==================================================

this is so not gonna be one of my more popular videos but whatever are you kidding our banter will carry it welcome to a walkthrough of the beautifully named finding a limit between interpretable causal variables and distributed neural representations I am joined by the one and only Atticus Geiger who wrote this paper and will help me walk through it I have spent five minutes looking at this paper so I'm very excited to learn about it along with everyone else Atticus want to say a bit of introduction for yourself sure thanks so much Neil I'm a PhD student at Stanford right now I'm oddly enough in the Linguistics Department though there is not going to be any real Linguistics in this paper and uh yeah this is collaborative work with zenwoo Chris Botts Thomas icard and Noah Goodman and excited to go through it I think it's going to have lots of interest and connections with mechanistic interpretability and I think trying to work out how to use Mech interp jargon to describe this paper in the best way I will be like a rewarding experience so yes ah yes I come from like the meckins Hub field which is its own weird cultural cluster with its own jargon Atticus comes from an academic background doing basically the same thing but with his own cluster of Jog and it'll be very exciting seeing how much we both get confused during this walkthrough also I want to apologize to an atticus's behalf for the emails being in the wrong order compared to the author list I promised the rest of the papers way higher production quality hey we've traveled back in time to give an overall summary of this before the video where we then figure out what another's actually about because it's kind of confusing elevator pitch for this paper also models are the right way to think about neural networks and interpreting them we have variables we have algorithms like sidewalk is wet if is raining and is wet or dry some like nice logical thing we want to impose these causal models on the neural network the way we do this is with causal interventions you pick some bit of the model you replace it with its value and some other inputs and you look at how this changes the output and compare that to what the mod causal model predicts to change the output like you think that the model is taking X and Y and not putting X Plus y if you intervene on the model suddenly you replace them and tunnel bit with like adding equal the inputs three and four and the inputs three and seven if you intervene on a bit of the model such that you only replace the first input the output should remain three plus four rather than with this random other thing this is either of a patch or an interchange intervention and the problem trying to solve in this paper is where we're trying to do this mapping onto a causal model but rather than having like the right units or variables as neurons or heads like the right units to analyze what we instead have going on is our internal representations are these big chunky vectors and there should be some meaningful directions to correspond to units but we don't know what these are and the point of this paper is using gradient descent to find the right directions such to these interchange interventions work and using this to validate we found the right units for a model I think this is exciting because we often do all the right units are anyway I will now return you to the actual video where we go and figure all of this out together but hopefully you can have that in the back of the mind doesn't matter all right all right first question what does alignments interpretable causal variables and distribute in your representations mean so alignment does not mean aligning AI with our ethical values here alignment we mean purely a mathematical object that is telling you a relationship between a tiny causal model and a larger causal model so the alignments we're talking about is an alignment that's going to say this variable in a high level simple model is Faithfully summarizing or aggregating the causal mechanisms of this cluster or set of low-level causal variables just stopping you briefly a causal model is just like any structure that you think could describe an algorithm yeah so when we say causal model we really want to be super General where we're just talking about any discrete set of variables that take on some values and then there are just functions that tell you the value a variable should take on given the values of its parents okay okay there's a lot of talking in that so we have this diagram and you're saying you start with two inputs p and Q you set two variables to be equal to p and equal to q and then you have some third variable that's like and so like I don't know p is zero Q is one you've got this node that represents zero note that represents one and then V three is like zero and one which is zero that's exactly correct so why do you have a node for p and then a node for V1 equals P aren't those like the same that's a good question so you can have sort of like chains of variables that are just copying each other and what you can think of that as giving you at the high level is the ability to express your hypothesis that the same piece of information is being carried in a flow of time so if you have just saying like v p to V1 copies p and then to like Z1 just copies V one that could be saying that hypothesis like P is here and then p is here and then p is here as the information flows through the network and if you could align those with low level neural clusters you'd be able to say yep the same information is included here causally as well you could do interventions and set it to be whatever value you want and so crucially this gets to a kind of like deep interesting point which is that being forced to encode your high level hypotheses is causal models sometimes we'll make you like do sort of like unintuitive weird structures or interesting things and maybe make you decide on Choice points that you didn't even realize you had to be making to State a particular hypothesis what do you mean by choice points say like the bubble sort algorithm there's sort of a choice Point like maybe not a bucket sort where you could do the left bucket or the right bucket that's a choice point but when you're forced to write down every single variable that is used you actually realize that you're able to sort of abstract away some details or not abstract away certain details and it becomes a lot more complicated to describe any interesting object once you realize there are so many different levels of description at which to describe that object like I want to give a closer model of how I put on my shoes I know I put on socks and then put on shoes but I could put on my right sock person my left sock first and this is just like an arbitrary thing I need to decide on yeah cool all right make sense so going back to what is aligning a causal model it's like you've got this kind of tiny toy causal model where you know what it means it's this really simple logical structure and you have some more complicated thing during a more complex type calculation and you want to be able to say everything in the small model exactly corresponds to a thing in the big model with the same structure but I don't know why do you have care about having a big model again so the big complicated model yeah why can't we just only look at the tiny ones well I guess that is sort of the dream of interpretability I think is at some point to have the tiny models right because the big models are just the artifacts in the world in the case of causal distraction it's a storm system like the weather or the human brain or an artificial neural network you have these very complicated black box systems with all sorts of connected micro variables but you want to sort of find faithful ways to aggregate this information into a form that a human could use to make decisions so I guess that's sort of like the dream and we have to we start with the black box all right so dejocking that we have goal of mechanistic intouchability or I don't know any reasonable interruptability so you have this weird mess of artifact of a neural network you want to understand the neural network the neural network is made out of maths but it's kind of an inscrutable massive linear algebra we want to be able to think about what it's doing in this nice algorithmic way we believe it often has learned reasonable algorithms like I don't know it's predicting the next token you give a text like ice cream and it predicts that Sunday comes next because ice cream sundae is a common trigram and this is implemented in the matrices in this cursed screwed up way but you can totally map this to a causal model like Piers ice Q as cream if P equals ice and Q equals cream output Sunday and if you can find bits of the model that map onto bits in that diagram you're like I've just understood the model everything is great yeah and I think it gets increasingly more interesting the more that these variables represent Concepts that are compositions of many of the inputs rather than the sort of things that correspond directly to the input like this p and V1 situation where this is just really the mediation of information but it's very interesting if you have like a paragraph of text and you need to make multi-hop reasoning steps where you need to sort of create a Boolean proposition that's like is this statement true or false given a synthesis of information over text so essentially the dream is to be able to articulate any sort of reasoning algorithm at a high level and then code that causal model and then look for the variables in that algorithm at a high level in the low level Network gotcha like you could imagine something like you give the model a page from Harry Potter and at the end you ask it who was writing the third broomstick and the model is capable of answering the question it's big enough but this is kind of a complicated task and it's going to involve a lot of synthesis of information in the text and like tracking different concepts like peers on each broomstick who are the characters where are they what are they doing which pronouns refer to which characters but like the dream would be that we could actually map this as some causal model and are we talking about the wild dreams of interruptability but we're currently staring at neurons or do you think like does actual interpretability work that looks roughly like that you've got a paragraph you answer a sophisticated question about it and you can understand how the model reasons through the question I don't think that work exists yet I do feel like it is within the bounds of existing tools though I don't think we would need rapid methodological developments like I think with the same sort of uh mindset as like causal mediation analysis or the Rome paper or causal scrubbing or our work yeah I think it's just a matter of scaling those ideas up to more interesting algorithms with like more interesting structures like loops and hierarchical variable structures or control flows and uh yeah yeah yeah all right do you want to get back to the title we still haven't finished defining everything the title okay yeah getting back to the title so puzzle models also yeah I guess the best maybe um completely lame and definition of causal models there's just circles and then there's lines between circles and then there's lookup tables telling you what value a circle should be given all the circles that point to it and that is unironically a causal model that it's like a very generic way of talking about things and I think pretty much anything any computational object you want to talk about you can talk about that way sorry that's not a good latest explanation of a causal Model A causal model is like some algorithm you have some variables you step through the algorithm it tells you what to do at each step and you give some examples that's a better Layman's definition but I think it's useful to think of these as very disjointed mathematical objects you know really in the end all they are are just circles and lines and look up tables and we're just talking about like connecting that kind of object with another one of that kind of object I do recognize that maybe my brain's a little different than uh also no these are blatantly squares rather than circles they become circles once you rotate them into distributed space all the shapes matter should we go back to the second half of the title yes the second half of the title all right distributed neural representations that's the second half the title we're talking about now right what does that mean well it's a neural what's a distributed what's a representation in this narrative we have going where we're trying to reverse engineer a neural network by finding a simple causal model aligning a variable and that causal model with a cluster of neurons if you actually are only focused on aligning a variable with a cluster of neurons it's a very limited way of engaging with the question because you're assuming a privileged basis representation so if you have a vector of like 50 neurons you could just say oh I think it's going to be this clump of neurons but you could also say take that 50-dimensional vector and then represent it in some rotated basis and then we want to align high-level variables with the dimensions in this rotated basis and so the reason these are sort of a distributed representation is that any dimension in this rotated basis is going to contribute to many dimensions in the unrotated basis all right so much jargon let's stop and unpack that also can I just say it is a bad sign that is taking us this long to get through the title but we're doing mechanistic interpretability one of the key things we need to do is break our model down into units of analysis that can be understood in a penalty model and tunnels are made up of tens of thousand dimensional vectors with Millions to billion dimensional parameters these are ridiculously complicated and we need to know the right units of analysis to like reason about things independently and be like this thing detects dogs this thing detects the texts in French things like that the most convenient possible world is that individual neurons like individual basis elements individual floating points within the vector of the model are meaningful like this neuron refers to a cat this neuron refers to a doc you are referring to clumps of neurons which seems a bit surprising to me can you say more about that like why would you ever want more than one your own I guess you just want to align a high level variable with whatever component happens to serve the causal role of that high level variable it may be a single neuron it may be a set of neurons within a larger neural representation or in the repeated case it might be some direction in the space in the real valued space or it could be some set of orthogonal directions gotcha and to Chuck are we talking about the residual stream or the MLP layers of the Transformer the residual stream all right so brief background on Transformers for people who are not familiar Transformers I mean Transformers neural networks they have a bunch of layers but notably rather than the input to the next layer being the output of the previous layer each Transformer layer reads from the residual stream this like accumulated memory of the model and its output is just added to the residual stream and so you can think of each layer as an incremental update everything the model knows is stored in the residual stream what the input is what position you're at whatever intermediate variables is computed like this text is positive and floppy this text is advertising copy about headphones I should be outputting a pronoun next and also the model's best guess for the actual outputs and it's kind of this compressed mess of stuff importantly there isn't any particular reason for the individual like elements of this to be meaningful Atticus calls these neurons I prefer not to call them neurons I reserve neurons for anything that comes after a element-wise non-linearity like a relative or a Jello because in like the middle of a transformative layer you'll do something like for every individual basis elements apply a jelly or a value to it in a way which actually does create a privileged basis yes absolutely in the Transformer residual stream everything that interacts with it is purely linear so you can just apply an arbitrary rotation to it and apply that same rotation to all of the weights that are read and write from it and the model would work exactly the same the one exception to this is Adam the fancier version of the category incent used to make these models which is cursed and means actually the basis is like a tiny bit privilege which there's a philanthropic paper about but let's leave that aside yeah and actually the first experiment in this is going to be just on a multi-layer NLP like a multi-layer NLP multi-layer perceptron gotcha like your ATM machine and pin number yeah anyways so finishing unpacking the concepts the residual stream is just like a big Vector for every token in the text given to a model you can kind of think of this as like a big Vector for every word you know what a token is and this Vector is I know let's say a thousand dimensional and the model is representing a lot of information here which corresponds to different directions in space like in the classic case of word embeddings you have ping minus Queen equals man minus women as a linear representation of things where linear representation is jargon for there are different directions that mean things and you can add up these directions to get whatever combination you want like the male plus royalty Direction gives you King the male plus no royalty Direction gives you man Etc so by distributed here we basically just mean we do not know which direction the things go in like we don't know what the meaningful directions are because we don't have this super convenient thing where just like each floating point is Meaningful and part of the point of this paper is figuring out the right units of analysis by finding the meaningful directions how's that exactly yeah and maybe specifically the phrase distributed representation is connected to sort of like the in the 80s when people were working on AI slash like Neuroscience pulsemanski kind of proposed exactly this idea of there being sort of privileged non-standard basis for interpreting uh neural networks and that being a way to understand these polysomanticity and neurons that they're talking about so that's a fun historical connection gotcha all right um I'm gonna take this as a chance to sign on my soapbox and explain a few other points that are worth knowing the first is just that I think a subtlety people often miss when thinking about Transformers for the first time is that there's a massive difference between the vectors representing a Transformer residual stream and the vectors in the middle of a transformer MLP layer immedially after the jealous this is because you can kind of think of a transformative multi layer as a bit of the model used to do computation like thinking and processing the jellus are used to like compute new features the model doesn't already have like it can go from the previous word was Eiffel the current word is Tower to this is the Eiffel Tower using the neurons but it can't do it in the residual stream and because there are jealous the basis in the MLP is like kinds of privileged and we can kind of think of the neurons as a starting point for like the right units of analysis well in the residual stream we basically just have nothing so maybe your mind will change after the results we see where we're actually unable to find a basis aligned uh representation in a pure MLP just trained on our task and we are able to find a perfect representation of the causal variables under a rotated basis and that's just an MLP with relu units so at least in that model it seems like they're still not getting uh the privileged basis I guess yeah okay so the second thing I'm going to say while I'm on my soapbox is this idea of superposition which I think is interestingly different from the idea of a distributed representation let me just briefly pause and share my screen so superposition going back to the difference between mlplas and the residual stream we have this beautiful theoretical arguments or why the residual stream of a model should not have a privileged basis while neurons should because models internally want to extract and represent a bunch of features of the inputs and they want to be able to do this in a way that doesn't interfere with each other like you want to be able to represent this word as a dog and this text is in French without interfering so that it's harder to compute about English dog text versus French dog text Etc because jellus or values act on each neuron independently it makes sense for the model to want to align each feature within Europe this is a beautiful theoretical argument sadly it's wrong in practice models often have polysomantic neurons where the neuron seems to represent a bunch of things like or represent dogs but it will also represent flowers or some crap like that this is like kind of surprising I genuinely think there is something that needs to be explained here because if the model just has as many features as it has neurons it should just obviously want them to be neural aligned and the superposition hypothesis says that the reason that things are not neural aligned is that models want to compress in more features than they have dimensions in order to do this this means they need to have features that are not orthogonal to each other I.E can interfere with each other if you have more features and you have neurons you're just going to have this problem like you can't have a feature Penny and this like beautiful paper toy models the superposition but I have a different walkthrough about trains this like toy model and found that this model would learn to perform superposition if the features were sufficiently rare it would compress them in so you know I have four and two dimensions and then five and two dimensions and they're not orthogonal they interfere with each other as far as we can tell this is a lot of what's going on in actual language models we think the superposition is a lot of why neurons and language model polysomantic the model is just Computing more features than it has dimensions and somehow shoving these into the different neurons and it's kind of wild I recently worked on this paper led by Wes Gurney that I should have a forthcoming book for about where we actually found evidence of superposition and models we were looking at how they detect compound words like Social Security found a bunch of neurons that detect it kind of badly like the blue bit of Social Security and then this is about of something not Social Security each neuron was kind of bad but the neurons added together and now fabulous this is important because it means that you can't even think about a model in terms of finding the right rotation you need to think about a model in terms of like understand all of the compressed features with the like more directions than you have dimensions and understanding how to deal with this if you only care about certain features you can think about things like finding the right rotation but this is ultimately going to be an approximation and the final point before I get up my soapbox now Atticus talk again is there's this big conceptual difference between superposition as a form of compression like the model just has more features than it has space to store them and needs to squash them in which is what the residual stream does this kind of has to be happening because models have a vocabulary of like 50 000 tokens but they have like a thousand dimensions in the residual stream so obviously they are doing some compression which we think is superposition and then MLP layers the generos are actually doing computation but they also seem to have more things they're confusing than features which is this whole other thing we're confused about so yeah returning the idea of dispute representations there's like actually two related ideas here there's you have features that are not aligned with the standard basis which obviously happen for the residual stream should not obviously happen for MLP layers but then you also have this idea of superposition where there's many more meaningful directions and there are dimensions and this necessarily implies distributed representations but it's also like a stronger and weirder claim that's more of pain yeah I think that's exactly right yeah and uh I'm yeah very recently excited about figuring out how this connects to superposition because it's exactly these sort of methods uh I feel like this is the just the right direction to be going in and that I don't know the crucial connection there also between more features than neurons but also sparse features that like that third part is as crucial as the first two for understanding this phenomenon because yeah you know when they'd have non-sparce features if they get exactly a one-to-one neuron feature correspondence in the toy supervision model and then it's by increasing sparsity which then means you actually don't have to keep track of many types of comparisons between features right just briefly unpacking's boss T I try to use the word prevalence because fasting is too many things so uh there's like a bunch of things you want to know about the world especially if you're a language model for example knowing who Atticus Geiger is is like a delightful thing that everyone wants to know but like no offense it's not that useful of course predicting the next token like most of the time but it will occasionally matter like you've got a tech stock mechanism contraptability it mentions Atticus Geiger and you're now going to use the wrong jargon for everything rather than the right track but it's doesn't come up very often and to be kind of dumb to dedicate an entire neuron to it because like I know gpd3 has like a few million neurons but it knows a lot more than a few million facts probably and because this is like kind of rare and the rarer two things are the easier it is to represent them in a stupid position because if you just have to tell which one is there when the other one isn't there this is kind of cheap well if the two things occur at the same time or just to carry a bunch it's just like way harder and this is also it's one of the only reasons superposition might be remotely solvable because when you have more directions and you have Dimensions it's kind of when it comes to recovering this because I don't know math theorem there are just infinitely many ways to represent a given Vector as a linear combination of a set of directions which are more than the number of dimensions but if you say it has to be a sparse combination where here sparsity is because any given input only has so many features yeah kind of the exact inverse notion 20 feature only occurs in many inputs in like a handful of inputs because like I don't know text about Neil Nanda you're not going to be needing to use all your knowledge about python code say this is now a constraint and this can let you figure out how some of the vector is represented in terms of these combinations one big problem is learning how to take a model and its activations and recover this meaningful set of directions but that is so not in scope for this video all right we figured out the title what next most of um yeah all right okay let me just try to summarize what the title means to fully round off this glorious hour of exposition we have interpretable causal variables and diagrams this is like some algorithm where we have meaningful inputs and we use these inputs to compute a bunch of intermediate variables that we know what they mean and we compute some output we want to find an alignment between this and a network by mapping activations in the network to bits in our diagram and like we have a bunch of theory we haven't gone or two about how you do this and like a real rigorous way we are not just bullshiting yourself exactly in order to know what the right chunks of the model are to Maps this diagram you need to know the right units of analysis but if you want to take say a model's residual stream it's like shared memory that accumulates everything there's not a meaningful basis where you can just be like this access this axis and this axis are the right units each means the same thing you need to find meaningful directions it's even more first because there's actually superposition going on where there's more features than there are dimensions but you're not even going to get into that in this paper you're just trying to answer the question of how do you find the right direction such that it is a node in your diagram yep right on then how do you do all of the like causal understanding of what's actually going wow you know if that's your summary of the title I mean why even read the paper that was a great that was a great summary the title uh I mean I feel like that really just yeah that's exactly what this paper is all right maybe let's end part one here people understand the title this is all they really need life is great thanks a lot for joining me we're gonna transition to part two actually I'm just hunting anything about this paper yeah yeah