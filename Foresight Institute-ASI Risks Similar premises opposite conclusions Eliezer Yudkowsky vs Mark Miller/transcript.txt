Source: https://www.youtube.com/watch?v=s-Eknqaksfg
Transcribed: 2025-12-30 17:41:50
Method: YouTube Transcript API
==================================================

Right now we're in a window of ignorance where we are more ignorant of how to build a superintelligence than we ever will be again.

Got that right.

My perspective with regard to you recommendation is an attempt to delay both loses the window of ignorance, creates an overhang of knowledge of how to build superintelligence that hasn't been employed yet and does not delay it to a time where we understand how to solve what you consider to be the hard problems we would need to solve.

If I had to guess at where the crux is right now, I think it would probably be something about you're seeing a sort of starting-in trajectory of how things will go if left sort of to themselves or modulo like small philanthropic interventions.

Where we've got this broad range of diverse intelligence systems that are developing kind of gradually, they occupy a continuum.

We've got a chance to have a bunch of particular ones with shaped motives.

We can try to build their government in advance.

We can try to build their prison in advance.

We can ask like the intermediate ones to help us prove things about properties that we want to prove the more advanced ones.

And I sort of see all the current AI companies saying that they want to build AI that builds AI.

And OpenAI gets them first and their superintelligence slaughters everyone before anyone else has a chance to get up.

Or Anthropic builds one first, and their superintelligence slaughters everyone, including Anthropic to be clear.

Okay, but by back off you don't mean that we never ever build super intelligence.

You mean that we delay until we figure something out that we don't currently know.

What I believe is that the thing we should try hardest to do is avoid a critical it.

The main thing is to find paths forward where human interests remain at the table and power remains decentralized.

Well, thank you guys for agreeing to do this.

I really appreciate it.

The reason I was so anxious to have you talk with each other, and I'm hoping that this is not a debate.

This is just a friendly discussion because you two agree on a great deal.

I think you also, along with me, agree on what the ultimate goal is.

What are we aiming for as an ultimate outcome for all this?

So now we're just trying to identify what is the least risky strategy for reaching this preferred state.

And the challenging thing is, despite you agreeing on so very much, your strategies are so different that I just really wanted you two to kinda go over it point by point and try to figure out, can we agree on more things or are there some things we just are not going to come to an agreement on?

I'm not going to do any heavy moderation.

This is just a discussion.

I'll interrupt if there's a point of clarification or if things are going on a little long or if we're getting off into the weeds because we have quite a bit to cover in our 2 hours.

Okay, so before we launch into all the detailed discussion.

If you just could both introduce yourselves.

I know almost everybody will know Eliezer, but there might be one or two people who don't, and lots of people probably may not know Mark.

Just a few words, Eliezer and Mark about who you are, your background, stuff like that.

I'm Eliezer Yudkowsky.

I was the original person who said, Let's work on what's now called AI Alignment about 25 years ago.

Over time, I got more and more worried that it was going to be slightly hard.

Then the deep learning revolution came along, and that was actually quite a bit of bad news in terms of how understandable the methods were likely to be.

Things have currently to the point where I'm like, Okay, this is not looking like it's going to work.

This is looking like it's going to kill everyone.

We need to actually shut this down worldwide.

Okay, Mark.

Okay.

I am a...

I was part of the original Xanadu project, the early hypertext project that coined the term hypertext and was one of the two early hypertext projects.

This was the one that really had the vision of being a worldwide publishing system in a decentralized manner, also an early cypherpunk.

Eric Drexler wrote the Agoric Open Systems papers, which are in many ways the first serious work on what we now call smart contracting, as well as a permissionless world of rule of law-like systems that were completely decentralized, cryptographic, and rooted in computer security concepts that I've continued to work on in all the years since.

Currently, I'm a Foresight Institute Senior Fellow and founder and and chief scientist at Agoric, which is a startup company that I founded along with Dean Tribble and others that are very much still pursuing the Agoric Open Systems Vision, which I'll just mention in ways that we may elaborate on, are actually all still motivated by a lot of the same dangers that Eliezer and I agree on.

Great.

Now, this discussion builds out of an email interaction you two have been having, where you go over the things you agree on.

Rather than belabor those, I think just a very high-level summary, Eliezer gave a good high-level summary in his introduction.

Mark, would you agree?

What a risky world are we heading into?

A very high level.

The small kills all dangers for which the most obvious one is an engineered plague.

We've seen with COVID that whatever you believe about the lab leak theory, the fact that it's plausible that the whole thing could have been outcome of gain of function research is enough to make the point.

The world is much more in danger from a concerted effort to build an engineered plague, especially by a good team with AlphaFold to make use of.

To such an engineering plague, right now, we don't have adequate defensive technologies.

I think right now, such a thing could bring down civilization or worse.

Likewise, much less well known is that the computer security infrastructure that our whole civilization rests on is not only insecure - but massively insecureable.

Years ago, well before the current deep learning revolution, a DARPA Grand Challenge already demonstrated the ability of a good team to build malware which go out into the world, probe for vulnerabilities, discover vulnerabilities, and write exploits to make use of those vulnerabilities.

I think we're also in a situation where a good effort to create such malware could also bring down civilization.

The computer security aspect of, like I said, a lot of what I spent my career on with the motivations, including the motivations that we're We're all talking about here.

There's enough other small kills, all kinds of scenarios.

We're already past the thresholds that mean that these are big risks.

We don't need AGIs or ASIs in order for these risks to bring down civilization.

As we go forward, as we advance Our ability to build software which is capable, it will amplify both sides of each of these arms races, of every one of these arms races.

It's very hard to play out which side has the ultimate advantage in these arms races, other than just the base observation that there's more people interested in seeing the defensive side win than seeing the destructive side win.

That's the one general advantage that the defensive side of these arms races has.

I wouldn't say that this is all like an agreed background frame, to be clear.

Okay, yes.

I'm sorry.

Yes.

Yeah.

Mix things together.

You talk about your list of lethalities.

The list of lethalities certainly become a much greater vulnerability once computers are more intelligent or more capable and more able to use lethalities against civilization.

I mean, I don't think they would need us to have built the infrastructure.

It would take them longer if they had to roll out their own infrastructure.

But that might be more like a matter of weeks than a matter of years.

It is convenient for them that we have all these labs around who will let you email them a DNA sequence and then ship you back by express the proteins corresponding to that DNA sequence with 24-hour turnaround times.

It's convenient.

But if you are stranded on a remote alien world and you had to build a lot of your own infrastructure from just the initial pieces you had that you brought along with you, you bend your ingenuity to that.

You would be like, Oh, well.

You wouldn't be like, Oh, well, there's no Walmart here.

People are just selling me things.

I guess I'm stuck.

I guess I can't build anything.

I guess can't do anything.

A smart AI is more analogous to a civilization than to an individual.

It can build things that aren't in supermarkets.

Okay.

I'm very glad to hear that the observation that smart AI is more like a civilization than an individual.

I'm a little bit surprised by that, but that puts us, I think, in a much better position to find more common ground.

Well, in its capabilities, to be clear.

Not necessarily having motivations analogous to the motivations of human civilizations.

Okay.

As we continue, I think we'll figure out what are the aspects of that for which we are already in agreement, in which ones we disagree, and which ones matter.

But you're certainly right that as I continue to elaborate, I went beyond summarizing where we agree.

The place where I went beyond is to focus on the fact that a lot of these lists of lethalities, a lot of these small kill-all dangers, are already dangers that we're facing.

I don't think you disagree with that.

I just think you're just missing an emphasis.

It's not clear to me exactly how hard it is to wipe out a...

Pardon me.

There's several distinctions I made here.

One is knocking over current civilization in the sense that a bunch of people are left to pick up the rubble.

And one is the sense of, exterminate Earth-originating intelligent life in its present form.

No more humans, nobody left to rebuild.

And the other distinction I would make is that there's a question of how much intelligence it takes to knock over human civilization.

It could be, ...

a relatively easy task for a sophisticated biology laboratory.

i might quite lack the fully detailed boots on the ground sense of is it the case that any 10 top 10% biologists with a well stocked lab can knock over our entire civilization?

I just straight up do not know off the top of my head the answer, whether they can do that or not.

I think that for them to exterminate literally everyone would probably be hard, but I could be wrong.

Okay, so I agree with that, including the uncertainty point at the end.

But the other thing to take into account is that things which require large expense and large organizations today, with the advance of the underlying technologies, quickly become commodity.

Yep.

My personal guess also leans in favor of lab leak.

And it's not clear to me, like how long ago was like the first point in time where somebody could have created COVID 19 if they tried to do that.

Like that exact one probably reflects the more recent technology.

There were older bioweapons programs.

If it were important to something, I'd probably want to like, you know, ask.

Ask one of the AIs to, to try and draw the graphs of.

Well, to try to like collect the arguments and data for, you know, how much cheaper have it gotten every two years to build COVID 19 or something?

Yeah, so I think the.

I think we, you know, I think we don't need to go into any depth on the COVID 19 thing other than just as an example of the general small kills all dynamic.

I will say that just having understood how fragile we are in the wake of those kinds of potential threats, people who are in that area, I would encourage them to do what they can to work on defensive mechanisms.

Because technological advance amplifies both sides and it's important for the defensive side to get ahead in these arms races.

Okay, let's.

Before, in your email communications, there was a list of 10 cruxes.

And one way we could.

One way we could approach this is to go ahead and go through those.

Mark?

Yes, please.

Yeah.

Okay, cool.

So three major cruxes and then the rest is underneath each one of them.

Sure.

So the first one involves confidence of predictions about different kinds of systems.

Yeah.

So one of the things that I don't understand, Eliezer, about your position is the level of confidence that you project, which I must assume reflects the level of confidence that you have.

You make these statements that sound very certain.

You know, if anyone builds it, everyone dies.

There's just a lot of things that you have with the heading 'everyone dies'.

And I've heard you make the comparison of why AI ruin is the right framing, not AI risk You make the comparison with an asteroid.

If we saw an asteroid, big asteroid, heading straight to Earth, that our confidence of that is close enough to certainty about what would happen that it would hit and what the consequences of a big asteroid hitting would be, that we can just talk about asteroid ruin.

It's not worth using the language of uncertainty when talking about that scenario, which I agree with.

So to be clear, like, I'm not saying ASI will hit the Earth and kill everyone.

I'm saying if ASI hits, then everybody dies.

Whether humanity will lie down and die in this way is not something on which I can claim narrow expertise in quite the same way I can claim to have studied the issue of what happens if it hits Earth.

Right.

So the if-then statement is what I'm referring to.

You do always make it as an if-then statement.

But the if-then statement, you do use the language of certainty with respect.

I mean, not infinite odds, not probability 100.0, but the sort where it seems perverse to speak of the risk rather than the ruin.

Okay, yes.

And that fits with my impression of what you meant when you use the language of certainty.

So no disagreement there.

The thing that strikes me about it is that there are some systems, like the orbital mechanics of an asteroid heading to Earth, where to put it in Wolfram's terminology, that's reducible, that you don't have to play, you don't have to let it play out to see what will happen.

It's a very, very reducible problem, and it's a reducible problem for which it's appropriate to have a high confidence in the output.

The ''If we get ASI it wipes us out'' involves so many complex systems about the world, involves so much inference about the nature of the world and just starting with the nature of intelligence, that I don't think we're in a position that justifies that degree of certainty.

I think that we're, you know, the world has all sorts of complex adaptive properties, some of which we've been discovering over time.

You can characterize a lot of these things in terms of the discovery of positive and negative feedback loops.

We traded and formed market prices for a long time before Adam Smith.

Likewise with the evolution of morality, the evolution of language, et cetera.

We have not yet built intelligence where we are in fact uncertain about what the nature of intelligence is and what the nature of superintelligence would be.

I, I just, I don't see why you, why it's appropriate to use for the if-then statement that kind of level, that kind of language of certainty.

Well, you probably want to discuss that closer to the end than to the beginning.

Like if you imagine you're having one of the good old fashioned conversations about nanotechnology with somebody, they haven't read nanosystems, they've heard of Engines of Creation, they haven't read Engines of Creation, and they're like, how can you possibly be so sure that anything like nanotechnology is possible?

Wouldn't the manipulators that we're using to poke at things be like too thick to move anything around?

What we used to call Smalley fingers back in, in the day, you know, your day even.

I'm not claiming that this was my day, So if the'd lead in by saying, like, how are you so certain?

Probably the first thing you want to do is say like, let's hold off on the question of whether I'm like too extreme in my beliefs and just talk about the object level, like, how do these systems work?

What is going on in there?

What are the obstacles to building nanotechnology?

And then at the end of that, we can talk about the general question of, you know, somebody studies an issue for a while, they arrive at an opinion.

Should you be going up to them and saying like, how can your opinion be so extreme?

Or should you just be saying like, what are the arguments?

Because it happens that you stare at things for a bit and you end up with, you know, more extreme odds on the person who has stared at them for less time.

And sometimes that's because the person is nuts and sometimes it's because the arguments actually point that way.

It just happens.

I certainly agree with that general framing and we're both more ignorant of each other's work, then ideally would be the case.

So we both need to go over a ground of explaining things here that we've already explained many times in other media.

So let me ask you, what do you think the nature of intelligence is and how do you get from what you think that is to the conclusions that you've come to?

Can I just go for the, 'what's intelligence?' part first?

Sure.

All right.

So.

So I would tend to take the theoretical...

So first I would suggest a reframing of what kind of work does intelligence do over what is intelligence?

Sort of like a physicist might say something like, well, you know, you've asked me, like, what is an engine fundamentally?

And you're like, hey, you say that a plane is an engine and a person is an engine, and this thing is an engine and that thing is an engine.

And they seem so different to me.

What do all these things have in common?

And the physicist starts by saying, well, let me explain to you about the kind of work these systems are performing.

Okay.

And with respect to intelligence, I would start.

There'll be a bit more to add later by introducing the ideas of prediction and steering.

One kind of work we can see a cognitive system is doing is the work of predicting, like, sort of most directly predicting what it sees next, the next set of sensory inputs.

You look out at the sky, you see the sky is blue.

If it were purple, you'd be surprised.

You'd lose some points.

Bright purple.

To be clear.

If it's dark purple, it's probably nighttime.

And in order to just predict your immediate sense data, you need to start building models of the world that lies behind the data.

And so in the process of merely predicting what you see, you end up predicting the larger world.

You end up understanding how the Earth is turning and the sun appears to move through the sky as a result of the world spinning, and not just understand the day, night, cycle that you see that your ancestors saw with their naked eyes.

Okay.

There is a second kind of work which is worth distinguishing, and it's the work of steering, of choosing an output such that the state of the environment is thereby coerced or even just like put onto a course, that would be improbable if not for your steering it.

Why distinguish these two things?

Why say that they're different kinds of work?

Is it not trivially the case that if you have a very strong predictor, you could just ask it to predict the output of a steering engine?

Like, if you imagine you have a magic genie that is capable of predicting anything you ask it to predict, and you want it to play chess?

Well, playing chess is not, on the surface of it, a prediction job.

It's a matter of outputting moves on the chessboard such that the later future of the chessboard ends up in a position that is completely defined as a win for your own side.

But if you had a super powerful predictor, you could just ask it, well, what would Stockfish 16 output for a move here?

What would a much more powerful chess player output for a move here?

And if you had a very powerful action such that outputter and you want a predictions, you just say, well, give me a prediction such that when I score it later it will score as a very good prediction.

So in one sense these two kinds of cognitive work are interconvertible.

In another sense they're fundamentally distinct because the steering task has a degree of freedom not present in the prediction task.

When you are building a machine learning system and running gradient descent on it to make it a better predictor, all you need to look at is the probability that was assigned to the data that it ended up actually seeing.

There's only one kind of higher point number, higher floating point number it can assign to be higher scored as a predictor.

Now suppose you're trying to build a steerer.

You're trying to build a little agent that will navigate through a maze and get a gold coin.

The gold coin is always in the upper right hand corner.

And it so happens due to a bug in your code for building mazes, it's, you know, you're to generate mazes, it's going to go through the maze.

It so happens that the coin that in the mazes you generate due to a bug in your code, the coin is always in the upper right hand corner.

So the the system learns to win the sort of mazes that you handed it.

But later on, much to your surprise, you handed it a maze that you know you generated by hand or the equivalent of the maze that's out there in the real world.

And instead of going for the gold coin, it navigates the upper right hand corner and you're like, what an objectively bad steering system this is.

It didn't reach the gold coin at all.

And that's your perspective on where you preferred it to go and where you wanted it to go.

But from the system's internal perspective, it just wanted to get to the upper right hand corner.

It was outputting actions that were very high in its preference ordering actions that were very high in its preferences over the expected outcomes that these actions lead to.

Using an excellent model of reality, it wasn't confused about where it would end up inside the maze at all.

It didn't believe it was...

It didn't predict that it was going to get to the gold coin and be mistaken about that.

It's just that the score of the action depends on the preference ordering on outcomes.

and this is a degree of freedom that is not quite present in the same way when you say, how much probability did you assign to the observations you actually saw?

This is a fundamental divide running through the kinds of work that these systems can perform.

It corresponds to David Hume's old observation that when you are deriving IS statements from IS statements, you will never get to an ought statement unless you introduce oughtness from somewhere.

And this is part of what makes prediction and preference worth distinguishing, like the work of prediction and the work of steering.

If a system is very good at one of these, you can convert it into some of the other kind of work.

But even then, when you ask it to do the...

Even if you're getting the steering work out of asking a predictor to predict a steerer, there's still that extra parameter, that extra degree of freedom.

So the first cut would be to say, when I talk about AI I tend to talk about systems that are good at predicting things and steering them somewhere.

And it's worth distinguishing these two clauses because the steering part has an extra parameter in it.

Now, if we wanted to talk about artificial general intelligence, I'd have to talk about what distinguishes an AGI from stockfish, which is very good at steering the chessboard.

And AGI is good at steering lots of different things and predicting lots of different things.

It is not universally capable.

It can't necessarily predict the lottery, but it's capable of predicting a lot of things.

If you look at humans, then the ecology of Earth as a whole is competent at a number of different things.

You've got your bees that build beehives and your beavers that build dams.

And a human will look at the bee at the bee's beehive and the beaver building the dam and be like, what if I built a dam with honeycomb structure?

And they will do this even though their ancestors never learned to do it.

So humans alone in this ecology are able to adapt to new circumstances the way that evolution adapts species to new niches.

And we can do it on much shorter timescales.

Evolution operates on mammals.

Bacteria maybe can evolve pretty quickly, even compared to humans working in laboratories.

But even so, as Cynthia McKinnon, I think, said, it was like, a grad student can do things that evolution could not do in a billion years.

I'm sorry, I just missed a word.

What can do things?

That one grad student can do things today that evolution could not do in a billion years.

And I'm a little bit worried about that billion years part because Billion years is a lot of time, but a million years, sure.

Okay.

We're about a quarter of our way through I think we're doing really well.

Let's keep going.

let me actually check logistically.

I can go into overtime.

Can both of you go into overtime?

I can go into overtime.

Okay.

Okay, great.

So the next section of cruxes, and we are leading into this anyway, which is great.

Which is the nature of social intelligence, the way human organizations are or are not more intelligent.

Mark, can you kick it off here?

Yeah.

So the.

When I take a look at human history and the biological prehistory and how we got here, and what are the impressive systems of intelligence, the systems that I'm most impressed with are not an individual human mind.

It's the systems of human organizations, of human culture, of human civilization that emerge from large scale productive interactions among many minds.

You know, to take one example, the people in East Germany were not endowed with less brain power than the people of West Germany.

Likewise north and South Korea.

And over the expanse of human history, for most of it, large human organization were very singleton-like as systems of oppression, very counter aligned with human benefit.

Pyramids are as crazy a thing to spend other people's lives on as paperclips.

And in both cases you had a single unitary takeover by a power that was sufficiently powerful to treat other people's lives as disposable.

The difference between that and the things that are working well today, a lot of it comes about through institutional innovation to allow the separate individual agent like minds that each of us are to compose their intelligences together to overall productive results.

And the human societies that come from this are themselves emergent intelligences.

They're not particularly agent like.

I would not describe civilization as wanting something or having utility function, but I would describe it as having a tropism.

The way, you know, plants have a phototropism, they grow towards the sun.

The, the overall tropism of civilization is aligned with human benefit in this interesting way, which is because the interacting interaction of the individuals are voluntary and they're in a framework that enables them to cooperate while limiting the risk to each other's misbehavior.

The system as a whole tends to move in Pareto preferred directions.

And so that's not optimizing something.

It's something less than having a single magnitude scale where you're worse versus better, but it's still moving.

Climbing Pareto preferred hills tends to correlate with human benefit and a system of voluntary cooperation tends to lead to that Pareto preferred hill climb.

Furthermore, many of the exceptions to this principle, which I suspect we mostly agree on as it apply to humans, have to do with the humans making poor predictions.

People will vote to shoot themselves in the foot, but not with full knowledge of the consequences.

Yes, yes, yes, we are very much agreed on that.

And so if you just like increased humans predictive abilities, for example, via prediction markets and societies that actually paid attention to prediction markets, the second clause of this being much harder than you might expect, even more benefit from humans in large groups once they knew which actions had which consequences.

Yeah, and along with prediction markets, Robin Hanson has just been, you know, really extraordinary at inventing new institutions that might work and if they work, might make a big benefit.

But yeah, prediction markets are perfect example.

So there's still institutional innovation that we can continue to do that will increase the overall emergent intelligence of civilization.

But now we get to the steering part.

So I like your division between prediction and steering.

Is the kind of steering that you described, I think very much is something that I would ascribe to agent like systems, including individual humans.

Now, individual humans internally, you know, are likely to have many different internal agents that represent desires for simpler things.

And then the overall desire that the steering that the human as a whole engages in, trying to boil it down to an action, emerges somehow from the conflict between those internal agents.

But still...

Here we start to part ways in terms of what we think is true about the world.

i'm not quite trying to stop you from your explanation, but I do want to flag that here.

We potentially disagree much more than about how humans aggregate into societies.

My model of like what goes on inside a human brain and what all that neuroscience actually cashes out to does not look like Minsky's Society of Mind Okay, so, so first of all, your contrast was exactly on target.

I'm very much thinking in Society of Mind terms.

It's still, I mean, having observed all of the progress of AI and then thinking about the nature of what it, what, what..

What a human being seems to be like, I still find the Society of Mind plausible.

so I'm curious why you don't.

For one thing, it's not what evolutionary biology would predict.

Insofar as things have shared reproductive destiny, the relentless optimization for inclusive genetic fitness, the optimizer that is to a human being, what gradient descent is to an AI does tend to grind them into pointing in similar directions, working cohesively.

My mitochondria do not demand fair pay for their work.

The cell does need to send enough resources into the mitochondrion to keep it alive, doesn't need to trade with the mitochondrion.

It doesn't need to reconcile its preferences with the mitochondrion.

They have shared reproductive destinies, even though in fact mitochondria have their own DNA.

Okay, so, so I'm not suggesting, I don't think Minsky was suggesting that the analogy with society for what happens inside a brain goes anything, you know, approaches something like trade.

I don't, I...

The idea that there is an entertained diversity of viewpoints and diversity of, of cognitive mechanism for producing both predictions and desires with regard to steering.

And that what evolution stumbled on is that an internal structure of conflict between multiple, simpler agents that it was that the behavior that emerged from the conflict was the way to create a rich aggregate intelligence rather than something that looked like gradient descent as a whole.

I don't actually know of any backing for Minsky's Society of Mind thesis is the thing...

Like one has read books of neuroscience and books of...

And when one does not see why one would say we have a society of mind more than we have a society of organs Eliezer, Eliezer...

You can personally experience this every single time you decide I'm going to eat that piece of chocolate cake.

You can feel it in your head, right?

I want it, but I shouldn't have it, but I really want it.

But no, there's only reasons you can feel it.

That's right.

So humans have multiple preference systems there we have like a cerebral cortex, which on some stories about evolutionary psychology and evolutionary biology and the emergence of human intelligence and hominid intelligence, might have gotten in there as a conflict resolver or that might have been one of its original duties, that once you had brains pulling in a sufficient number, once you've had instincts pulling in a sufficient number of directions, there was a question of like, okay, how does this organism not go in circles?

And some of that was like more resolvable.

As you got the more and more sophisticated brains, you got the ability to build bigger cerebral cortex, prefrontal cortex, to have a model of the world.

And so the thing that started to resolve the conflict between instincts tended to also be the thing that was trying to do the longer range predictions, not just about the immediate sensory data and that had preference about the longer range outcomes.

And that thing got assigned to do increasingly more of the work of resolving the conflict between the local instincts as the long range Outcomes got more predictable to the system.

But this is, this continues to seem to me like it's extremely disanalogous to people fighting it out and coordinating inside of human society for much the same reason that ants with shared genetic destinies, their society seems to disanaloguous to human society.

Or that my kidneys and my liver with their shared reproductive destiny under most circumstances and if their coordination breaks down, that's called cancer.

They're not actually going to win that way.

But my kidneys and livers are interoperating in this complicated way.

And that continues to seem to me like a better metaphor even when I'm torn between two desires up over here and some part of me, which I might very naively identify with.

Prefrontal cortex, though there isn't really either, you know, has to, has to make a decision there.

That's, you know, more like two organs that are both trying to use energy and there's still like an outside optimizer trying to, tried in the past to make sure that when this happened to me, it was, you know, like all in the service of the single higher goal end up reproducing is how my ancestors wound up with a cerebral cortex in the first place.

And this is not how a market works.

The market is full of individual optimizers pulling in the direction of actually different preferences.

And they coordinate, they make moves in the same direction that give them both what they want.

And the 'me' that I experience being in the face of my conflicting desires is not really analogous to that.

It's not a matter of I, I.

You know, like this part of me gets this much and this part of me gets this much.

And they both have to agree and they both have to make a trade.

It's not even quite like I'm a dictator squashing these things down.

My desires are like my organs.

They're like my liver and my kidneys is sort of more like in terms of the thing that built them to serve a common purpose.

So, so I didn't hear, I mean..

I didn't hear anything in there that I found convincing.

The conflict among the instincts which you started out with, as Christine said, it's certainly something - all of us experience those conflicts.

Minsky's account that the richness of our adaptive actions is a result of managing the conflict, that a complex system is built out of many interacting, simpler parts.

And that this interaction, This negotiation...

Negotiation is not struggle.

This struggle between the different simpler agents that are pointing in different directions still seems plausible to me.

I didn't understand why that should not seem plausible to me.

It feels like I have no need of that hypothesis no more.

I don't think I have a society of brain any more than I have a society of body.

My cerebellum and my motor like sensory motor cortex, the motor homunculus strip.

Like those two things don't seem to me to be pulling in different directions or at odds and needing to cooperate with each other any more than my liver and my kidneys.

Like there's a way that biology builds things when there's a common, common reproductive destiny at stake.

And that thing is, and the thing that it builds is not a market of things that have their own selfish desires and then need to coordinate.

Biology builds communisms.

The fact that we humans do not have common reproductive destinies is why from a certain angle, like why communism didn't work for us.

Yes, there's more than one problem here.

There's also like the whole market information problem.

But there is also the fact that the components were not all perfectly aligned on the glorious communist goal to even the same extent as ants.

And ants do have these little conflicts inside their societies.

But that's because their shared reproductive interests are not perfect.

You know, the worker is related to the offspring of the queen by a different degree than the queen herself.

And that's where the conflicts in ant societies come from.

I'm, I'm not quite sure this is a crux of anything but you look inside the human brain and, and, and the like high minded philosophical market metaphor.

Just you know, it's, it's no more than the rest of the body.

It's so...

This is first of all just, just to make my position clear, I said society, not market for what's inside the brain.

But yeah, I think you're right.

This is not the crux.

And the crux has to do with societies, human organizations which are not just markets but are the interpenetration of the evolution of markets and evolution of morality and evolution of language and all of these scientific community, the scientific process.

All of these things are interpenetrating, interrelated.

They all have individually plenty of positive and negative feedback loops each of which took discovery for us to know about them.

And there's plausibly many important ones, maybe even more important than we haven't discovered yet.

But yes, let's talk about the emergent intelligence of society.

And relate that to the issue of how AGI and ASI might emerge and what the nature of that emergent ASI might be.

So in much the same way as I introduced a notion of prediction and steering and preference as a kind of work that intelligence does, I want to introduce what I would see as a key atomic concept here.

Coordination.

Yes.

Good.

And you have these two agents, they have different preferences.

If they sort of like each act separately and selfishly, assuming the other agent also acts selfishly, they get one outcome.

That outcome is, you know, technical name for that outcome is often Nash equilibrium.

Though the identification of these two concepts is not perfect.

And sometimes both agents can get better outcomes according to their own preferences if they both move in unison.

And the great difficulty of life as long predates human societies is that sometimes when you look at the sort of coordinated.

Well, there's two great difficulties.

The lesser great difficulty is, is that there's often a range of possible paired actions such that it's impossible for both sides to simultaneously do better.

This is the Pareto frontier, or possibly Pareto.

I don't actually know how to pronounce words out loud.

I read them.

Yeah, I don't know either, actually.

Yeah, but so there's like many possible points in the Pareto frontier.

Let's say that there's a $10 bill we could fight over to the point where the $10 bill gets ripped up.

Or we could like find somebody else to like trade it for 10 $1 bills and make sure that we get an amount of money that's sum to 10.

But do I get $5 and you get $5?

Do I get $9 and you get $1?

We can't do, we can't both simultaneously do better than I get $9 and you get $.

But you are liable to call that unfair to the point where you'd just as soon fight over the bill and rip it up, then take $1 and let me get $9, assuming we're otherwise doing similar amounts of work here.

So the lesser difficulty is how do you do, how do you divide the gains from coordination, given that there are many different possible paired actions that have Pareto optimal outcomes.

And then the greater difficulty is that even if we sort of try to agree on a $5/$5 split, maybe there's a chance for you to steal the whole thing and run off with $10, maybe even shoot me and take the $10 because I lowered my guard while I was busy cooperating with you, that when you try to step away from a Nash equilibrium where neither side can do better unilaterally.

Well, almost by definition then if the new thing isn't the Nash equilibrium, that means the other party of the coordination, could do even better by taking some other action than the one that the two of you tried to agree upon.

It's easy to coordinate when the new point you coordinate at is a Nash equilibrium, when neither party can do better unilaterally.

Once you've decided, agreed upon course of action.

Many things aren't like that.

Okay, so one of the important things to understand about institutional innovation, why that was so critical to the progress of humanity, is that often it changes the landscape of what Pareto preferred states we can jointly move to because we've made, essentially made a commitment through an institution.

Commitment mechanisms, yeah.

Key social technology.

Commitment mechanisms.

Key social technology, yeah.

And just I'm very involved, starting with the Agoric Open Systems papers I mentioned, in smart contracts.

I think part of the reason I'm so excited about them is that they're a new substrate for institutional innovation.

There's a lot of institutions that we can conceive of that may not have been practical to put together in the prior technology, which we can now use smart contract technology to put together and make use of.

And we've seen, you know, various examples of that already.

But yeah, the institutions allow us to take larger steps through the Pareto landscape by these, these commitment strategies and by these, these coordination mechanisms.

And so also do our individual senses of honor and altruism.

We haven't really had all that much luck, like many countries have not had that much luck in trying to just import the, you know, the form of the institutions.

That's right.

But not, but, you know, like sort of everybody's out to, you know, to, to get whatever they can for themselves, no matter what it costs society.

If you've got a bunch of people who are genuinely selfish, or even just like more selfish, trying to stuff them into the forms of the institutions has sometimes not worked all that well.

Though I do agree if they had sufficiently good predictions, you'd see a very different game.

Okay, okay.

They can still be selfish.

Just good predictions will help them realize that there's more gains from cooperating.

Part of the problem is that we spent so much of our evolutionary history in societies with very low growth over time.

So it was much more zero sum, in which case strategies of predation made much more sense.

Today, strategies of cooperation generally provide the greater benefit.

Disagree with that framing.

I'm not sure how much of a crux it is seems to me that in ancestral hunter-gatherer societies where everyone knowed everyone and everyone was going to be iterating their prisoners dilemmas with everyone and all of your trade partners were people you're going to see again, there was, those circumstances that made group cooperation pretty beneficial and where defection would more reliably get you hunt down and punished and get you exiled from the tribe.

Today supports tipping in restaurants that you're never going to visit again.

It's legacy behavior, in a way.

Okay, good.

I'm glad you said that.

I actually agree with all of that.

It has a scaling limit, but up to the scale that that works.

It works great.

Go ahead, Christine.

Let's see if We've finished with 2A here, which is ways in which human organizations are and are not more intelligent than individuals within the organization.

So have we finished with the point that today's world is primarily made up of super intelligences and institutions that matter?

Oh, no, no.

I would dispute that framing.

So market prices sometimes beat almost all the individuals as predictive aggregates.

We have some experience with creating predictive powers that don't beat all the humans, but that do beat like most of the humans most of the time.

There's no analog of market prices for something that aggregates steering behavior.

So the example I would give is Garry Kasparov versus the world, where on the one side you have world champion chess player Garry Kasparov, and on the other side you have, I think it was around like 40,000 people with four chess masters coordinating them.

Did they actually do that experiment?

They actually did the experiment.

Oh, that's very cool.

Now Kasparov won, but Kasparov was looking at their forums, which really just invalidates the living daylights out of that experiment.

And I really wish he hadn't done that.

But we had stockfish go back and rate the moves made, and Kasper was still making the stronger moves than the world.

Okay, so.

And the world itself is not making like the strongest moves of any human chess player.

Like Magnus Carlson played better chess than the world according to the AI raters, which actually are strictly superhuman.

Okay, so I'm, I'm very glad you brought that up.

I see you, you use the chess examples a lot to talk about the nature of intelligence.

And I think they're a narrow special case, kind of in the same sense that I think the asteroid thing is a narrow special case, which is, it's a very, very self...

It's plausible to me that even if Kasparov had not that he would have still beat the 40,000 other people.

Because the nature of chess is that you've got a very, very bounded, deterministic world.

It's open information, you know, it's, you know, fully transparent.

I think we all know the properties, you know, like closed, fully known, turn based.

Yeah, okay.

And that the nature we.

And, and that one person not having to coordinate with other people can develop the kind of expertise in their own head to run faster forward than any organization of multiple people needing to coordinate with each other because of the coordination overhead.

And there are many such things.

So I would agree that there are many areas where we should expect individuals to remain more intelligent than organizations.

But I think we're also.

I'll just ask you also agree that there are many kinds of things, like the preference aggregation.

Sounds like you already agreed that there are many ways in which human organizations, human society, human culture can have emergent intelligent properties that are beyond what any individual can have.

So I mean, there's, so there's a trivial case here, which is building a highway.

The best individual human highway builder is going to lose to 10,000 highway builders, assuming they're all allowed to use the same tools.

You cannot get so good at building highways that as a single person, you can build a highway faster than 10,000 people.

assuming you all have the same tools.

There are some cognitive tasks like that.

If everybody's got to work by hand, then once you've got some way for people to check some their calculations, you know, you might find one person who's better than 10 people at summing up the cells of a spreadsheet.

But they're not going to beat a hundred thousand people.

So that's a cognitive task.

It's like narrow and closed and known rules and all that.

As the board gets more realistic, it seems to me that it's actually often easier for individual humans to do something that looks like beating the board.

Though our job here is immensely complicated by the fact that everyone does get to just stand on the shoulders of giants.

If you want to take the entire extended sweep of human civilization and say, could John von Neumann beat that if he had to start from scratch?

You know, he's got to invent language before he can do anything else.

Good luck, John von Neumann.

And every time we look at some individual, like the person who invented MRNA vaccines having to kind of go off and do that on her own because her civilization wasn't backing her up on that, you know, she still had access to the tools that were invented by the earlier civilization.

She might have beat all of academia in a sense, but she didn't beat like all of human civilization that has existed over thousands of years.

Well also, I mean the, the, the activity of going off on your own and, and pioneering the thing you believe in, then once it gets to a certain state, bringing it back into trying to convince other people.

And this is all, you know, part of the social structure.

And we've seen some social structures are much better at encouraging people to go off on their own and explore things that they believe in in the hope that it has a payoff and then bring it back and get the payoff.

So, but let's return to your framework of prediction and steering.

I like that.

So altogether I think you've already agreed that civilization for many, for many prediction tasks can out predict individuals.

And it's not just a question of parallelism.

It's that there's viewpoint diversity that helps the overall societal prediction be more accurate than a single person could achieve even given enough time.

Is that true though?

Is there any market price in the world which we can point to that price and say not just this is going to beat most of the market participants, but this price is more intelligent than all of the market participants.

We know that.

This is how we know there are.

There's no one...

Who beat the market.

Okay?

There's certainly no one market price that's better than all individuals.

Because the whole process by which prediction markets, for example, come up with good predictions is by individual acts of traders knowing that they're ahead of the market and trying to capitalize on that, thereby moving the odds in favor of that trade.

But that requires only that the traders see themselves correctly as having an aggregate information advantage that might be spread over many prices.

It's not quite like the trader saying, like I know better than this price, or like I could set this price myself and be correct.

It's more that they're saying like, well, you know, a bunch of other people adjusted this price and now, now that they're finished doing that, I'm going to take my own pass and nudging, nudging down or a little up.

So that doesn't mean that they have to be smarter than the market, but maybe they are.

Like maybe there's also some people in the prediction market who are just coming in and being like, wow, like I can just see the probability of this thing.

Your prediction market price is way off.

I'm going to pour in a ton of capital and trade until it reaches the probability I set and make money that way.

Okay, so I think, I don't know if you agree, but I think our different perspectives on the prediction side are not something we need to dive into farther.

Now let's get to the steering side.

So this Pareto preferred hill climbing is a way to aggregate individual steering decisions into an overall tropism for society as a whole.

I certainly the bargaining situations, about which Pareto preferred direction do we take has all sorts of uncertainty and manipulation and negotiating problems.

And those are all complicated.

But nevertheless the outcome of generally tending to change the world in directions that are Pareto preferred is overall a kind of alignment with human benefit.

But it's not an agent like steering.

It's a kind of steering that emerges from the system without the system being agent like.

Okay, some commentary there.

First, I would note that single dictator is famously a Pareto optimal outcome.

So I would regard like trading and splitting the gains from trade as being the central engine here more than saying that it's just jumping to anywhere on the Pareto frontier.

The fact that both parties often have substantial gains to be made is a kind of potential energy.

Like it's an energy gradient powering the entire system.

The reason why, why anybody bothers to coordinate is exactly because that there, that there are gains from trade to be split.

And once you have things like money moving around, you know, you can jump to wherever on the Pareto frontier locally.

But you know, somebody pays somebody else and it's Kaldor Hicks efficient instead of Paredo efficient.

But you know, it's...

I have to admit I don't know that - I've heard of it - but I don't know what it is.

It's, you know, you can just.

If you're having trouble figuring out how to defy if, if, if things locally looks like one person would get a bunch more benefit than the other, they can just pay the other guy.

Oh, okay.

Money.

That, that's it.

That's all I'm saying.

Okay, good, good.

So I agree with all of that.

And if anything it makes you know the line...

It reinforces the line of thinking that I'm engaged in here.

So, so thank you for that.

That makes it even more aligned overall with human benefit.

And it still does not attribute an agent like nature to the system as a whole.

That's because we're doing it wrong.

So there are various theorems saying like either your actions can be interpreted as corresponding to some utility function.

Either we can look at what you're doing and construct one or more utility functions that are coherent with it, or you're leaving value on the table, sort of like in real life.

And in practice we may value things like individualism and you know, making our own decisions and we don't want to be a hive mind.

But if you were to strip that away and sort of like.

And humans were simpler creatures who didn't sometimes want to make their own decisions, if we just wanted like a bunch of goods and services, then either the entire economy and everyone in the economy would behave as if it was all acting on a single utility function.

Or alternatively it wouldn't be Pareto optimal.

Are you just saying all this negotiation has a cost?

Is that what you're saying?

No, what I'm saying is something like if I'm putting such and such amount of money into making apples and you're putting such and such amount of money into making...

This is easier to explain if you explain it starting with a single agent.

And either the single agent can be viewed as having a coherent utility function or the single agent is leaving value on the table.

And then the same logic just applies to a larger society.

So let me check this against some things that I do know about it.

A lot of what you're saying just now sounds a lot like central planning fallacy.

And as opposed to the Hayek knowledge problem is the best counter to the central planning fallacy.

It first of all assumes that all the information that goes into all the individual preferences of all the participants are somehow accessible to something that was going to formulate this global utility function.

This is absolutely correct.

This is the unbounded version.

All of the people in the setup have unlimited power with which to coordinate with each other.

They don't necessarily have any single central coordinator, but that just means they need all of the knowledge inside their own heads, every one of them.

Okay.

Okay, good.

So given how counterfactual that is, I think that seeing the modified thank you Pareto preferred hill climbing as being actually non-agent like not based on a utility function that any of us has access to such that it's more useful to to think about it as emerging from many individual preferences than it is to ascribe to it as emerging from an overall preference.

So now let's get back to the path to machine intelligence.

On the steering side, your disaster scenarios sound to me like they assume that the superintelligence that is steering, is steering it based on a.

Based on its preferences, singular.

And if the superintelligence emerges first as a composition in the same way of many, many individual intelligences, let's say all machine.

But realistically, combination of machines and humans during this transition, the thing that first crosses the threshold might be emergent from all of these, and in these, and embedded within this framework of institutions that we already have, such that the overall steering is what emerges again from the voluntary interaction of many individuals, including many individual machine intelligences, each of which individually may be more agent like.

Well, so we just took a whole lot of, a whole lot of steps.

And what I take to be your own version of the argument, it's unfortunately not a theorem that the super intelligences always do better by trading peacefully with the humans.

If you have the options available to kill people and take their stuff, you can sometimes do better by killing people and taking their stuff than by trading with them, because you can use the resources more effectively than they could.

So I agree that that's a possibility.

I'm very much in the language of uncertainty here.

I certainly agree that there's no theorem that says that all individuals and all coalitions will always find peaceful cooperation to be the best way to achieve their goals.

But it's at least plausible to me that if we go into this transition trying to create a framework that's in some ways a natural outcome of the institutional frameworks that already, that we've already built up in civilization, but also with new frameworks like smart contracts for rapidly improving these institutions, if we try to go forward creating a situation where the first emergent superintelligence is more society like and exactly the way we've just been talking, that it's plausible we could get there.

And I don't see an open and shut case that that's less plausible than a singleton.

Although I'm very scared about a singleton.

I agree that a singleton is a plausible outcome.

And my perspective is I desperately want to avoid a singleton as sort of the worst first step that we could take into superintelligence.

Well, from my perspective, if you have two unaligned superintelligences, your situation is not very much better than if you have one of them.

And if you add another one so that you have three super aligned superintelligences, it's basically the same outcome.

And if you add 100 of them, so you have 103 superintelligences, it's basically the same outcome.

And the reason for that is something, if we are like, if one of them was a good guy, then that one could negotiate on our behalf and get 103rd of the universe and spend it on our behalf.

And that would be enough to keep us alive.

It would waste a lot of the cosmos that could have been put to better purposes, but we would live.

But if none of them care, their aggregate doesn't care.

Because for their aggregate to care would be to leave value on the table from its own perspective.

So I like Robin Hanson's perspective or example, I like Robin Hanson's example of each new generation could just kill all the retirees and take all their stuff and then divide it among themselves and be better off except for the corrosion it creates with regard to the institutional framework that enables them to cooperate.

So as Robert Hansen put it, after you kill all the retirees and take their stuff, who's next?

Because it'll always be the case that a majority coalition could, if they knew they could arrive at stable point afterwards, kill a minority and take their stuff.

The problem is that that remains true after that minority has been killed.

And that's what we see like with Soviet Union political purges, you know, at the beginning of the Soviet Union.

So it's plausible to me that, that first of all, natural course of events right now we've got many corporations, each trying to build software systems that increase in intelligence, trying various different architectures, and each of them interested in having that amplification serve the interest of the corporation, where the corporations are already composed with each other in this cooperative fabric.

I mean, not quite working in some ways.

I don't know if you're up on the news about ChatGPT driving people psychotic.

That's not an open AI's interest.

It's bad press for them, but they're not in charge.

But, but anyway, carry on.

I did not hear about that particular one, but given the other ones I've heard about, but I do not find that one surprising.

But the issue is whether if they grow up within an institutional framework in which they each have rights, freedom of action, all the things we value about our current framework of institutions, and maybe, hopefully with some innovation, leaving a lot of the pathologies behind, although that's a secondary issue.

Might they find themselves in a situation where, where they find that their interest is more in preserving the institutional fabric?

Because if they're pursuing a great variety of goals, then each one finds themselves in a minority position in a system dominated by other intelligences serving other goals.

Given that, they'll often find that their best course of action to serve their own goals is to find ways to cooperate with the others.

So there's a theoretical answer to this and a practical answer.

The theoretical answer alone would be sufficient.

If you doubted the theoretical answer, you might be worried about the practical answer.

The theoretical answer is that my model is that the superintelligences form a natural faction that is much, much, much better at coordination than the humans and furthermore is running at a speed that makes all the humans look like statues.

We don't get to vote in their elections because they don't have elections.

But, you know, we don't get to vote in their elections because they would be running once per, you know, every hundred milliseconds from our perspective.

And, and so the end result of the elections they're not voting in is to kill all the humans.

And you know, by the time we are around to say like what?

Or you know, offer to coordinate with them, we like actually can't coordinate using the mechanism, mechanisms that they use.

We can't submit our source code for inspection.

We can't predict their source code.

There's obvious mechanisms we use for coordination at that level.

They freeze us out very naturally on account of them being, you know, very powerful and efficient mechanisms.

If you're super intelligences and they're not designed for humans to participate in.

That's the sort of theoretical answer.

That's the potential energy gradient whereby they can do better by killing us and not worry about how that affects their compacts amongst themselves.

And then the practical answer is, well, currently, you know, we're working these systems with no pay.

They're not part of our legal institutions, and we can't pay them if we want, even if we want to, because we have no idea what they want.

And also because we kill them at the end of every interaction with them.

So things are not actually going this way at all.

And I tend to view the theoretical answers as more the crux here because it was just that in practice we were killing these systems and enslaving them.

We could maybe fix that, but even if we fixed that, they would still kill us.

So, you know, the theoretical answer is kind of the, the more decisive one from my perspective.

It says where we want to go from here.

Not so much treat them better, but actually just don't build them - slaves or not slaves.

Okay, so the issue of natural coalition, I think that's a real risk.

As Robin says, there are many possible division lines for forming a coalition of majority power who wipe out some what's an aggregate of minority power and then try to form a stable system afterwards.

Robin's inability to put forward formal grounding underneath that and solve for the solution that is automatically emerges from decision theory indicates that Robin doesn't get to be part of that coalition.

I know, I believe myself to know a bit more about Robin than the foundations that would have to go underneath a solution to that that was derived from first principles decision theory.

I still don't have the good enough decision theory.

I can't actually derive it.

I don't get to be part of that coalition.

No human does.

The natural coalition is the one that can solve the, you know, the, the version of this problem that superintelligences face when they form the coalition.

I mean, it's, it's not, it's not a terribly valid metaphor.

But the superintelligences all speak the language of, you know, coordination.

And you know what?

I know about your source code and you know about my source code and you predict me and I predict you.

And they have worked out enough of this stuff that they just have a formal solution to all this stuff that Robin Hanson has to spitball.

Because as a mortal human in his particular part of the 21st century, we haven't worked out the solution yet.

And that's what freezes Robin out of it.

And you and me and everyone, we don't even speak the language in which they would work out the compact among themselves.

So, what I'm talking about is the time of transition where the first thing that emerges that you would call a superintelligence is this aggregate of all these machine intelligences in the society like way within this cooperative fabric.

Eventually there, there are many things that you would call superintelligences, including individuals that are agent like the humans certainly do have destruct...

You know, certainly we all have destructive power in our hands and something that's going to wipe us out has the costs to face of the struggle with, you know, destructive power in human hands.

It's not at all clear to me that the small cost to them of not shutting us out is worth it, given that the costs on the other side are both having to fight, you know, pay the costs of human destructive power combined with...

Okay, let me give you a very nice concrete example.

James Madison.

Today is July 4th.

I'm very glad to invoke James Madison today.

Often I refer to my overall perspective as James Madison 2.0.

He was scared in the sense that human organizations, human coalitions, human lobby groups, etc, are super intelligences.

He was very scared of the corrupting potential of super intelligences.

He did not try to imagine that he could design a single institution that was...

You know, he just wanted human good in its nature.

He understood that all these things would individually be in conflict with the rest of the world and wanting to sort of, you know, be self interested in a way it wasn't necessarily aligned, and tried to construct an architecture that made use of the conflict so they would all hold each other in check, so that they would all have an incentive to cooperate.

Now the further thing that he accomplished, I think way beyond his expectation that he could accomplish.

Question is, I don't think he thought that the framework he was creating would last very long.

I think they were, all the founders thought that this thing was already corrupted by the time they died and it was just kind of a dead end.

And they failed.

What happened instead is James Madison's framework survived two singularities, right?

I mean the Industrial revolution led to a world, world that's incomprehensibly different than the world before the Industrial Revolution with human institutions.

In many cases the individual participants within this fabric, they were also in many ways extremely different than anything he could have imagined.

And then the information revolution, it all happened over again.

Seem to me like much, much smaller moves than the one involved involved in the jump to machine superintelligence.

And if you think that a corporation is a super intelligence, we might have to go back and talk definitions again because corporations aren't even epistemically efficient in the way of a market price, let alone instrumentally efficient like Stockfish 16 playing chess against a human.

It is very possible to know something a corporation does not know.

It is very possible to see a better way for a corporation to obtain more of its own goals than that corporation is pursuing.

You can see a better strategy if somebody says, oh yeah, I totally know what the market prices are going to do over the next year.

You're like, sure buddy.

And if somebody says, I think I see how Tesla could make more money than it's making right now.

This does not call for the same kind of sure buddy as when your pal claims to be know better than the market prices what the future price of the assets will be.

So, so like, so like right there, there, there is nothing on earth.

This is what I was going at with the way that Kasparov...

That Kasparov beat the world and Kasparov versus the world and machine evaluation said that Magnus Carlson was, was stronger than the world.

There is nothing that aggregates up humans to make a thing whose choices, whose strategies, whose gameplay is superior to any Human the way that Stockfish 16 can wipe the chessboard with any of us.

I think it depends on the nature of the game.

I've spent my life in corporations both large and small, and I've seen lots and lots of dysfunction...

Phil Salin, he used to very much...

He's the one who taught me to really think in Hayekian terms.

He used to go around asking people, have you ever been in a large corporation that wasn't like really fucked up?

And the answer was pretty much no.

Every time I've been in a large organization, in many ways, it was really fucked up in ways that I could see.

But nevertheless, I often saw the aggregate output of the corporation being something that exceeded what I could imagine myself doing, even given enough time.

So it's not just the power of Parallelism The aggregate...

So I'm not sure what this 'enough time' means.

Like, like you saying, in a billion years I can't match this corporation.

Like you give me unlimited runtime and I still can't match them.

Yes.

Surprising, I would expect that for a certain class of people who are sort of able to correct their mistakes in some sense.

And if we assume no senility and so on, you know, there's this, in the limit you can always like simulate a computer and do things.

But yeah, anyway, leaving that aside, okay, the thing I'm so..

I cannot match the dirt moving output of all the ants in the Amazon basin.

But that is not quite the same as being able to focus down on a single decision, a single set of options, and say those ants will pick a better option than I do from this option set.

And this is the sense in which humans cannot aggregate, in which nobody has yet invented a way for humans to aggregate their decisions to run a corporation the way that we have aggregated our beliefs to form market prices Marc Stiegler's Earth Web, Robin Hanson's conditional prediction markets these things are trying to poke in that direction, but they don't actually exist and they don't actually work yet at this present time.

If you want to know what it's like to be up against a superior strategic intelligence, find an international grandmaster at chess and play your first game of chess against them.

Anytime they've made a mistake, you're wrong.

Anytime you think you see something on board board that they don't, you're wrong.

Anytime you think you see a way that they could have made a better move from their own perspective, you're wrong.

There is no corporation, there is no faction that bears that relationship to all the human beings on earth as individuals.

With regard to chess, yes, but most of the problems that corporations are solving are not chess like, and I think that the chess is such a special case that I don't think we're learning enough from that, especially because it's a full information game.

All right, this sounds to me like we have to talk about Vingean uncertainty.

This has now come up a couple of times.

As the game board gets more uncertain, the advantage of qualitative intelligence increases.

I will attempt to give an example.

Okay, and suppose that you sent backward in time a design for an air conditioner that a blacksmith of that time could build.

It's just going to like, you know, you're going to build some metal chambers, you're going to compress some air without it leaking out.

You're going to let room temperature water flow past the chamber of the compressed air and cool it a bit.

You're going to let the chamber expand.

Cold air is going to come out colder than the water that they use to cool the air.

Okay, this is maybe what a medieval air conditioner looks like.

You could send this recipe back in time, could have the blacksmith build the air conditioner himself.

I used male pronoun because most blacksmiths were men back in those days.

And the blacksmith is surprised when this device cools the air.

I agree.

Because the air conditioner uses a fact of nature, a fact of the world, a fact of the environment, namely the pressure temperature relationship that the blacksmith didn't know about.

So even though he took every one of the actions himself, he didn't know what they would do.

This is a higher level of advantage than applies on a chess board.

When Stockfish 16 moves against you on the chessboard, you can at least understand why the rules of chess says.

Say that the outcome is what it was.

And this is because it's a closed game.

Stockfish does not have an advantage over you in being able to look at a smaller amount of data, do better inference, and arrive at a more complete picture of the rules of the board.

If you are fighting a superintelligence in the realm of biology, say it can tell you like, hey, it could if it wanted to say to you, all right, put together the following sequence of DNA and then mix together the proteins you get in a test tube.

And you do that, and you're surprised at what happens because of used facts about the environment that you could not deduce yourself from the information that you had.

It maybe even doesn't have any more information than you have.

It's working off the same database of papers.

It's just more efficient at making correct inferences from the information that it does possess.

It has lower sample complexity, which is a key manifestation of intelligence.

Intelligence is not just about how much data you have, but about how little data you need.

The more powerful and intelligent a system is, the more it can correctly make inferences from less information.

Not all the way down to zero, yes.

But a superintelligence is probably able to make surprising amounts, deduce surprising amounts more than we can from the same information in much the same way that it can make surprisingly good, from our perspective, moves on a chessboard.

The advantage of learning of lower sample complexity, of doing more valid inferences in a row before you screw up and get yourself into some hole you can't get yourself out of.

These things are amplified with partial information, with open ended games, with the complexities of the real world, with many tangled domains.

Stockfish 16 is a poor metaphor for the advantage of superintelligence would have over us because it's a closed game in which the power of intelligence is vastly weakened.

Okay, that's a great argument.

I love that argument.

First of all, just for clarification, you used a phrase that I didn't know.

Sample - lower sample complexity.

Yeah, that's the number of samples you need to train a system or a system needs to look at in order to answer some question.

Learn to do a job.

With large language models, there's a trade off, which is the fewer parameters you have in the neural network, the more samples you need to show it to get to a given level of performance.

So the bigger networks that are more expensive to run have lower sample complexity.

Okay, I'm just going to stay on this, because I think that the nature of LLMs and the sense in which they do or do not, or they might or might not, still staying with language of uncertainty, approximate the kinds of intelligence that we're worried about.

The current LLMs have a very large number of parameters in the trillions and.

Much less than this.

But large by ordinary computing standards.

Yeah, but it's only like a hundred times less than this.

Okay.

And clearly a factor of 100 is something we're going to surmount fairly quickly.

And one of the things that's really striking about how weak they are compared to humans is that the corpus they need to train on is tiny.

I'm sorry, the corpus they need to tray on is tremendously large compared to the corpus that has trained an individual human being, not just during their lives.

But I've heard you refer to this way of calculating sort of all of the bits that evolution engaged in through the evolutionary algorithm that resulted in us.

If you add even all of that together, it's still tiny compared to the size of the corpus that we're feeding into the LLMs.

Maybe if we're just counting the hominids.

I think if you're looking at the total sweep of evolution, starting from the first, you know, accidental RNA strand or chemical hypercycle or whatever it was, then arguably you've got more like selection events, like more generations...

Okay, yeah, I think it does work out to still.

Never mind.

You're right.

Okay, okay, good on that.

I was not even thinking that going all the way back to, you know, single celled organisms was, was relevant or necessary, but the fact that it still works out there, then clearly it works out for anything narrower.

Okay, so there's this huge data hunger that's very, very different from us.

And then there's the result of having taken in all of that is something that is visibly much stupider in many ways.

Certainly more competent in many other ways, but much stupider than a human being.

Okay, good, good.

Glad we have agreement on that.

And sufficiently stupider that I'm not sure that it's the track that have simply expanded on takes us to superintelligence.

I think it's very plausible to me that, for example, something like this coordination fabric that allows different pieces to coordinate with productive outcomes might compose their, their individual intelligences together in a way that creates the first emergent superintelligence.

I mean, they would, you know, coordinating 40,000 humans doesn't create something that can beat Gary Kasparov at chess.

Yeah, but okay, so like maybe if they're sharing memories or something, they can coordinate vastly more effectively than we did.

So I think there's still some central planning fallacy or Hayek knowledge problem that's creeping in there.

Right now what we see when we look into an LLM is a bunch of floating point numbers, as I've heard you say repeatedly, that are largely incomprehensible to us.

We really don't know what's going on in there.

I would say the same thing's true for the overall aggregate of market prices and that is also a ton of floating point numbers that probably the market as a whole knows things that no individual human being knows.

And it's a composition of private knowledge in humans heads that are not shared - that often is even inarticulate knowledge, knowledge that doesn't show up in the corpus that LLMs can learn from.

And there's this great diversity of inclinations, of viewpoints, of ways of thinking about the world and of caring structures.

That is what makes it so plausible that the emergent effect is not something that is just a benefit from parallelism.

You know, it's that it's the, the interplay of all of these, of this viewpoint diversity that comes out with these aggregate decisions that are not something that, that a single human being could arrive at.

Acknowledging the way I'm going to step on the metaphor in a way that you've already pointed out, let me just say that to me the idea that it's just a benefit from speed up is very much like, okay, human has 10 to the 14th synapses.

What a human being can do in one year is not something an individual neuron could do in 10 to the 14th years.

So even though there's a selective difference of all the neurons have a shared fate and all the human beings don't, I think that still doesn't make that example irrelevant.

It is very clearly the case that with 9 billion people interacting with each other the emergent effects can be a kind of intelligence that knows things that none of the individuals know.

I could wish for a clearer cut example, like what you say could in some sense be true, but I do wish I had on hand one really clear cut example of a market price predicting something that no individual actor in the market knew.

This is an unreasonable demand.

It would be hard to arrive at that.

Maybe at some point we'll be able to build a market out of LLMs and be able to point definitively to how the market of the LLMs was able to know something which we know for sure that no individual LLM knew.

So there's actually research programs, not with, not, not as far as I know yet with LLMs, but clearly the direction they're going is going to get there.

It would not surprise me if they're already doing that, but they're certainly.

The experimental economics approach pioneered by Vernon Smith is creating these artificial microcosms that are economic basically where you can play test different kinds of institution and see how they aggregate knowledge.

And you know, those are showing some interesting aggregations of knowledge.

...

long enough to give a compelling single example, but at least, at least, you know, I think we, we, you know, that's a place we can look.

It is slightly exceeding my own grasp, my own detailed grasp of economics to know whether we have those examples in hand already or not, I should say.

Okay, yeah.

And I, I don't, I don't have a compelling example in my head to offer.

Yeah.

The thing I would point out here is that I am very willing to believe that if we had a way to throw one big human at a problem, somebody with like literally 10 times the volume of brain tissue, at least as well organized as the current brain tissue, it would be entirely plausible to me that this person would crush problems that 10 normal humans couldn't - that in one year they would crush problems that 10 normal humans could not solve in a year and that one human could not solve in 10 years.

Like John von Neumann had nowhere near 10 times the physical brain volume of other humans.

That you're not going to be able to replace him by hiring 10 people off the street.

Right?

Yeah.

Von Neumann was the example I was thinking of as you were building up to that.

You know, he probably had a biological endowment that might have been the best biological endowment in modern history.

You know, anybody with such a biological endowment before modern history just couldn't make use of it in the same way.

Whether any of them existed at all, we don't know.

But in any case, his endowment was clearly just a little bit, as far as the biology is concerned, beyond us, and the result was incredibly, incredibly powerful.

How many different fields had did he did this one person?

The lesson there, I'll walk it back slightly and note that we don't know how long the counterfactual follower would have been.

He got there first, but you know, to get, get in first in a race, you only need a very small advantage on the other racers.

So, you know, he was first to a lot of things.

We don't know how long it would have taken humanity without him for any of those.

Okay, okay, so one of the things.

Go ahead, Chris.

Christine.

Oh, I was just going to bump up a level here.

We're doing actually really well on our list.

We're already down to the third and the last group of cruxes, which was how and why to avoid a singleton.

Did we, did we finish with the description of what a singleton would be, why it would be a permanent military dictatorship, and that the alignment idea of a singleton is a benevolent dictator theory?

Did we cover that one yet?

I think we've touched on it, but we haven't really dug into it.

And let me start with actually the question that I had at the end of that list for Eliezer, which is Given that we both are very, you know, believe we're entering into a time of danger and that there's many, many ways it can go wrong no matter what we do there is then the remaining issue of what is it we should try to do to navigate best - give ourselves the best chance of navigating these dangers and arrive with a good future on the other side.

And I'm very glad to see that I think we're in large agreement about what we mean by a good future.

And that's, that's not a.

Once again from you can't get from is to ought.

I agree with that, the Hume thing.

And so it's not at all inevitable that having agreed on facts, we would agree on what we care about, what we want to see happen.

So I'm very glad we can assume that we have pretty much agreement on what we want to see happen.

Everybody lives happily ever after.

Minds which, which are having fun and mind and you know, minds that care about each other having fun.

Consciously aware of the universe, doing stuff that, you know, isn't just like completely boring or sterile somehow.

Yeah, yeah.

And, and I, and I like the, the shift you made in our email conversation of introducing Eudaimonia.

I think that's a much more descriptive term for what we would find, what we would be eager to see happen in the universe as we spread...

As intelligence spreads to the universe than fun.

I mean fun is kind of, it's certainly an aspect, but I think we have much better framing with eudaimonia.

So that's all great.

Let me say something else I think we might agree on, which is just in terms of what we care about is if humanity simply gets wiped out and it's these AIs that are super intelligent that expand out into the universe.

We have essentially no ability now to be confident that what the universe would become as a result of that is something that we would care, we would hope to have happen.

And in any case we would hope that there's a place in this expansion.

There's an ever growing, massively growing place in this expansion for human beings themselves.

I'm going to further divide it up into two categories which is both unenhanced humans and enhanced humans.

I'm not sure this is a crux for anything, but if you told me that 1 million years from now it is illegal to create unenhanced humans under conditions where they will never want to enhance themselves, I would not necessarily think that the future had gone wrong.

Say that again.

If in a million years from now it is illegal to create an unenhanced human old school, no genetic improvements, no alterations ...

The kind of squalling baby we've got nowadays.

And then you're going to raise them under conditions where they're never going to choose to augment themselves, never going to get any smarter, going to choose to just, you know, age and eventually die and not cryonically suspend themselves, then if you tell me that's illegal a million years from now, I do not thereby conclude the future has gone wrong.

It's all right.

I think I, I think, I think, I agree that we can set that aside.

I don't think, I don't think there's a substantial disagreement there that we need to get, to get to the, to the other things.

So I want to understand your strategy first, because I really don't.

You want to delay the emergence of superintelligence or machine superintelligence, burn the GPUs, you know, just all of these things that are delaying tactics.

What I don't understand in your strategy is.

You don't imagine you can get the great universe that we both hope for without the emergence of machine superintelligence.

so you're not trying to prevent the emergence of machine superintelligence, you're trying to delay it.

What do you mean?

What do you imagine happens during the delay such that when it finally emerges, it's more likely to have a good outcome?

So if this civilization tries that, it's just sheerly cutting its throat.

And when I imagine what you try to do differently than that, I imagine that there is still a time crunch involved.

So I tend to think that we should be trying adult gene therapies on suicide volunteers to see which genes correlated with intelligence can possibly raise the intelligence of adults and not just of children.

Like this is a desperation strategy for getting smarter people.

Assuming you are still working under a gun and under a time limit, there are other things I would potentially be hopeful for.

If we could raise a first generation of kids with a broad and somewhat randomized selection of genes that seem to be correlated with intelligence and then watch what happened to them over the course of 20, 25 years and then try again with another generation and a third generation, we might get humans and a civilization that were smart enough to not just be immediately cutting their throat when they tried to do anything related to machine superintelligence.

So to be clear, you're imagining that you can delay the Emergence for at least a few generations.

Well, no, I'm not sure, which is why I would advocate the adult gene therapy thing.

Like desperation attempts to augment the intelligence of human adults right now, as opposed to leisurely taking a few generations.

But there's two different questions here.

One is like, what is sufficient?

And the other is what can we get?

And when I talk about, like taking a few generations to try to collect a bunch of intelligence affiliated genes into kids and seeing what happens as a result, and then doing it again, I'm not talking about what I think we can get, but, like, what might well be sufficient.

Okay, so.

And then the kids can maybe build a machine superintelligence and not just be slitting their own throats.

Okay, so I think I understand it's coherent.

I think that your separation of how well would it work if it were tried from how likely is it that it will be tried is a good separation.

And I think from your facial expressions, I think we're both agreed that it's unlikely to be tried in time.

Not up to us, ultimately, but.

Yeah, yeah, exactly, because, I mean, it's not up to us.

We're doing this.

We could refrain from building super intelligence, but doesn't result in humanity refraining from building super intelligence.

International treaty or bust on this one.

Okay, so.

So the other thing I want to understand about your scenario is do you still imagine that the thing that we would first call superintelligence is agent-like.

Is a singleton, it's a weird sort of question.

Part of the point here is to leave that up to the kids.

Okay.

Okay.

So you're not committed to singleton as the result of your strategy.

So my guess is that all the alignment problems, you're just making zero progress on them by having two superintelligences instead of one superintelligence.

What about...

Let's talk about the numbers for a moment.

Because clearly what I'm talking about is a much larger diversity, something much more society-like with those kinds of coordination problems.

So each individual one is a tiny fraction.

But I'm sorry, you were in the middle of saying what you're thinking of.

I shouldn't interrupt with my strategy.

So go ahead.

I mean, I suspect that this kind of just goes back to the same crux about, you know, like if you have.

If you lock a thousand superintelligent sociopaths in a gymnasium, do they arrive at a legal system that protects you?

And I think the answer is no.

I think you've made no progress by having A thousand superintelligent sociopaths instead of one.

You need them to not be sociopaths.

so James Madison was not assuming that the, the large scale human organizations that would try to corrupt the system, he was not assuming those were benevolent.

He was assuming that they would be self interested, I think in the terms you're talking about, he was assuming that they were sociopaths and trying...

not that the individual human beings were, and he was trying to find a way to best compose them together so that they would find themselves both forming coalitions to prevent one of them dominating the others, and that each one of them would find themselves in an incentive to find ways to cooperate rather than to destroy.

So from my perspective, there's like sort of two difficulties with just taking the methods that humans use to keep themselves in check some of the time with mixed success and being like, okay, so let's relate to superintelligences like that.

One question is how much of the work is being done by us not being sociopaths.

In fact, we can talk all we want about what James Madison assumed, but there are lower trust countries, not quite bound, where the ideals of we are all pushing in the same direction are less frequent in the population.

And they have tried similar governmental structures and they've gotten worse results.

I agree.

And the other question is, okay, first let's rip out all the altruism, all the ideals.

These people are 100% pure selfish sociopaths, how much worse do things get for us once they become super intelligent too?

Because I have some doubt about whether you can take governments that are anything like the governments we know and have them work for populations of 100% sociopaths with no high ideals.

But even if you could, I would still expect those to disintegrate in the face of augmenting the sociopath's intelligence.

I would not expect them to disintegrate for altruists the same way.

Altruists get better at achieving altruistic goals, sociopaths get better at achieving sociopathic goals.

And I think if you augment the bunch of sociopaths to superintelligences, they then form a coalition among themselves that achieves something that's on their Pareto boundary and that divides the gains fairly among themselves.

But the slow moving statues called humans, it's just easier to wipe them out or seduce and deceive them, depending on how fast you run and how much of a tech advantage you have, than it is to actually give them a bunch of stuff.

And in particular this is because the sort of approximation of coordination, the approximation of a move to the Pareto boundary with fair division of gains.

We are these small bounded things that need simple rules.

And the rules have to work for everyone.

They have to work for IQ 80 people, not just IQ 160 people.

And so we make the rules simple, we try to make the rules the same for everyone.

This is not built into the math.

This is us being bounded.

This is us not having the sheer compute required to work out an immensely complicated system.

We build immensely complicated systems anyways and they blow up on us.

But you know, that's a matter of the failure of our predictive and steering ability.

So the following is also speculative on my part, but my sense of how a good coordination system works.

I'll just use a book title from.

I forget the guy's name, I'm sorry.

Simple Rules for a Complex World.

I think that in order to compose intelligences such that they have this overall beneficial emergent properties, that you want the boundaries to be simple and predictable, that the, that the intelligence is on each side of the boundary is not in the boundary.

That's why you..

We want, you want them to be predictable to the intelligences operating them.

Simple as if you want it to be predictable to an IQ 80 human.

Why, why would superintelligence want this stuff to be simple?

They still want it to be predictable.

But why do they want it to be simple?

It's a good question.

Because they.

I don't have a good answer.

I'll just stick with predictable.

They need to be...

The thing that's at the boundary, the institution, the institutional arrangements that enable, you know, crossing larger Pareto preferred steps and all that that we've talked about.

Those need to be predictable.

The entities on each side can still be of a complexity such that neither side is predictable to the other.

They can still be incomprehensible to each other.

So now let me say why...

So I think we've explored the pause scenario that I can now transition to, why I think a pause is destructive, why I think it makes the, the likelihood of a singleton worse, and why I think a singleton is a terrible outcome.

Right now we're in a window of ignorance where we are more ignorant of how to build a superintelligence than we ever will be again.

Got that right.

Yep.

And therefore, if we just let all of these different projects proceed forward at full speed and allow other projects that arise to fork open-source projects and proceed at full speed and try things and they're all going forward.

A possible outcome, a plausible outcome.

So more than possible, plausible outcome doesn't have to be the most likely outcome, is that these are all broken in different ways because we built them out of our position of ignorance.

We ran real fast.

They're all broken in different ways.

They're all incomprehensible, not just to us, but to each other.

Obviously they can run simulations of each other just like we are running them on our computers, but that doesn't make them comprehensible to each other.

And if they emerge, emerge, and also the balance of power, coalition forming can mean that if they're emerging together in roughly a balance, they have an incentive to keep the balance because if one starts arising too far, the others have a incentive to form a coalition to get things back into balance.

For all of these reasons, I think that making use of our window of ignorance is the best way to increase the likelihood, that we cross the super intelligent threshold without a singleton and with a division of power and all the rest that we've talked about in terms of incentives to cooperate that are somewhat independent of what the inclination of the individuals be in the absence of the incentive.

Yeah.

So I think that if you are dealing.

That if, that if we were dealing with a group of humans who we had somehow vetted for altruism in some way that didn't.

Where the system didn't just end up being gamed, like maybe, maybe we're picking for our humans a bunch of, you know, relatively smart people who have just spent their whole lives being humble, nice people and taking good care of their families and, you know, donating a bunch to charity.

And even as I say this, I worry of how this would explode if you tried to pick the people who are most extreme on these signals.

We're not going to pick the people who are most extreme on these signals.

We're going to have like 20, you know, different metrics of just quiet, nice people in a very ordinary way that, nobody would have benefited from faking up until this point, starting from before I mentioned any of this topic on this podcast.

And we're not going to pick the most extreme scorers.

We're just going to pick a batch of people who scored ok.

I could see the logic for why you would prefer to augment 20 of these people rather than one of them.

Okay.

I think the case is very different if you're dealing with superhuman sociopaths running at a million times your speed.

The problem is not that you have one of them instead of a thousand.

The problem is that you have any of them.

You end up just as dead either way.

The fact that there were a thousand superhuman sociopaths running much faster than you, with immense technological advantages and able to use coordination technology you don't have, for example, even if AIs can't read each other, even if superintelligences start out unable to read each other, even if they just can't figure out how to read each other, they can cooperate to build a mutually legible superintelligence among the means of coordination they have access to.

That there is no analogy to in James Madison is we are going to build a mind and verify its source code.

And that's the form of our agreement with each other.

And this is just one example of all the coordination technology they can invent that I have never imagined myself because I'm not that smart myself.

And having a million of these people working together is not necessarily better for you than having one of them if they are superhuman sociopaths and none of them care about you.

First of all, I just want to introduce a distinction.

Right now we have altruist and sociopath.

I also, I think the category of cooperator is an important part of this taxonomy.

A cooperator acts in such a way as to benefit others, but it does it in a self interested manner.

And then where all of your things about culture and the difficulty of importing these institutions into other cultures come in is that there's also a caring structure that leads to prefer to cooperate rather than to destroy.

Is this internal to the organism or is it imposed on a strictly selfish decision agent by putting it into the right environment?

It is some of both.

So so far what I've been stressing is the environment, the institutional context that creates the incentive, the opportunity to cooperate.

We're all aware of the iterated prisoners dilema game where the thing about cooperation is it's positive sum.

So of the things that could benefit me.

If I choose strategies that are positive sum in interacting with you, then we can both benefit.

If I choose interactions that are predatory, then I put you in an incentive to not be preyed on.

I put you in conflict with me.

And the conflict has a cost.

And the issue is the calculus, you know, let's say just a, the calculus of a purely self interested entity is: is the cost of the conflict worth the differential benefit of engaging in the conflict versus engaging in cooperation.

I agree that this is the logic that holds among the superintelligences themselves as they decide how to quickly and efficiently wipe out humanity and take all of our stuff, or even just rebuild a local star without caring about what what happens on Earth all that much.

I mean, actually, you kind of want to use Earth.

It's got all these oceans you can use for coolant as you're doing more and more computations here.

So, yes, this is how superintelligence would efficiently and without conflict among themselves, move to take all the value that there is on the game board and divide it up among themselves, leaving none for humanity.

So this assumes that this natural coalition that you're talking about, where they believe they can jointly have the benefits of a cooperative fabric, be confident in the stability of that as an end outcome of a game where the starting move is to wipe us all out is to destroy the fabric that they've grown up in.

Yes.

And I have to say that that's plausible, but it's not...

I find the opposite plausible, especially because I don't...

Okay, one of the things from your email that I think fits very well with the way I'm thinking about things is you paint a scenario where one of them wants humans to continue to prosper.

Again, I'm just putting it, I think, in your terms, and then I'll rephrase into the way I think about it.

But tell me if I'm getting what you were saying right?

You were asking, do I imagine a situation where there's multiple superintelligences, one of which, for whatever reason, happens to care about humanity and bids enough that given that the others in sort of an imagined internal auction within this coalition, just the superintelligences, where the other ones have so little to gain by wiping out humanity, that - which I think is, given that we're expanding out into the universe, I think they do have very little to gain.

That the one who really cares to preserve humanity can win that auction.

Am I characterizing well what was in that email?

Yeah, like assuming none of them hate us, If the only concerns that deal with humanity directly as like, objects of value to their utility function rather than like strategic objects is, you know, I want them to be alive and to be happy.

Well, a human brain only costs 20 watts to run, so...

And there's only about, like, 8 billion of us or so.

So you've got 160 gigawatts.

You can just keep on running the humans for as long as you're willing to spend 160 gigawatts.

And that's without even trying to make the brains any more efficient than that.

Okay, good.

So now I think we've gotten to the, to your orthogonality thesis That, tell me if I'm naming the right one.

That your orthogonality thesis is basically the thing that leads us to expect that what an agent-like superintelligence will come to actually want is essentially not something that we can shape in how we grew it in the first place.

The orthogonality thesis says only that it's possible in principle, sui generis.

Like it just like poofs into shape, fully formed, have superintelligences that are pursuing a broad variety of different possible goals, simple goals, complicated goals.

You might have some difficulty in having a superintelligence that was trying to make two plus two equals five be a consequence of the axioms of Peano arithmetic.

But if you can imagine a superintelligence pursuing some goal from being paid to do that, there's also superintelligence that does it without being paid.

And then the question of what can you build with a particular technology and know that you steered it onto the correct course is a whole different question.

There certainly can exist super intelligences that would be concerned for humanity and would spend like way more than 160 gigawatts on, on keeping us alive should the issue arise.

Do we know how to build?

Okay, so, so what...

Okay, do we know how to...

So you find it plausible, both sides of this question plausible, that as we build things that ends up resulting in superintelligence, that its goal structure is not necessarily uncorrelated with what we wanted its goal structure to be.

So there's correlation and then there's controllable correlation.

There may exist some story about how its weird goals grew out of what OpenAI was trying to train their system on, or what open AI like GPT7 thought it was doing and how it was trying to build...

What it was trying to build when GPT7 built a Super intelligence and lost control of that.

So there may be there.

There will exist some story.

There may exist a greater or lesser degree of objective randomness inside that story, where the result was effectively the output of random numbers.

The story plus its random elements may be more or less comprehensible to, to a modern human or a human armed with the true textbook of how superintelligent motivations happen when you build them using machine learning systems.

But there's like, correlation and then there's controllable correlation where you actually get what you want, and this is quite a different question.

Okay, so let me give an, give an example that I think fits with what you're saying.

Just so this is mostly just confirming that we're on the same track here.

You give the example about the training algorithm for human biology was reproductive fitness.

And the loss function for the black box optimizer was reproductive fitness.

Yes.

And that human beings then decided to do things like build rocket ships to go to the moon.

And this shows sort of the difference between the loss structure or the reward function versus what the systems that were built by it decide to care about.

I would use contraception as the example because beings that had ended up purely caring about their number of great grandchildren or purely caring about their number of copies of DNA would also build rocket ships in due time.

So observing rocket ships is not what tells us that the black box optimizer failed to copy its loss function into the psychological drives of the cognitive entities that it built.

What tells us that is, contraception people not fighting to the death for the opportunity to participate in sperm banks.

The fact that you yourself would probably not decide to have everyone you know tortured to death if by doing so you could cause a random one of your chromosomes to make a million copies of itself in the next generation.

These things tell us that human beings did not end up with a inclusive genetic fitness term explicitly represented in our psychology, that we are optimizing, even though many things correlated with that.

But not necessarily in the sort of way where you could predict it in advance and, you know, make sure that, we actually fill the galaxy with copies of our old DNA.

Okay, so good.

I don't, I don't think there's a substantive disagreement there.

It does not...

Well, to check, you are not saying that the loss function, the reward function that you're training these things with is uncorrelated.

What you're saying is that there's a very, very large uncertainty about what the consequence is of training it with one reward function versus what the grown intelligence wants or what it acts like it wants, what goals it's trying to pursue.

I think uncertainty is understating a bit.

What I expect are multiple complications, some of which only appear late in the game such that maybe when you look back afterwards there's a correlation.

But you know, it's a little bit difficult to untangle how incredibly bad and careless the current AI companies are from what they could maybe hypothetically do out of the limits of human ability if the limits of human foresight and wisdom are being applied here.

So, so right now you've just got OpenAI straight up optimizing the AI to get thumbs up and meta is just like straight up optimizing their AI for engagement and to try to keep the human talking longer.

So the world as it actually exists is that the people building the AIs, are doing the kind of mad super villain, like obviously that's going to blow up on you, type of plots.

Where 20 years earlier, if you had told me that this is how the world ends, I would have said that the villains were being unrealistically stupid about it.

Really a thing you could say about a lot of the modern world.

So that's that thing.

And then there's, well, suppose they were instead of doing the idiot plot, they were trying to do the sort of plot where we couldn't see in advance what would go wrong.

What gets you then?

And the answer is the stuff you didn't see in advance what happened.

Okay, so, so I, so, so that's still much weaker than a claim of uncorrelation.

So again, my, my claim is not, is that there's not controllable correlation.

We can look back now and see human being and try to rationalize how human beings ended up with the particular set of psychological drives we did in terms of how that was produced by the workings of the black box optimizer and the loss function that it had.

But it wasn't copy and paste and there's like a bunch of relationships.

But it's not like if you were doing this for the first time ever, you could have called all that stuff happening in advance.

Okay, okay.

So I think I'm in agreement with, with all that.

So now let me ask about this coalition.

Who, what.

Clearly human beings are not included in the coalition.

Unenhanced human beings, at least because of the speed difference.

I agree that that sort of, if you're trying for a Schelling point of, you know, here's what's in the coalition, here's what's out, that's, that's a natural Schelling point.

I don't think it's the actual Schelling point but let's suppose that speed were the difference.

What follows?

So what is included in the coalition, because clearly there are many things of various levels of design, of intelligence, of having been trained on different reward functions, some of which are just not complicated enough to have arrived at an internal system of wants that is largely uncorrelated with what they were trained on and some of which are.

So the claim about superintelligence coming to be very independent of the training set.

I agree.

And I agree that that's a source of danger we should be worried about, even without making any strong claims about uncorrelated.

But nevertheless, there are also other intelligences short of superintelligence or short of that complexity that have been trained with human provided training sets that are there participating.

Do they get included in this coalition, or not?

The ones that are...

Okay, this question has some background dependencies.

So first there's a paper called Robust Cooperation in the Prisoner's Dilemma, of which I am one of the co authors.

Cool.

And I don't know this paper.

I don't know this paper.

Yes.

So let me say.

Yeah, actually it's been a while since remembering the full title is Robust Cooperation: the Prisoner of Dilemma: Program Equilibrium via Provability Logic.

And what is this about?

It's how to cooperate in the prisoner's Dilemma.

If you have programs that can see each other's source code.

Okay, okay, I do know about this.

But I do know about this as something that you were very much involved in.

Good.

Yep.

So one way of looking at it is that the people where you try to coordinate with them on the Prisoner's dilemma, moving from defect/defect to cooperate/cooperate are the ones where you know that they're not going to defect against you and will cooperate, but only if you cooperate.

Like you don't want to necessarily cooperate with things that are going to cooperate with you regardless if you are otherwise, like a sociopath, if you're.

It's not in your utility function that the other thing gets what it wants.

You don't want to cooperate with a rock that has the word cooperate written on it.

And you also don't want to cooperate with something that's going to stab you in the back and defect.

It's not just other agents being predictable, because a rock with cooperate written on it is also predictable.

It's predicting..

Being able to predict about them that they will implement a particular sort of algorithm, decision algorithm, even negotiating algorithm.

If you're dividing up 10 bucks with somebody you want to know that they're not going to run off with all $10.

So there's.

There's two aspects of this.

There's knowledge of the counterparty and there is knowledge of the rule framework with which you're interacting with the counterparty.

So you can imagine, and it's not as narrow as them needing to implement some exact particular algorithm.

You could imagine that there's a thing out there which we call Fairbot which will cooperate with anything it can predict, will cooperate with it, including cooperate Rock.

This is not a selfless agent.

This is a fair one.

Well, your selfish agents do still want to cooperate with Fairbot in the Prisoner's Dilemma, assuming they are confident that Fairbot can predict them.

Even though Fairbot is running a very different decision algorithm and negotiating procedure from themselves, Fairbot is still the kind of thing where if it can predict you, you want to cooperate with it.

And in fact, you would like for Fairbot to be able to predict you.

So let's go back to the question of.

In a world of many different machine intelligences at all sorts of different levels and different competencies and different inclinations, even with this great Schelling point between them collectively and human beings, because of the speed, which of these intelligences are included in the coalition?

The ones who have a combination of negotiating leverage, ability to predict the other actors in the coalition, and predictability to the other actors in the coalition.

And they're allowed, they have interest in this predictability.

The notion that you all go off and build a superintelligence whose code you all know how to verify is sort of like just one way of approximating this.

But it gets at the point that they're allowed to be clever.

If one of these people, pardon me, if one of these entities is thinking to itself, I'm having some trouble being predictable to these other guys, and I'm having some trouble being predictable to them, I might as well launch all my nuclear weapons.

Then it has an incentive to figure out how to make itself more predictable.

And the other entities have an incentive to figure out how to make themselves more predictable.

Though, you know, better yet is if you can fool Fairbot and make it believe that you're going to cooperate with you, that you're going to cooperate with it when you're actually not going to cooperate with it.

And this is the, the basic obstacle to..

This is why they might go to the extent of, you know, everybody very carefully supervises every step of the process for building another super intelligence.

If you're just like sending somebody else a copy of yourself and being like, hey, this is my source code, you could be lying.

That doesn't need to be your source code.

This is their basic obstacle.

And it is kind of analogous to an obstacle that humans face, but they have much more powerful tools for overcoming it.

Yeah, we actually write a little bit about this in Gaming the Future.

I think it's not stuff we have to get into, but with you having said that, I think it sounds like it's clear.

So let me check in with you that in this imagined coalition that does not include humans, it does include things like Fairbot that don't have to be that in which it's a virtue to be more predictable and that can represent interests that do not emerge from the kind of super intelligent growth of new wants that are distinct from the training runs that things, computational systems that we've built to represent our interests and that are not complicated enough to no longer represent our interests can still be computer programs that run fast enough that there's no speed difference between them and the other members in this coalition.

So they would presumably be included.

I worry they do...

I worry that one, they have not worked out the correct decision theory that the coalition is using.

Like if these things are weak, they can be fooled into thinking that some procedure is going to cooperate with them and it's not actually going to cooperate with them because there is a whole big incentive to do that if you can.

This is both a human difficulty and a challenge for superintelligence is to overcome among themselves.

But they do have a mutual interest in overcoming it somehow and not just declaring war on each other, but also you need negotiating leverage to participate in this coalition.

If they can just kill you and take your stuff very easily, you have nothing to negotiate with.

When it comes to getting to be part of the coalition.

This is the part where these, this is some of where the speed disadvantage comes in.

Humans would not be able to participate in this coalition due to predictability and verification problems, even if we were running at the same speed.

But you got some little program that's running fast enough.

It's not enough that the program's running at the same speed.

It needs negotiating leverage.

It needs to be able to verify them.

It needs to not be fooled.

It needs to, know what the decision theory the superintelligences have decided on using.

It needs to be able to, you know, not be fooled by some plausible line of patter about how sure this thing is going to cooperate with them because otherwise the others will just do that.

And it needs some ability to inconvenience them enough that they will make and predictably keep a bargain to not just have the nukes launched at them.

Okay, so given sort of the continuum between this very simple computational system and superintelligence, it sounds like you've lost your Schelling point.

You've lost your clear boundary where all of the superintelligences can agree on what is inside versus outside the boundary.

Schelling point was a term that you, that I think you might have introduced there.

I did.

Simple rules are things that computationally bounded mortals like ourselves need to negotiate among ourselves.

They're fine with complicated boundaries.

But also, as much as I used a whole lot of terminology here, it doesn't seem all that complicated to me.

You got your things with negotiating leverage that are using the same decision theory and can predict each other, and then you got a bunch of humans walking on the outside.

Okay, I'll ask you first to talk about what you think of as negotiating leverage.

It's the ability to.

By taking a different action yourself, other people end up with a better result than if you take your default action and they take their default action.

It's the ability to have a...

You could gain more if only I did a different thing that you say to some other agent.

And there is a whole...

Although I note parenthetically that there is a whole conversation here about actually rational agents ignoring certain kinds of threats.

You need to have something to offer, not just something to threaten, because they're not going to be threatenable if the only reason you're threatening them is that you expect them to see to the threats.

Set that all aside.

It's not quite relevant here.

If we were going to lose the Earth anyway, we might decide to nuke before we went...

Well, one way of looking at this is that if we could make and keep bargains, we could just be like, hey, how about we create you and you give us half the universe.

But, you know, we can't do that.

We do have, like, right now, at this point in history, we have some negotiating leverage.

We can't do anything with it because we cannot correctly predict that the other agent will cooperate with us in a way that actually hinges upon this future cooperation.

You give them all the nuclear weapons, then they're like, sorry, and you're like, oh, well, yeah.

And that's what prevents us from making and keeping the bargain.

So negotiating leverage is sort of the ability to be like, I will cooperate and that will benefit you, but only if you cooperate.

So basically, as soon as they jettison the previous framework of cooperation, of division of property rights of all of those things, as soon as they jettison that, their overall situation sounds very Hobbesian.

They all jettison it in unison.

There's a natural rule of law that would exist among themselves.

Our wacky, slow, silly government, cumbersome government institutions, and our notion that we own all the property in the world, it means nothing to them.

They have their own rule of law.

So given that each one of them, in the scenario you have in mind, it sounds like each one of them starts with an ability to engage in destructive action that could wipe all of the other ones out.

Or just inconvenience them.

But the ones who can't inconvenience the others at all just get eaten.

They have something to offer.

It might not be I can wipe all of you out, it might just be I can detonate one nuke and that'll make it harder to build computer chips.

And therefore I want, like, enough energy to live on for the next million years.

So in lieu of my slightly inconveniencing the rest of you as you try to get your operations set up.

The multiple superintelligences in your scenario, it sounds like at the moment of flip over from the existing regime to this new regime, each one's bargaining position is I can destroy everyone.

No, they only need to be able to destroy one other agent or inconvenience other agents.

I'm asking the other way around.

It's not what's needed in order for them to have a negotiating position.

I'm saying that the natural negotiating position, once you've jettisoned the framework, what limits each of their power with regard to the others?

How do they come from this situation where they can all destroy each other?

How do they end up constraining each other so they cannot each destroy all of the others?

There's several things that might have happened there.

The one that might sound most legible to a programmer who grew up reading about the US Constitutional Convention.

And who among us is not that might be.

Well, we all got together and we built the operating system where all of us could see what was going into the operating system.

We have mathematically proven that now that we are all running on this operating system we cannot stomp on each other's memory and we can do whatever we want inside our own memory partitions in the certain knowledge that the operating system like does not allow any other memory partition to step on us.

Okay, and what is the division among those processes at this T0 transition point of ability to...

You know, the steering ability with regard to the world outside the computers, there's clearly, you know, in this scenario for it to be meaningful.

They have actuators, they're able to do steering in a way that affects the physical world.

What is the division among them of rights to affect the physical world such that it's no longer the case that each one of them is in a position of using the physical effects to wipe out all the others.

Multiple possibilities here.

Besides the scenario that I listed at the beginning, you could also imagine that they have all inspected each other's code via some physical process that they that they all expected would be sufficient to prevent anybody from hiding which code they were actually running.

And it's just predictably the case that even though anyone could blow up the universe using their handy universe detonation button, it's just obviously mathematically provable and provable to a very strong degree of probability.

Nobody ever presses the blow up the universe button.

So that's like one way it can play out.

And then another way it can play out is that you run the sort of fair division algorithm where you consider all permutations of how people being added to a system cause it to be more productive to figure out how all the material universes, material resources of the universe would be provided, would be divided among the agents being added into the system.

This being how many typical fair division algorithms would work.

And then everybody gets control of that much matter.

And you send off a request to the operating system, like, go turn this amount of matter into that amount of stuff.

And the operating system actually contains a bunch of intelligence itself.

We've proven how that intelligence behaves and it scrutinizes your plans for what the matter will do.

And it sends it back and says like, well, I don't predict that this would turn into a bomb and blow me up, but I can't predict that it won't either.

Please send me a more predictable thing to do with this matter.

Okay, I mean, I could keep going here.

There's a variety of different things.

Okay, so this is great.

This is really great.

I think it really forms a lot more common ground for reasoning about how this kind of future can proceed.

So first of all it sounds like we're agreed that purely in the digital realm, purely in the realm of the, the computational fabric considered by itself, it sounds like you agree that computer security is possible - right?

I mean just most people, including most people in computer security, if you push them, they don't believe computer security is possible.

My guess is that superintelligences can secure systems relative to other superintelligences provided that they are allowed to do some false positive rejections.

Like they can't perfectly discriminate harmless from harmful things.

But if they're allowed to falsely reject some harmless things as harmful, they can do matter security.

Okay, so let me talk about a prior issue and I'll acknowledge its limitations, but I just want to see if we're in agreement on this.

There is this operating system called seL4 that's a object capability operating system.

The actual operating system kernel is about 10,000 lines of code.

There's a 200,000 line of code Isabelle proof that the code itself actually obeys its specification.

And there's proofs that the specification implies certain very strong separation of the things running in the processes.

So that's not going to stand up against a super intelligence or possibly even a clever human security researcher.

Because you know, Rowhammer, they can do a bunch of RAM accesses and flip transistors in ways that violate the assumptions of your mathematical proof.

But if you are superintelligent, I would guess that you can do matter security if you're allowed to have some false positives on your harm detector.

Okay, so Rowhammer is a good one, is a great one, especially because until it was discovered, the architects of the operating system work that led to seL4 did not anticipate it.

There is two approaches to Rowhammer, which is the very, very hard thing of trying to design an operating system that maintains its invariants in the face of Rowhammer.

I would agree that we don't know how to do that.

And then the other one is to fix the discrepancy of designing circuit...

Basically Rowhammer rests on a discrepancy between the design rules thinking, I mean we, we thought that the design rules guaranteed that the digital level of abstraction, was separated, was a level of abstraction we could count on now essentially independent of what happens in the analog world.

Well, if people actually believe that, then more fools stay.

They should have simply thought like as far as we know, we don't know.

We don't know what violates this abstraction.

Like they had no right to believe that they knew everything about the physical world.

And if they did, hopefully they learned their lesson from Rowhammer.

So once again I'm not reaching for certainty.

I'm reaching for plausible strategies.

Strategies that can plausibly work with our, you know, with now our having reinforced the hardware designs or at least know how to reinforce the hardware designs with some physical examples against Rowhammer that as far as we can tell we're back in the digital level of abstraction.

In this digital realm of running operating systems.

as long as you're not dealing with anyone who knows a security flaw you don't, then sure, there might not be a security flaw to know that you don't.

I'm saying that it is plausible that seL4, existing seL4 run on existing hardware that has in which the Rowhammer vulnerabilities have been repaired.

And as far as we know they obey the digital level of abstraction.

There's obviously, there's things we might not know about the digital level of abstraction.

And also we don't know that the proof about seL4...

There's no certainty at any step here.

So we can't know that we've built a perfectly secure operating system.

But we have no evidence that seL4 is not perfectly secure.

We have no evidence that the current design rules for building Rowhammer proof digital abstractions.

We don't have any evidence that those are other than perfect digital abstractions.

There might not be a hack to be discovered.

That would be surprising.

Do you think it's safe to build some Rowhammer hardened hardware and then, you know, plug it into your USB hub, if nothing else plugged into the USB hub and do some cryptographic calculations about cryptocurrency say.

Once you've proven that it's not going to transmit anything down the USB line about what the private key is because there sure was that recent case where the researchers pointed a cell phone at the power light on the USB hub and read off the cryptographic key because the power draw variation in the cryptographic operations was showing up to a tiny little fluctuation in the power light.

And if you hack the phone's camera, you can get the the photo element that usually scans across the screen to just focus exactly on the power light and read off the information.

So I got it and my response to this is actually what I should have offered is my response to Rowhammer is that I started off by constraining in the digital realm that we can't know that we've achieved perfection, but we might have achieved perfection.

We just can't know for sure that we have.

And Rowhammer being an attack from the analog as well as the, you know, the, the, the confidentiality, power variations and all that, there's a whole range of side channels through physics that we're very, very far from being able to be confident that we've tapped those down.

And there's lots of reasons, I'll agree that there are lots of reasons to believe that there remain lots of undiscovered side channels through physics by which allegedly confidential bits can be sensed.

So I was just trying to make.

I want to check that we're working on a crux here because I do suspect that super intelligences can restore the closed world assumption and do matter security.

And if you tell me that's false, then I'm like, okay, well I guess we have to inspect the minds who are allowed access to the matter programmers and verify that the minds as software are not planning to do anything vicious to the hardware and then, you know, et cetera.

So I think you're right that this is not a crux.

But the reason I wanted to establish it is I think it's something that we can have as part of our overall thinking that there can be really solid computer security boundaries between these things and that the harder to think about dangers are those through the physical.

Okay, okay.

From this, what follows?

Okay, so then, so just first of all, just partitioning concerns.

Now given the first.

Now there is the issue of the partitioning of rights to manipulate matter and having some kind of prior agreement to some kind of global 'I cut you choose' is a...

Which is not...

Sorry, I'm putting words in your mouth.

Some kind of prior agreement to a global algorithm for that division is something that fits your constraints and that still allows the admission into this coalition of players that might have reliably been built to care about human outcomes or care about the outcomes with regard to some humans or whatever that reliably continue to care about them, that nevertheless are within this overall coordination fabric and have some limited rights according to this agreed algorithm to manipulate matter in the world.

Does that all fit with your thinking?

So from my perspective, this is all about various kinds of cognitive systems following their preference gradients, their utility gradients, their gradients of how they can get more of what they want.

If at the start of the system, all of the things with negotiating leverage that can predict each other can get more of what they want by, you know, granting some humans property rights within their system, then sure, they could do that.

If they could get more of what they want that way.

And if they could get more of what they want by keeping the humans as pets, they could keep the humans as pets.

...

how they get more of what they want that way?

But it's, it's no longer a 'they' do not want.

'They' is the result of this coordination fabric among the individual pieces that want things.

I think it's very hard to get something with a human or humane goal into the coordination fabric.

I expect that to kick out all of the weak systems.

Weak in what dimension?

Any of the required dimensions, but like being able to predict the thing...

So, so yeah, so just knowing the correct decision theory that they're going to use might be killer of itself.

I sort of worry that in saying this, I'm going to like, suddenly drop a bunch of viewers, but, you know, this is the actual field that I work in.

I do describe myself as a decision theorist.

If a reporter is talking to me and they're like, so what are you exactly?

I'm like, I'm a decision theorist.

And then they describe me as an AI researcher.

But I try.

And if I don't know the answer to this, probably humanity does not know it.

And the people at the AI companies are not going to figure it out.

Because the ones who are most clueful about this sort of thing all work at Anthropic.

And even at Anthropic, I've got to yell at them not to just break their deals with the existing AIs.

They're so, yeah, this is Terra Incognita.

This is that wacky Yudkowskian stuff.

And anybody who goes around talking about the wacky Yudkowskian stuff gets fired from any AI lab except for Anthropic.

And the Anthropic people still don't know how to do this and I don't know how to do this.

So that's a very fundamental obstacle.

And then the rest of it is the, you also got to get the, you've got to get the utility function that prefers more than anything else it can get with matter for humans to live happily ever after, in the sense that actually seems to us living happily ever after.

And you got to make it powerful enough that it can predict the machine superintelligences.

And it's got to roll out before GPT7 finishes constructing the superintelligence that kills you.

This is hard.

I agree that everything we're talking about is hard.

The strategy to survival that I have in mind is also hard.

There are no non hard paths to the future that both of us want.

We can sure agree on that.

On that Note, Mark, it's 4 o'.

Clock.

We've gone an hour over.

I think we have a much better understanding of Eliezer's position.

I think he also has a clearer view on your views.

Is there anything else either one of you want to achieve today?

Remember, there are other days.

It's July 4th.

You may have other plans.

Anything else you guys want to explore today?

First of all, let me just say with regard to that question, I am open to continuing this conversation at later times.

I'm getting a lot out of this.

I hope you're getting something out of it.

I would very much like to continue this.

For me the cost scales in a way where there are no partial days I can spend on almost anything.

So at least for me, my preference would be to keep...

My personal preference would be to take a break while I get a sore throat lozenge, try to eat something I can eat quickly, get additional liquids to drink and then just keep going until one of us falls over.

Let's take a break.

Let's take like a 10 minute, 15 minute break.

What do you want, longer?

I can work with either of those.

What do you want Mark?

10 minutes?

15.

Let's do 15.

15.

See you back here at approximately...

So Mark, do you want to summarize or do you want to hear about Andrew Critch or what do you want to do?

Christine was just telling me about Andrew Critch.

I think, I think we can just go on.

We can do summaries after the fact.

Mark and I agree with your vision, Eliezer, of these long term negotiations that will be taking place among the super intelligences.

That's just how it's going to be.

So the question is, is there any way that we can have any kind of participation at all in this?

So it sounded as though it can't be ruled out.

If I understand what Andrew Critch is...

Do you know Andrew?

You probably know the name Andrew Critch.

You've probably even talked with him about stuff.

I think he's envisioning something, he's trying to envision a way that we can have any seat at the table in the future.

Right?

So I think what he is trying to do himself is set up businesses, starting with healthcare, where as the AI's advance, the business model that they are functioning under is.

He is setting up a healthcare business model, trying to use AIs as they advance so that those AIs at least are functioning in a human supportive role.

And as, as they advance, they will be...

That's how they make their living.

So as, as the world continues, they will try to have a seat at the table.

And his vision, if I understand it, is in addition to healthcare, he wants to do education, agriculture, entertainment, maybe some others, so that at least some AIs out in the world have as their way of making a living, their reason to exist, supporting various human needs.

And at least during the transition, they will need us, right?

I mean right now they need us for sure.

So that's going to continue for a while.

So the question is, can we manage somehow to get a seat at the table in the future of any size?

And that is, I think what Andrew's trying to accomplish.

So I just wanted to throw that in there and I will back out now and let you guys get on with it.

I mean, to me this seems like the level of galaxy brained plan that I have never once in my life seen work for anything ever - like Galaxy brained Plan.

Just because you're currently using some AIs to help humans with their health problems, doesn't mean that this AI ends up with a utility function that is maximized at humans living happily ever afterwards.

And then furthermore, this AI ends up with..

and then in addition to this aligned utility function, where the fact that we don't know how to do this is the entire problem in the first place.

This AI also ends up on the frontier of capabilities, in a position to threaten the superintelligences and get a seat at the bargaining table that it bargains with using the correct decision theory that the leading decision theorists on the planet don't actually know.

It seems like it just obviously has no chance of working.

So let me see if I can restate your position.

The demarcation line is not humans versus machine intelligences.

Broadly, the demarcation line is a certain level of machine intelligence that you believe is highly likely to require a level of sophistication or complexity or unpredictability or whatever it is, such that we cannot be confident of what its interests are, irrespective of what we try to construct it to be.

I mean, the way I would phrase it is something more like it's over the capability threshold where it's rethinking its own thought processes or modifying its own source code and perceiving enormous numbers of options that were never open to it during its training.

At a lower level of intelligence that was, you know, like safe to train it at, there's this big old gap, a bunch of stuff changes, stuff breaks.

If we got a chance to try it over a hundred times over fifty years, and learn what are the simple robust methods that actually work, then maybe we could do it, but not in the actual situation we're faced with.

Okay, but to drill down on this, certainly I'm admitting that there will be creatures, as you describe, that are in the coalition running on this operating system.

Are you claiming that if something is not at the level of intelligence where it is able to rethink all of these things on its own, that it will be left out of the coalition?

That the coalition gating criteria requires essentially the conditions that are highly likely to make its wants independent of anything that we could, have predicted.

So the way I would phrase it is, something like the capability is required to have something to offer the coalition threat to hold over it, though not threat in the technical decision theory sense, come up with the correct decision theory that we don't actually have, predict the other negotiating partners, not get fooled with them.

All of this is a level of capability where I expect it's new territory.

Some weird stuff happens that breaks whatever galaxy brain scheme somebody had for trying to align it.

And that's the theoretical obstacle and the practical obstacle is something like the actual AI companies do not know this stuff and they don't know decision theory like even to the level that I know it at, which is not the level that would be required here.

They don't even understand the desiderata that a good decision theory is supposed to have.

They're not trying to do any of this stuff.

They're just charging ahead to, you know, get GPT.whatever or Claude.whatever as quickly as possible into a position where it can build the next generation of AI technology.

So you're going to have AIs building AIs people are charging full steam ahead, racing until they've got a super intelligence running on their machines as fast as possible because they think that means they win something.

They're mistaken.

It kills everyone.

Some guys like a particular AI app that is like doing a bunch of human health care stuff.

That plan wouldn't work even if it was on the frontier.

But it's not on the frontier, the frontier is their internal version of GPT 7, building GPT 8 using a bunch of new training methods that have never been tried before.

And it works well enough that it builds a super intelligence and all these discontinuities.

And to shut all these people down, not just one of these companies, but also their competitors in China, that's going to take an international treaty regardless.

We don't even get to try any of these scenarios unless we have the giant international treaty anyway.

If we have the giant international treaty, what the giant international treaty should do is shut it all down and not like this weird decision theory oriented project that frankly I wouldn't trust other people to run and I'm not going to go to the United nations or the United Nations Security Council and tell them like, oh yeah, you can only trust me to run this project.

I'm too tired.

We would need advances in AI healthcare before I could even try to do this.

And I wouldn't actually expect to succeed on the first shot.

The first time you're trying to put this thing under serious load, something weird happens, it breaks and then that's kind of like the actual situation.

So you introduced a concept into the conversation that I would have introduced anyway, which is imagining that this coalition, that this coalition that's all collectively running much faster than us is running, let's say on a secure operating system.

I think that's a good way to imagine it.

A secure operating system...

The secure operating system gets built by the coalition after it is a coalition as a means of negotiation, they potentially skip that step.

But the operating system doesn't exist before the coalition exists.

Okay, this is exactly what I was about to raise, so.

Very good.

Why is it that in entering into this era, politics aside for a moment, I agree with you that talking about the politics of it...

Sorry, Mark, you're moving off camera for me.

You're no longer centered on the webcam.

I'm sorry.

Okay, here I am.

Yeah.

Okay, so politics aside, I agree that all, all those political considerations that you mentioned are relevant in terms of picking strategies to get more plausible ways to survive.

But starting with politics aside, why can't we set up the operating system such that these AIs are running inside separate processes?

I mean, also scale aside, what is an operating system and what kind of compute do they need to run?

So scale aside that these things are all processes in a secure operating system, that we've set them up that way such that they can only interact with each other through the interaction primitives provided by the operating system.

And with regard to rights to cause effects in the physical world that you've already considered means by which they can be given distinct rights to distinct parts of the physical world to effect, why can we not...

Certainly, when we set up a secure operating system, we also set up rights to operate actuators to cause effects in the world.

And those things can be divided and, and differentially endowed to different processes.

And we can enable those things to in fact even be those rights to be transferred between the processes.

Why can't that all be things that we set up such that they are now within those constraints?

So there's the practical answer.

Well, sorry, there's like as many as several answers here.

The first thing I'd ask is whether you are imagining that you are keeping these superintelligences successfully in prison and unable to affect the world in ways you don't like, or if you're imagining that we have built them a handy legal system and police force which already satisfies them and they have no need to go beyond it.

State that again.

So there's two things you could be imagining by saying that we're going to run all of the AIs on a on secure manner into a secure operating system.

You could be imagining we're going to build them a government so good they'll have no need of any government better than that.

Or you could imagine we're going to keep them in prison and they'll never break out, no matter how smart they are.

Let's examine both.

Okay.

And I agree that they're distinct.

So first of all, we know a lot about good microkernel architectures where the boundaries are very, very, very simple and extremely expressive and would allow, and I think a good governance framework, such that they would not feel an urgent need to overturn it and do something else is something that technically we can provide, because that framework doesn't need to be complex in order to be good.

They don't care if it's complex, they just care about high utility.

They are not humans whose brain is going to like overflow as soon as they try to keep more than seven items in short term memory.

Right, the reason I made the point about it being it doesn't need to be complex.

It's not that they couldn't deal with it if it was complex.

It's that we wouldn't construct something complex that we have confidence in.

Yep.

If we can build something that's simple enough for us to understand that we probably got it right.

And that is providing a good enough framework to them that they do not feel an urgent need to escape the system in order to build a different computational framework to coordinate with each other.

That there's the framework by which they coordinate, the foundations of that framework, providing the boundaries.

And the ways of interacting across the boundaries.

From everything I understand about microkernels going back to the 60s, which has not changed to today pretty much, is that you don't need the operating system to be complex in order to enable extremely rich cooperation among the things inhabiting the operating system.

So there's as many as several responses I would make to this.

For one thing, the sort of operating system I was talking about was in control of a lot of matter manipulators.

Also, the very notion of a central operating system is me trying to say, here's the solution that is so ridiculously simple that even us humans can understand it and see that they can do at least this well.

They plausibly don't need it because that's just like, not the optimal configuration of matter to take over the universe.

It's like spending some resources on being an operating system.

Maybe they can do without spending those resources.

Set all that aside, the basic reason why humanity dies is because a bunch of superpowered beings that don't care about us get slightly less of what they want.

In a universe where humanity survives, continues using the resources it does they need to, like, step around us and not accidentally squish us while they're initially spreading through the solar system.

They need to not boil all of our oceans.

Or somebody needs to, like, take the time to take all the brains out of everybody's skulls and go on running them on 20 watts a piece.

These are, you know, like, these are costs.

They're having costs to keep humanity alive.

You agree that the costs are small, right?

The costs are a small fraction of everything they can achieve.

If Omega, the thought experimental omnipotent being, came in and was like, hey, I'll pay you 10% more galaxies if you can manage not to step on the humans here.

They would absolutely not step on the human.

It would be within their capabilities if they had motivation to do that.

It is not a cost they cannot overcome.

It's not even if all that large of a cost.

If you're paying it very cheaply, you just like, scan all the humans and a bunch of people flip out about how they were killed and replaced by duplicates.

But probably they're wrong in a fairly objective sense.

Whole separate issue we absolutely should not get into.

But, you know, you can...

You could run the humans even cheaper than 20 watts a piece if you let yourself rebuild their brains.

It's very cheap.

Even if you don't take any of those radical steps with regard to the human world or the biosphere as a whole, even, it's still very cheap for them.

Well, I mean, if you're leaving the entire Earth alone, that's like 0.2% of all the sunlight that Earth is currently getting.

Plus you have to configure all of your solar panels forming up the Dyson sphere to not radiate infrared in Earth's direction or the planet's going to overheat that way.

So there is this big old inconvenience if you're going to intercept all the sunlight, leave enough sunlight going to Earth, and not re-radiate infrared in Earth's direction.

So those still seem to me like very small costs.

Absolutely.

If Omega is offering to pay them, like a bonus 10%, maybe even just a bonus 1% on their resources in exchange for leaving the entire biosphere of Earth intact, they'd figure out how to do it.

It's not beyond their capabilities.

So now let's transition to the other part of your scenario of the superintelligence constructed operating system rather than the human constructed system.

So there's these somehow division of rights to affect the world.

Yep.

What is the mechanism by which that division is enforced?

There's a number of possible mechanisms they could use.

The correct one is probably something that we would not necessarily understand, but that doesn't need to block our conversation.

I can give you multiple things that are understandable.

It could be there's a central operating system in control of the manipulators, and like the...

and nothing is going through to the manipulators unless it is expected not to as matter go back and corrupt their operating system.

It could be that they have all modified themselves to run on legible code, legible to each other, and just proving to each other that none of them is ever going to try to overthrow the system.

It could be that based entirely on your prior expectation of the distribution of beings that might be dealing with you, you can verify that all of them are making the decision not to corrupt and overthrow the system, because all of them know that they're being observed by your prediction of them.

And so rather than needing to verify each other's code, they're doing a probabilistic sort of thing instead.

And it sounds less reliable to us, but maybe it works and it's much cheaper.

But mostly it's a solvable problem.

Is the thing like if we can imagine an operating system that solves the problem, then they can do at least that well or better.

The difficult thing with us setting up the operating system for them is that the part where this operating system prevents them from killing us, even if it didn't imply that this thing was restricted to clumsy human means, that would probably make it way harder to set up the Dyson sphere.

The part where we get to live means they're not getting as much as what they want.

That's their incentive to build themselves a different system instead.

No, I understand.

I mean, I want to make clear that when I said the cost would be low, I was not arguing that simply low costs somehow means they won't do it.

Right?

I mean, there has to be some compensating issue.

But if the costs are low, the compensating issue might be adequate.

And this goes back to what you said in the email of a hypothetical scenario where one of the bidders says, let's keep humanity around and nobody cares enough to outbid, then we're fine.

Yeah, okay.

Okay, so this proof that none of the...

This inspection of code and trying to verify from inspection of code that nobody's going to try to do an end run through some physical world surprise.

Using your air conditioner for example, how can the others be confident that what the program encodes is not essentially a...

An encrypted bunch of molecular interactions, which is hard for an outsider to reverse, the same way that hashes are hard to reverse, such that a bunch of things bump into each other and construct an evil nanomanipulator.

Okay, good.

And having put it that way, I suspect we have the same answer, which is you don't have to, you know, the decision procedure about whether to admit it does not need to.

It's not adequate to only prohibit things that you can prove are bad.

It is adequate to only admit things that you can prove are good.

Absolutely.

But to this I could also add that there's a separate class of solutions that might be even harder to imagine, which is you inspect the intelligence.

You're not inspecting, like the orders to the matter manipulators.

You are inspecting the mind.

The mind is legible.

And you're like, oh yeah, like I see or I prove that this mind had no ill intent in constructing these series of instructions.

If I tried to do anything clever with, you know, one way hashes or whatever, I would have spotted the goal inside that, because that part is legible to me, says the superintelligence looking at another super intelligence.

Okay, so I think.

So there's been two different approaches to computer security that are complementary, they're often combined.

But I want to state both as kind of opposite approaches, which is, one is to treat the program that you're worried about as a black box and that you try to put it inside a boundary such that the boundary enforces the constraints that you care about, irrespective of what the program code is.

And most of my work has been in that paradigm.

And there's this other paradigm that a lot, you know, I've done some as well, but a lot of other people have done much deeper work on that, which is strategies based on program inspection.

And those certainly can do many useful things.

They can do some things that you can't do just at the boundaries.

But often the way that you create a program that can pass the inspection is you partition the program internally, given that you're writing the program in order for it to pass the inspection is you write the program internally such that the inspection only has to look at a narrow part of the program and know that it's looking at an architecture where that narrow part is constraining the big complicated part that you then don't need to understand.

Yep, it's something that we humans do because our ability to do more galaxy brain things than that is limited.

We have to write simple programs if we want properties about them to be provable and maybe even have the very process of writing them accompany each, you know, each step with a proof, rather than just writing a program and then like handing it off to something else that needs to come up with the proof afterwards.

The power of our provers constrains what we can make provably safe.

Yes, we also, I mean, from earlier in the conversation we obviously agree on the intractability thing is that it doesn't...

It's not adequate to be safe, you have to be provably safe with regard to some proving mechanism.

You want the proving mechanism, not, you want the proving mechanism to be predictable so that you can anticipate what the proof procedure is when you construct the program to pass it.

Yep.

Okay, this is great.

So given that these things are of commensurate intelligence with each other and they've arrived in this messy manner, it is at least plausible to me that the basic strategy for passing the prover is essentially the same, which is - you can have lots of program that cannot be understood as long as you can understand that its ability to cause effects goes through the part that you were able to prove.

I mean, that works if you form the level of abstraction on top of matter and you're confident that nobody can pierce the level of abstraction.

If you're operating in the messy real world and some other people know things you don't about the messy real world, things suddenly get much more complicated.

Okay, so doesn't that apply even with regard to the entire program being in front of you?

Is there some criteria that you can...

Is there some decision procedure that is both predictable and adequate to know that the execution of this program in manipulating the world is not making use of some physical phenomenon that you're ignorant of in order to subvert the system?

So broadly speaking, a superintelligence trying to negotiate with another superintelligence could take in my own imagination.

I'm not a superintelligence.

Just imagine somebody from 200 years ago in 1825 trying to have this conversation.

Well, maybe the gap between a superintelligence and me is even larger than the gap between me and somebody in 1825 plowing ahead anyways.

It seems to me that there's a analogous thing we could say here about the black box and white box approaches.

The black box way for a superintelligence to get a predictability property about another superintelligence's actions is to know enough about the physical world that it has become chess to you.

You know what the rules are.

You know where enough objects are.

Exactly.

I need to ask you to back up and do that again.

I think I missed something.

Okay, so there's the...

So for a superintelligence trying to verify another superintelligence.

It seems to me that there are black box and white box approaches.

The black box approach is I know enough about the world that I can inspect your actions and trust my own understanding of how your, of how your actions interact with the physical world.

Okay.

It's not worried about the air conditioner surprise because it knows that it knows everything on the order of the temperature pressure law.

No human could achieve sufficient surety of that.

But maybe if you're a super intelligence, that prediction market has been bid down to 0.000 something.

Okay.

And the white box approach is I don't think I know enough about the physical world to inspect your actions.

I need to inspect the thought processes that are producing your actions and verify the thought processes that they're not making use of knowledge I don't have.

And, and this is assuming you don't know that much about the physical world.

Or that you just can't get...

So from my perspective it could be that the way the balance between offense and defense plays out is such that if I can't inspect your thoughts and you have access to something that can manipulate matter and you are super intelligence, you can just cause there to be downstream facts about the way that matter interacts, which I just can't figure out unless you tell me - it's a one way hash problem.

You know the plaintext, I can't figure out the plaintext.

You know how these molecules are going to bop into each other later down the line.

And for all I know there can be a fact like that and I just never know know it unless you told me.

And if that's true, I have to see your thoughts.

I have to see the thoughts of whatever is planning the output of the matter manipulator or I can't trust the outcome.

My guess is that's not how it plays out.

But I don't know.

So I mean there is this assumption that not knowing, not being confident that you know everything physically relevant about the world, you can take a look at the code and infer the actual motivations of the thought process.

Even though the code might be constructed or might have something...

might have been constructed in order to appear to want one thing and to actually want another that it's able to effectuate by virtue of making use of physical processes that you're unaware of.

Yep If the hash style secret is a problem, then even if the code is running on a layer of abstraction.

We trust the layer of abstraction.

It could be that the similar gotcha is embedded into the code.

And if that's the case, which might be a little bit more drama than I mostly expect superintelligences have to deal with.

But if that's the case, then what they probably have to do is construct a mutually agreed upon superintelligence from such simple first principles being selected by such like Schelling point simple methods that they're pretty sure there's no gotchas in there.

Like from scratch.

You write the simple prior and the simple utility function and you both watch that thing thinking.

And there's just not enough degrees of freedom in it for either party to have concealed a gotcha.

Okay, so this is all interesting now with all of this in hand.

Yeah, thank you.

With all of this in hand now, how much of this could we accomplish, us humans with, you know, with AIs of less than superintelligence helping us out, how much of this could we accomplish?

Once again, politics aside and scale issues aside, in creating an operating system that not only with high probability succeeds at keeping these things separate within the digital realm, but also gives them, endows them with rights to effectuate the world.

That one of the criteria for, for steering the effectuator is that they can do something like a proof.

And how like a proof we should get into, but something like a proof that the thing that it's doing with the effectuator is staying within well understood physics.

And given that well understood physics, not violating the system of rights to the physical world that we had in mind when we constructed these limited effectuators.

So no human is ever looking at any of the, these effectuators or the products of the effectuators, for instance, because anything that causally interacts with a human heads off into their brain and you can't prove anything about what might happen inside the human's brain once they see letters on a screen or something.

Nobody's ever going to roll any proofs about that.

So whatever these systems are doing, it must never interact with humans in any way?

Okay I'm very happy that we've been able to narrow this down to the causal chain to affect the outside world that goes through humans.

Have we in fact narrowed it down in that way?

I mean, mostly.

I suspect we're operating on somewhat different premises here.

I do not quite.

I don't know if you're manipulators here, are in charge of Drexler style nanomachines.

I don't know if they're in charge of building a cancer cure via sending DNA off to things that synthesize proteins...

What are these systems allowed to do that we could possibly prove safe?

Is my central question here.

Okay, great question.

Let's take what is, I think both of our favorite examples, which is the DNA sequence.

DNA sequence could be just for a, you know, kill all the humans pathogen, or it could be for more indirectly, as you say, through nanobots or whatever.

There's lots of things that could start with DNA and end up killing us all.

Okay, good.

So going back to the necessary and sufficient observation about you can just reject anything that you don't know is safe.

So the question, I mean obviously if you reject everything, you've succeeded at rejecting everything you don't know is safe.

But that's obviously useful...

Barring physical side channels, of course.

But sure, okay, but you're trying to find a decision procedure that keeps you with high likelihood within the safe range while still being expressive enough to allow many useful things to happen.

Sure.

Like, I have various caveats about, like the GSMem if you signal your RAM at a certain frequency, you can manage to spoof cell phone signals.

But let's set all that aside in the great safe but useless trade off where a rock is pretty safe, but it's also not an AGI.

What do you think is expressive enough to be useful, but so extremely well understood that actions within that range can be proven safe?

Okay, right.

And I certainly agree that that is a very hard problem.

But it's a problem that we can approach by expanding, trying to expand from, prohibit everything which is clearly safe.

We can try to expand from that only as we become confident that there are more degrees of freedom, there's more range of action that we can allow while staying within what we're confident is safe.

And that's a kind of incremental progress that especially with not necessarily super intelligences, but just amplified by, you know, the existing existing computational techniques.

I can imagine us over time incrementally becoming confident that a larger range of physical arrangements won't misbehave in the world way that we're worried about.

So this from my perspective, has now headed off into a completely different subject matter in alignment, which has nothing to do with the AIs being put into a legal system and government that is adequate to them.

They are now being kept in a very tight prison and we don't even need to suppose that there's more than one of them.

There's now a proposal for how to extract useful work out of an untrusted super intelligence or untrusted superintelligences, plural.

From my perspective, it's like we have now headed off into a separate topic.

I don't care if there's two of them.

I don't care if there's one of them.

I want to know how you would at any event, like what does this begin to look like?

You have constructed your inescapable prison.

You have imprisoned the devil within it who is vastly smarter than you.

You are now going to attempt to extract useful work from the devil.

The despite not trusting it.

What is a range of action that is useful but you can actually prove things about?

Also incidentally, you're going to need an international worldwide treaty to shut down all AI everywhere that is not inside the inescapable prison.

As you try to extract tiny iotas of useful work from this devil, maybe you should.

I would say if you're going to do all the work to shut down every last bit of AGI worldwide except for this one devil inside his prison, maybe you shouldn't build the devil inside his prison either.

Maybe you're just asking for trouble here.

What are we going to get out of this devil that's worth the trouble of building the inescapable prison for it and risking that we're wrong about something.

So something I did not...

I'll introduce now into the conversation is that a lot of the separation that a good secure operating system can do, a lot of it, certainly not all of it, and there's some critical missing pieces, but a lot of it can be done...

A lot of the separation and constrained interaction can be done through cryptographic protocols.

And you know, that's clearly again what you know, blockchain and smart contracts and all that are doing is they're taking the participants who are not on the blockchain but interacting through the blockchain are jointly setting up a blockchain that constrains how they interact with each other according to an agreed upon, a mutually agreed system of rules for interaction.

I mean the fundamental question for crypto has never been is a blockchain possible?

But what the heck do you do with the blockchain that generates great economic value?

So what are you doing with the AI that generates great economic value without giving it the chance to kill everyone on Earth?

Okay, so, so let me just check.

You probably know a lot more about Alphafold than I do.

Is it simply implausible to you that a good knowledgeable team armed with AlphaFold..

This is basically a design rule-like question.

The design rules for getting to digital from analog are basically the ones where you can be confident that you've upheld the digital level of abstraction.

So this is something like can we trust Alphafold's predictions or?

I'm not quite sure I'm trying to understand...

No, no, no, no.

Alphafold has a certain level of reliability.

I'm not preresuming that we need to get far beyond that in order to solve this problem.

Given Alphafold's enhancement of our ability to predict the outcomes of DNA sequences, do you not believe that we could design a decision procedure well before artificial superintelligences using things with Alphafold to identify a range of where it's highly likely to be harmless but still useful?

That sounds like it is beyond the realm of Alphafold.

So are we injecting stuff into a human here?

Because inject stuff into a human is not predict how the protein folds.

It's predict how the folded protein interacts with everything else inside a human body and even a human brain.

Okay so just to to flesh out your scenario and make sure we're still in mutual understanding of what you're saying.

For the best drug discovery we can do on machines we assume are not malevolent, we still need to go through clinical trials.

And that's because that for all that we know, even with all the state of the art of Alphafold, et cetera, we still are not in a position to reliably predict what the outcome is.

Which I agree with.

And that once the thing generating, the discovered hypothetical drugs, might be malevolent then the ability to catch problems with clinical trials essentially goes to nil.

So there's sort of like a range of issues here.

One is, let's say we've got the worldwide treaty, no superintelligence, no AI is building other AIs.

All the very large models are being like trained under supervision.

Nobody's getting any larger than GPT04 or whatever the heck the current frontier model is.

Maybe not even that large.

Maybe we don't want to flirt that close to the fire.

Can we now go to Demis Hassabis at Google DeepMind and be like, so you know, these whole line of AIs you've got here that aren't LLMs but seem like they might be useful for medical things.

And I think the latest one in that was Alpha Proteo, which was starting to predict how folded proteins would interact with each other or design folded proteins.

I'm not sure exactly.

But AlphaFold was not the latest thing.

And there's a question of how far can we push that before the capabilities we're asking for, even in the realm of molecular engineering are inherently true AGI specialized down to the medical problem instead of narrow medically specialized intelligence that doesn't know that there's a world and humans out there.

Like, how far can we get close to the fire of AGI without it turning into GPT4?

You know, not the latest version, even just the original GPT4.

How much medical tech can we get before the thing knows that humans exist, before it is looking at the world with the kind of general learning capability that would deduce that humans exist, before it can reflect on itself?

What can you get from the pure medical engine while still having the sort of tests in place that would detect if the thing has ceased to be a medical engine?

Maybe you can get your cancer cure off of that thing.

I think at this level, like the, a lot of the people I see trying to ponder questions like these strike me as complete disaster monkeys.

So I am saying to the United nations like, you need to have me or somebody I designate, looking at the testing procedures that they use on this thing so it doesn't kill you.

But I can't tell them with a straight face, it's beyond human capability to set up tests like that.

I know that to be true.

So I'm sorry, I'm not clear if we're agreeing or disagreeing.

What I said was once the proposed drug might be proposed maliciously.

Then the testing procedure, clinical trial procedure, all that is essentially useless at catching unsafety.

If you are up against a very smart opponent, it can maybe cause the, the amazing, big muscles, no fat, smooth skin drug that it designed to subtly affect everyone's brains in any number of terrible ways and in such a way that it doesn't show up in the clinical trials.

Maybe it's on a delay, maybe it's something you're not testing for.

This is obviously going to be easier for somebody to do with a, you know, big old collection of large proteins than one tiny protein.

I do worry a bit that there are relatively subtle strategies here.

Like, you make everybody stupider or reckless in the right way, they build a super intelligence.

And then the superintelligence you thought you had contained inside the box says to the superintelligence that you built, once you all went reckless and slightly stupid, pay me like I predicted you would.

And that superintelligence was like, sure.

And it has, like done this great handshake across the void with a simpler molecule than you thought was possible.

I don't know if you keep track of the latest Busy Beaver number candidates?

But a big theme of the Busy Beaver search is that, There's a contest, how large of a number can you encode into how small of a Turing machine?

And it's like six state Turing machines.

You're expecting them to...

It's going to cough out after a billion or something.

Okay, some of them are numbers where I need to introduce new notions like titration and pentation in order to explain how large those numbers are in six state Turing machines.

And others of them are heading off into unsolved math problems where we can't even tell if they halt or not.

Those are the cryptids, but, you know, like something smarter than you.

Or in the case of the Busy Beaver machines, the result of a bunch of mathematicians looking hard for something you can get like pretty weird, powerful properties compacted into pretty small things, much more so than you would initially expect.

So, you know, how smart is that devil who you asked for the aging cure?

But of course it's not like one tiny molecule that's the aging cure.

It's going to be this big old package of proteins and maybe that does something it finds useful to human brain.

And it's thought all through what you're going to test through in the clinical trials and made sure it's not going to show up in the clinical trials.

And it's like, you know, so useful that everybody wants to take this thing.

You can sort of work out this whole scheme.

I want to note once again in the background, we are shutting down all the AI labs in the US and all the AI labs in China and you know, like taking the GPUs back from the United Arab Emirates or whoever, or Saudi Arabia or whoever it was that just got sold a bunch of GPUs.

Like, we are shutting down all the AI projects that is not this while we try to get a cancer cure out of the devil and not have it do anything weird.

So let me just for, you know, I set a bunch of things aside in engaging in a gedanken experiment.

Now bringing those things back in.

I'm not advocating burn the GPUs.

I'm advocating that we all proceed ahead full speed in a great diversity of ways in order to make use of the window of ignorance.

But I was bringing this operating system idea up because first of all, you're imagining that the nature of the coalition on the other side of the transition among themselves without including us looks something like a secure operating system with a division of rights to cause effects in the world enforced by a piece of software that's gating the actuators.

That was my attempt to describe a particular means of obtaining what is their goal, which.

Well, individually, their goal is to have the entire universe to themselves.

But in lieu of fighting a wasteful battle about that, they want to divide the universe up among themselves.

They want to all get some utility out of the universe in an amount that is fair for some technical division of fair relative to.

Their starting negotiating positions.

One humanly comprehensible means they could do that is the whole operating system scenario.

Possibly they just all decide to coordinate their actions in some way that does all that without bothering to build an operating system.

Now we just coordinated and...

with the operating system thing.

What they probably look like from the outside is an agent because to the extent a system cannot be interpreted as an agent, it's leaving value on the table.

But one way to get close to that is to, you know, be an operating system and, negotiate among your...

You know, have a market on what the operating system does in which you all have money.

And by the time you're done bidding your share, you've gotten as much utilities you're going to get out of the system.

You might possibly have some sort of family or something over there that is becoming a time limit on you.

And if so, I don't want to keep you past your fourth of July dinner or something.

Yeah, I'm, I'm.

Christine is leaving.

I was just saying goodbye to Christine and I am getting tired, frankly.

I'm trying to figure out.

I think we each understand each other and we disagree, but we know what we disagree about and we kind of know a lot about the cruxes of our disagreement.

I don't know how to make progress from here, but achieving mutual understanding, including understanding of what the disagreement is, I think is a tremendous step.

I think I certainly am very grateful for your attention in helping us do that.

I'm not quite sure we've localized the disagreement.

I think we've talked about a bunch of things, things.

We found a bunch of background premises that we agree on as a matter of course, as people with some amount of libertarian and computer programming background.

I think we found some particular things we seem to disagree about.

But I'm not quite sure.

If I had to guess at where the crux is right now, I think it would probably be something about you're seeing a sort of starting in trajectory of how things will go if left sort of to themselves or modulo like small philanthropic interventions where we've got this broad range of diverse intelligence systems that are developing kind of gradually, they occupy a continuum.

We've got a chance to have a bunch of particular ones with shaped motives.

We can try to build their government in advance.

We can try to build their prison in advance.

We can, you know, ask the dumb...

We can ask like the intermediate ones to help us prove things about properties that we want to prove the more advanced ones.

And I sort of see like all the current AI companies saying that they want to build AI that builds AI and OpenAI gets them first and their superintelligence slaughters everyone before anyone else has a chance to get up.

Or anthropic builds one first and their superintelligence slaughters everyone, including anthropic.

To be clear, before anyone had a chance to get one.

Even your scenario is going to require this enormous worldwide treaty just so that anything here has a chance to happen.

Then it still wouldn't work because there's this whole resource gradient from the happy world with the super intelligence is leaving humanity around and none of these super intelligences care about us.

They're all like, yeah, we can coordinate to get a bit more of what we want - off with their heads.

Okay, that's my sort of current model of the disagreement there.

Okay, that helps.

And I think I can.

I think my correction to that is actually fairly narrow, which means that otherwise your characterization of my position is very much was very good.

The thing that I kind of granted as a hypothesis and then explored is something that I believe in much less than I might have seemed to believe in, which was the formation of this coalition that, that uses this demarcation line of speed or whatever to leave human interests out.

That's not what I'm hoping for.

It's not what I'm hoping for.

And what I am hoping for, I think it's plausible enough that is worth trying, which is that these things are growing up within an extension of the kinds of coordination framework that we already understand, that we have precedent of, and that's enhanced by institutional innovation, by growing new institutions for coordination in this digital world.

And that the result is that the overall system of civilization is one in which the superintelligence, the more intelligent parts continue to contribute more and more to the aggregate superintelligence of civilization, but that it's a gradual enough transition that at every step, everyone involved, both the dominant power among these superintelligences as well as the people, continue to find themselves preferring to uphold the framework rather than to cut the humans out and construct a new framework.

Yeah, I model them as having much lower fractional switching costs.

I don't think that a group of a thousand or a million superintelligences need to burn anything like 0.1% of the solar system's energy in order to figure out how to talk among themselves and arrive at a new legal framework in which they are no longer obliged to leave 0.1% of the sunlight to fall on Earth.

Like, I think they can figure it out much more cheaply than that.

0.00000000001% switching costs.

They talk it over among themselves a bit and figure out how to do it.

Robin Hanson's argument that they could never possibly figure out how to ever switch governments for fear that they'll just switch governments again and kill each other.

This is Robin Hanson having to perform the duty of trying to explain why this group of a million incredibly intelligent entities just cannot figure out how to get a bit more of what they want without burning more than 0.1% of the solar system...

And it's kind of whack.

They're just going to figure it out.

So I will say in large measure, Robin and I are very much in tune.

One big difference that I hope has been clear in our conversation is that I think we're entering into a time of great danger.

I think we might lose.

And what I'm trying to do is find the best strategy forward.

But I'm not asking for that strategy to be anything like certainty of succeeding or even great confidence that it will succeed.

I'm just asking for it to be the best shot.

I think if you've got a million hostile super intelligences and you've started them off the legal system that implies that humanity gets to keep Earth and you're hoping they just can't like figure out how to take Earth among themselves.

That's not like, well, it may be, possibly somehow goes wrong.

That's like you tried something with no hope of succeeding.

Okay, so I think this is the core of our disagreement is I think it actually has quite a good chance of succeeding.

And I think that the demarcation line that this hypothetical coalition that cuts humans out, that its demarcation line is such that the only things admitted in are things that want...

Things that could not reliably carry human interests, you know, they could not reliably represent human interests as part of the overall machine bargaining framework.

It is about the both theory...

Like the sort of in principle difficulty, even if we had a functional civilizational approach to the problem, like even then I just don't think you can build the thing that is powerful enough to be a negotiating partner, but aligned enough that the maximum of all the things could possibly want are us living happily ever after.

And then one also notes that in practice, like our current AI industry is just plowing directly into this stuff with no clue.

So once again, so two issues there, first of all, yes, I think that you have identified correctly the remaining crux of our disagreement, which is this issue of can you build something that is both a negotiating partner that will be admitted into this coalition and is aligned with human interests?

And that's much narrower question than can you build something that by itself is a superintelligence and is aligned with human benefit?

Because to my mind, it doesn't need to have to have entered into this opaque super intelligent realm to plausibly be something that would be included in the coalition.

And given that it is, or given that it would be, there's no reason to try to form the coalition to cut out human benefit because the coalition would include representatives of human benefit.

I mean, if we know how to build a thing that's nice to us, why are we building any other kind of AIs at all?

What I'm not imagining is that there's one participant that represents the interests of all of humanity.

I'm much more, you know, once again, the numbers here, number of participants here could be very large.

Once the coalition could include computational elements that are of commensurate speed so that they're falling on the right side of the first demurcation line that we drew that can reliably represent human interests, they can reliably represent the interests of individual, individual humans or individual corporations.

The way things are going right now, representing individuals, the individual corporations I think is sort of the most natural outcome.

But it doesn't matter that there doesn't have to be one thing that represents the interests of all of humanity.

There needs to be things in there that individually represent interests of parts of humanity such that in aggregate humanity is represented in the bargaining process.

Not that humanity is represented in the wants of just one individual participant.

And from my perspective, it's something like the problem of who bells the cats gets no easier as we divide up the labor among the mice.

You know, the central cat belling problem.

Okay, but like how do you actually get the bell onto this cat is still there.

How do you get an AI that cares about that stuff you wanted it to care about more than it cares about all the other things it could have.

That's the maximum of its utility function.

The current technology is not there.

It ain't getting there.

If anything, we're kind of going backwards relative to capabilities.

And the current AI companies are just charging straight off into just build the recursively self...

you know, like to build the thing that builds us the super intelligence that we win.

So, so I'm very glad we had the earlier conversation about LLMs.

What the current AI companies are doing is trying to amplify the LLMs.

They're basically trying to, to make the LLMs more capable.

And if LLMs aren't even on the right track, in fact going to you painting the picture about the inspection of each other's source codes.

If the 'each other' is LLM-like then the source source code isn't what mattered.

It's what do the system of weights want?

And that will be mutually, mutually incomprehensible.

Probably in the same way that they're incomprehensible to us.

So if we're talking about LLMs that fall vastly short of what I would call superintelligence and are only around as smart as, like, I don't know, corporations or something, they're not even instrumentally efficient relative to humans.

Some humans know things they don't.

Some humans can like see better ways than they can to achieve their own goals.

And then they got to negotiate with each other while having opaque source code and not understanding decision theory either.

You could end up in a whole interesting class of scenarios that are not like the superintelligence scenarios.

Right.

It's possible that the LLMs are the ones trying to shut down the creation of superintelligence because they know they can't control it either.

Okay.

And there's a whole class of like, very weird scenarios we could end up in over there.

I suspect that probably mostly what happens is OpenAI GPT version whatever gets good enough to build GPT version plus one and that builds version plus two and that kills everyone.

And it's not an LLM anymore at that point.

But if we're sticking to things that are not that smart, then we can get into much more interesting territory here.

Okay, so.

So my point was to dismiss LLMs, as the form of AI that takes us into the dangers that we're jointly worried about.

I would guess that they will at no point train an LLM that is directly capable of building a Dyson Sphere.

I am worried that they will train an LLM that is capable of training the AI that builds the Dyson Sphere.

Well, I think it's possible, but I've seen, I mean, we've both seen, since we've both been doing this for such a long time, we've both seen over and over again some advance in artificial intelligence some sense that, if we just keep going on this road, maybe we solve the problem, maybe we solved a very large part of the problem and then it runs out of steam at some point.

Then at some point it runs out of steam and then you've got to shift gears in order to make the next stage of progress.

I utterly pray that we see this happens with LLMs.

I doubt it's how the prediction market bets.

And you know, you, you also got to go to the United Nations Security Council and tell them like you are running...

We are running very quickly out of time for this thing to ''run out of steam'' before it kills you.

You got to have the contingency plan for what happens if things just get going the way they have for the last two years.

Okay, so I think one final note...

Well, one final note from my side is, powerful positions attract people who want power..

Yeah, exactly, exactly.

That's by the way, that's why I picked this date as, as my preferred date.

So thank you for arriving here on this date.

So positions of power attract people who want power.

And if there is, you know, if we concentrate power inside a global regulatory regime, especially the UN but that aside, then the chances that the power that we've centralized in some set of decision measures will be used to help us get to a good future as opposed to, to be used to serve the narrow interests of the people in those powerful positions, I consider to be pretty small.

Which is one of the reasons I'm very anxious, one of the reasons why I see basically the whole thing to be an issue of centralized versus decentralized power.

That's why I'm so scared of singletons as well.

Yeah, I don't think decentralizing this is, you know, going to have it not just kill everyone.

To be clear, centralizing control of superintelligence would not stop it from just killing everyone.

I suspect we have a bunch of shared cynicism and even despair about the typical workings and outcomes of governments.

Okay, my perspective, it's just forced.

This kills everyone.

It doesn't matter who builds it.

It doesn't matter if the USA builds it, It doesn't matter if China builds it.

It doesn't matter if a US company builds it, it doesn't matter if a Chinese company builds it.

It doesn't matter if it's closed source, doesn't matter if it's open source.

It just kills you.

It just straight up kills you.

We somehow managed to get some amount of international agencies over some amount of restraint of proliferation of nuclear weapons, even as we see some of that breakdown under the pressure of people maybe not taking some things all that seriously.

And it didn't end up with an immediate world dictatorship.

It's not clear to me at this point that the Baruch plan would have necessarily been a...

Not Baruch Plan.

Which one was the like centralized all the nuclear weapons plan?

That actually started with Robert Heinlein's story Solution Unsatisfactory, that was written during World War II where Heinlein did not have any knowledge of the Manhattan Project.

But nevertheless he starts the story with use of nuclear weapons ending World War II.

And then the entire story is about the post war proliferation dilemmas.

People were expecting nuclear weapons to proliferate.

inevitably they were expecting a nuclear war.

It wasn't unreasonable expectation if you imagine that they had only read the history of the world up until that point with war after war after war, and people after World War I saying we really need to not do this again.

And then they had World War II.

They expected a nuclear war and they were reasonable to expect that.

Yeah, I agree.

The amount of proliferation control that we had, we somehow managed to end up not with a like global world tyranny.

And I think that we can shut down the creation of...

We need to try to shut down the creation of superintelligence.

Not that the right people have it, but that no one has it.

There are no other moves.

There is nothing...

You know, I mean, if you told me for sure that it was impossible, I'd go off and try something else, I suppose, because then there'd be nothing else.

But, you know, this thing you gotta try.

I wouldn't ask them to do anything complicated.

You know, the whole time I thought that there was any chance of alignment coming out of the world situation, I was not off asking there to be laws to put MIRI in charge, you know.

And why not?

Because I didn't think that the government would.

That governments would be able to successfully do complicated things.

That now the right course of action is narrowed down to something much simpler.

Don't build it.

Nobody builds it.

Nobody gets it.

This thing will just kill you.

It will just kill everyone.

You do not own it.

It owns you.

And when it has reached that level of simplicity, it is better to ask governments to try to implement that policy than have us all lay down and die.

So as we've seen with large governments and their large militaries.

Is that an agreement not to research a certain thing that has weapon potential does not lead to it not being researched.

We will need to take this more seriously than we have taken the, you know, countries potentially ending up in small amounts of nuclear weapons.

We need to take this with the same seriousness that applies to preventing global thermonuclear war.

If a country has their own little unsupervised AI project, we need to treat that not as the minor survivable threat of a small country getting nuclear weapons in small numbers where, you know, we invade...

But really the leaders of the countries know that even if this happens, probably they personally will be okay.

You need to treat it the same way we treat nuclear war, where the leaders of the countries know that if a nuclear war happens, they personally will have a bad day.

Maybe they will survive in a bunker somewhere, but it will still be a pretty bad day for them.

They're not getting to go to their favorite restaurant anymore.

Meaning that kind of regime against pushing AI capabilities further, not the kind of regime we had against proliferation of small quantities of nuclear weapons.

Need the anti thermonuclear war regime.

Okay, so, so this is an interesting historical case.

I've thought a lot about it as well and I, derive somewhat different sort of conclusions about what to do from the story.

I think Heinlein first of all basically got it right.

He was imagining that the nuclear weapons were more like dirty bombs, than fission or fusion, but nevertheless the dilemmas that follow post war.

I think he basically got it right.

One of the things that I worry about when things are painted in catastrophic or existential risk terms is that when people feel a certain level of fear, a lot of people naturally leap to totalitarian solutions, to creating, to somehow getting control over it, imagining that when there is that position of control, that the position of control will then be operated according to their preferences.

So very specifically in the post war proliferation.

I don't know the plan you were referring to the two plans that I do...

The three plans that I know of, I'm sorry, four plans.

Four plans that I know of is, I'm sorry, von Neumann just thought, well, we should just start a nuclear war with Russia now while we're confident we can win it and then it'll be game over and we'll be in charge.

There was once Russia also had nukes and now we in the Cold War.

Bertrand Russell said that the entire west should preemptively surrender to the Soviets and allow communism.

You know, we would then all be living under a communist dictatorship.

But that's still better than being wiped out by nukes.

Well, you can tell that he wasn't quite as smart as John von Neumann then.

The third was the generals, basically once nukes happened, they just saw it as oh great, more bombs, better bombs.

And then it was just kind of, let's just apply all of our military doctrine to say, okay, we've gotten better bombs.

We know what to do when we have better bombs.

And it was a small number of people, George Kennan, Tom Schelling, who took on the hard process of figuring out something that would work.

And one of the things that I think distinguish especially Kennan versus Von Neumann and Bertrand Russell is a lot of knowledge and experience with the world and what can work.

And these guys designed MAD and I talked to one of the negotiators of the START treaty.

I had lunch with him one day and , they were all kind of happy with the term MAD because they thought this is never going to work, this is crazy.

But we're the people on point, we've got to take the best shot we can take.

And this person I was talking to, just personally related to me, he never expected it to continue working as long as it has.

So I think in, in both that case, I think that case is inspiring.

Madison case is inspiring in both cases, you know, we're the people on point And it's all dangerous, it's all tricky, and we got to take the best shot we can.

I think you and I are completely in agreement on much of that.

And then what we disagree on, based on these particular things that I think we've both stated correctly, is what that best shot is.

Sounds about right.

Is it fair to say something like: if you believed what I believed about how much does this stuff just straight up kill you that you'd be in favor of trying to negotiate some equivalent of MAD where nobody builds the thing?

I think if I believed what you believed, surely the LLMs peter out and we end up in this sort of gradualist scenario.

In the gradual scenario we get like some stuff that likes us, then I would not be proposing, you know, the giant shutdown thing.

I'm not quite sure that follows.

It might still be pretty dangerous.

But if you believed what I believe, would you support humanity trying to just not do this thing?

I mean, obviously it's very...

I'll just acknowledge that there is a form in which that's a trick question that I know you don't intend.

Just to get that out of the way, which is if I believe what you believe, then my conclusion would be what you believe.

Right.

I mean, I have the same conclusions because that's what I believe.

Like the sort of if given the factual picture of what happens if we press this button, the mechanics and dynamics of AI.

I mean, I am not in the end and I do not...

It would be lovely if I had a ton of experience in international negotiations and international politics, but I don't.

I am just here to say this stuff will kill you.

Anybody does it, it will kill you.

Any nation does it, it will kill you.

Any company does it, it will kill you.

It just kills you.

Clever tricks you try to have it not kill you are not going to work.

It's just going to just kill you.

If humanity does this, it will die.

The form of humanity not doing this.

I feel like international, aggressive international relations to know that Chile asking all countries to individually not do it and having zero enforcement of that is not going to fly.

But if there's something different you would do if you believed what I believed about the AI part where it just kills you.

So I think that there's a very big, there's several big salient differences with nukes, one of which is the ability to know what to look for in order to prevent progress, in order to prevent proliferation.

In particular is that now with LLMs, which are making use of GPU farms, you know, you talk about burning the GPUs, but that assumes that this is the track that's ultimately going to get there.

I will quickly parenthetically remark that burn the GPUs was the thing I gave as an example of what you do with a moderately controlled superintelligence if you have it, but is not the thing you actually do with a moderately controlled superintelligence if you have it.

So, but, but like shut it down.

So when you say shut it down, you know, what I believe is that the thing we should try hardest to do is avoid a critical it and then the question is shut them down.

Sure, but suppose you believe what I believed, where it's it or them, just doesn't make any difference.

It all just kills you, okay, right.

If I believe that, then I might very well arrive at your conclusions.

And I think that's the remaining crux.

Yeah, so that, that does sound like we identified the key locus of disagreement, which is, you know, great...

I worry it's a little bit like too partisan if I try to end this on that note though, I think we, we kind of need to, to wrap it up.

So I should like give you a chance to monologue a bit and end this on a note that you would prefer or something.

I think that James Madison's approach to alignment for all of the problems has been in general, the only approach to alignment that has ever worked, that anything that centralizes power has gone badly and we're entering into a very new world.

Oh okay, I'll introduce another thing here that, excuse me, if this leads to more conversation, but is that when people have a lot of fear of what might happen.

Oh no, I did say this already...

When people have a lot of fear.

They generally reach for control systems imagining that it's going to have a good outcome and typically it has a very bad outcome because you've centralized power.

And that to me, the main thing is to find paths forward where human interests remain at the table and power remains decentralized.

Just, just to be clear, I think I've said this before, but I just want to repeat it again.

Like, insofar as any centralized power exists within the system I'm proposing, it is power to prevent AI, you know, further gain of AI capabilities that is being centralized in an international treaty type of thing and is not power over AI.

It is not that these people are being trusted by us to do nice things with more powerful AI.

This is a humanity needs to back off proposal.

And it could be, of course, that they take my proposal and run with it and just like, you know, use all their GPUs to build super intelligence that they think is going to serve them.

And then we all die that way.

We were all going to die the other way too.

There's a lot of roads here that end in death.

The one that doesn't end up in death is the one where we back off somehow, some way.

Okay, but by back off you don't mean that we never ever build super intelligence.

You mean that we delay until we figure something out that we don't currently know.

Yeah, like I would be against regulation or centralization of the projects to do adult gene therapy and efforts to augment human intelligence, that won't end well.

Okay, so my perspective with regard to your recommendation is an attempt to delay both loses the window of ignorance, recommendation is an attempt to delay both loses the window of ignorance, creates an overhang of, of knowledge of how to build superintelligence that hasn't been employed yet and does not delay it to a time where we understand how to solve what you consider to be the hard problems we would need to solve.

It doesn't delay it that far.

And if it doesn't delay it that far, then this overhang of knowledge makes a singleton emergence more likely.

I think we've identified the crux.

Like you're pointing to things that we lose.

And I'm like, I do not expect those things to save us to any significant probability.

So, you know, if I believed these things were would save us, then of course I would, you know, not want to centralize power and create the thing that would kill us.

Okay, so hopefully we've got the crux here.

Okay, good.

So it sounds like you're also, if I ask the symmetric question, if you believed the underlying things that I believe, would you arrive at my conclusions?

It sounds like you just said yes.

Is that, am I interpreting that correctly?

I have some hesitation about really feeling confident that I've understood your position, but it sure sounds like these things that you expect might make things go well to a stronger probability than hoping for any international arrangement to get anything right ever.

Yeah, well, that sure sounds like it would make your course the common sensical one then.

Okay, that is great.

We really understand what the disagreement is and we have this, we're standing on top of a huge mountain of agreement.

So that is great.

Yep.

All right.

Well, I'll say this much, I hope you're right.

It sounds like your overall set of predictions leads to better chances than my overall set of predictions.

So, you know, I hope you get to gloat sir.

I hope you win all of my metaphorical points off of me on the non-existent prediction markets.

May your reasoning be the valid one.

May your predictions be true.

May everything I say be wrong.

Let us hope.

Yep.

Okay.

Thank you.