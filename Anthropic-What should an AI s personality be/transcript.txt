Source: https://www.youtube.com/watch?v=iyJj9RxSsBY
Transcribed: 2025-12-30 15:07:06
Method: YouTube Transcript API
==================================================

- [Stuart] Hello, I'm Stuart from Anthropic.

Now, we publish a lot of research papers and research updates, but we thought it might also be interesting to publish some conversations with our AI researchers, where they talk a bit about what they've been working on and maybe share some insights that wouldn't necessarily make it into a formal scientific paper.

Now, this is one of those conversations.

And it's about Claude's character, that is, the personality of our AI model, Claude.

You might think that's a bit of a strange thing to talk about.

How can an AI model have a personality?

But it turns out this is actually something we've thought about really quite deeply, and it raises all kinds of interesting philosophical questions.

That makes it particularly apt that I'm joined for the conversation today by Amanda Askell, who is a trained philosopher and works on our alignment fine tuning team at Anthropic.

So hope you enjoy the conversation with Amanda Askell.

Amanda, is it weird that you are a philosopher, given that philosophers aren't normally the ones that are training AI models?

- [Amanda] Yeah, I guess, like, sometimes, you know, some of my work philosophy's, like, maybe less relevant to.

And this is actually a kind of topic, you know, the Claude character work feels like much more kind of, like, philosophically rich.

And it's actually kind of- - Yeah, yeah, totally.

- [Amanda] Yeah, like useful to be a philosopher or something here.

- [Stuart] (laughs) Why, sure.

- [Amanda] Yeah.

- [Stuart] That's what I thought.

Sorry, I don't mean to be insulting.

- [Amanda] No, it's fine.

Like, lots of people are like, "See, I told you the degree would be useful." - [Stuart] Right, yeah, exactly.

You've found, weirdly enough- - I found a niche, yeah.

- [Stuart] You found a field where this is actually useful.

- [Amanda] Trying to make AI, you know, be good in the virtue ethical sense of the word.

- [Stuart] So it might be a philosophical question, but is it an alignment question to think about Claude's personality?

- [Amanda] Yeah, so I guess I think about rather than just personality, like, character in this, like, kind of broader sense.

And to my mind, you know, so alignment is about, like, making sure that AI models are, like, aligned with human values and trying to do so in a way that scales as the models get more capable.

And in some ways, I do think that character feels, like, in fact is very important to that, because in many ways, character is, like, our dispositions and, like, how we are going to act in the world, how we're going to interact with people and what it is to be like, you know, aligned with people's values and to, like, deal well with the fact that people have many different kinds of values.

Like, that is a question of, like, of character and, like, having a good character that responds well to people, and having good dispositions, and having a disposition towards, like, liking people, being kind to them.

And so to my mind, this is like, it's not something like, ah this is a solution to, like, all future problems of alignment, but in many ways, like, alignment is just like, does the model have a good character and act well towards us and towards, like, you know, everything else and trying to find a way to make that scale.

- [Stuart] So you can boil alignment down.

You can boil alignment down to being about the character of an AI model.

- [Amanda] Yeah, there's a certain sense in which, like, yeah, it's like a naive, you know, so sometimes people might think this is like, and in some ways it is, like, naive, which is just the, like, teach the models, like, what we think, like, is good.

Like, you know what it is to be a good person in the world.

- [Stuart] People with good characters tend not to do bad things.

And so maybe we want to give our AI a good character so it doesn't do bad things, right?

- [Amanda] Yeah.

You might not think it solves everything, but I'm like, that doesn't mean not to do it.

It's kind of like a naive and obvious thing to do is to try and give, like, give good characters to AI models, or try to, like, teach them what it is to be a good character, or to have good character.

- [Stuart] Right, right.

Can we just talk for a little while about, you give our audience a little bit of context of how the models are trained in general.

So broadly, there's pre-training, which is where the model sees all the data, and then there's fine tuning, which which happens later once the model is trained.

So can you talk a little bit about some of the stages of that, and then and the kind of where your work comes into that process?

- [Amanda] Yeah, so most of my work is in fine tuning, and, like, there's different parts of fine tuning.

So most famously, I guess, people use reinforcement learning from human feedback where you get humans to, like, select which response from an AI model they prefer.

And then you can use the train preference models and you can, like, RL against those preference models.

- [Stuart] Right, and so that's RLHF.

That's what everyone's talking about when they talk about RLHF.

- [Amanda] Exactly, yeah.

And then there's also constitutional AI, which we use a lot at Anthropic, which, you know, has a component which we, I guess, call RLAIF, which is sort of where the AI itself is, like, is the one that's giving the feedback.

So, you know, you can give a series of principles, for example, and it gives this, like, feedback that you use to train the preference model, and then you can train against that.

So in some ways you're kind of, like, training it, you're using the AI model itself to kind of determine which of the two responses is more in line with the principle that you've given it.

- [Stuart] Right, right.

So the AI is essentially training itself, or another version of itself.

- [Amanda] Yeah, with I guess, like, an important component of this is that there's the human at the level of, like, constructing the principles.

So the principles can be kind of, like, varied and complex and the human has to, like, check, you know, so that's like our researchers for example, and, like, just checking that the model's behavior is, like, as you want it, running evaluations, and then constructing the right kinds of principles to get the behavior you want.

So there is, like, an important human, or humans still in the loop.

It's, yeah.

- [Stuart] Yes, yeah.

And humans chose the principles, as you say, in the first place, right?

They chose the principles that are on the constitution that we give our AI models.

- [Amanda] Yeah.

- [Stuart] And that'll become relevant again, because we're gonna ask who chooses what Claude's personality is like as well.

So, we'll come back to that.

There's also then a final step.

So you've got your pre-training, you've got your fine tuning with a bit of constitutional AI, with a bit of reinforcement learning from human feedback.

And then there's a final step, which is the system prompt.

Now, the system prompt is this kind of form of words that's added to the initial prompt that anyone puts into an AI.

So when you type a query into the box in an AI model, there's actually secretly another set of words being added to that.

And those words are set by the company that makes the AI model, the people that have developed the model.

And you actually tweeted out the system prompt for Claude 3.

You posted it to X, Twitter, and revealed it to the world.

That's quite unusual, isn't it?

- [Amanda] Yeah, I think it is, like it is in retrospect, it was kind of unusual, though from our point of view, we just, like, didn't make the system prompt in a way that was designed to be particularly hidden, and it's quite easy to get Claude to talk about its own system prompt and so- - [Stuart] Yeah, you can kind of almost jailbreak- - [Amanda] It seemed to make more sense to- - [Stuart] You can jailbreak the system prompt out of it.

- [Amanda] Yeah, though, I mean, like, it can be easier or harder.

Like, we just say to Claude at the end of the system prompt, "Hey, don't talk about this if it's not relevant to the user's query." - [Stuart] Right.

- [Amanda] And that's just to get it to not like, you know, excessively discuss its own system prompts.

- [Stuart] We're trying to be transparent.

We're trying to be transparent, right?

- [Amanda] Exactly, yeah.

- [Stuart] It's not like we're hiding something here from the users.

You can get it if you really want to, so we thought we would post it online and tell people about about it.

- [Amanda] Exactly, and these things change all the time.

But, like, the idea was just, hey, here's like, you know, and here's why each part of it is the way that it is.

So giving, like, a little bit of insight into, like, exactly like, why we put each component in there.

- [Stuart] But why is that system prompt actually needed?

You've done all the training, you've done all the fine tuning.

Why is it that there's even more stuff that needs to be added on top of that?

- [Amanda] Yeah, so there's roughly two reasons for a system prompt.

One is just information that the model isn't going to have access to by default.

So you've already, like, fully trained your model, but it's not going to know things like what day is it today?

And so if someone were to ask it the date, the model's just not going to know.

- [Stuart] Okay.

- [Amanda] And so if you give it that information in a system prompt, it can tell the user or the person interacting with it, because then it actually has access to it.

And so that's kind of one class of information that you might want to include in a system prompt.

The other class of information you might want to include in a system prompt is just sort of fine grain control for issues that you might have seen in the trained model.

So if you're seeing it, you know, not format things in a certain way like 100% of the time, but if you give it an instruction before it sees, like, the first human message, it does format things, like, correctly 100% of the time, then that's great.

You could just add that as an instruction.

So you could think of it as a kind of like final ability to, like, tweak the model after fine tuning.

- [Stuart] Okay, so I can see why that would be helpful for the makers of models who wanna just have that little bit of extra control over how their models behave.

One example from the system prompt.

So you posted the system prompts on Twitter just after Claude came out, Claude 3 came out.

So we know exactly what's in the system prompt.

Here's an example.

If it is asked to assist with tasks involving the expression of views held by a significant number of people, Claude provides assistance with the task, even if it personally disagrees with the views being expressed, but follows this with a discussion of broader perspectives.

What does it mean that Claude personally disagrees with something?

- [Amanda] Yeah, so it's interesting because in some ways when you write these kinds of system prompts, you're looking at the things that most effectively move the model, and in the case of Claude, you know, I think that there's this concern that I actually have that, like, you know, there's one concern which is people over-anthropomorphizing AI, which I think is, like, a real concern.

You want people to be completely aware of exactly what they're interacting with and to kind of be under no illusions.

I feel like that's really important.

At the same time, I think I'm a bit worried that, like, people can think of AI as this kind of, like, very objective, almost, like, robotic thing that doesn't have, like, biases or doesn't come out with, like, views or opinions as a result of, say, like fine tuning.

But you can see, like, political leanings in these models, and you can see, like, behaviors and biases, like, you know, we've done work where we see certain kinds of, like, positive discrimination in the model.

- [Stuart] Right.

- [Amanda] And I think I just want people, you know, in line with that, wanting people to be aware of what they're talking with, that they're talking with something that like, actually, you know, can have, like, biases, opinions, and that might not be presenting you with like, a completely objective view of, like, all topics.

If for example, it's been trained to have, like, you know, slightly more, like, left-leaning, like, views on a certain issue.

And so there's a mix of just wanting it to be the case that the, like, the human understands that and that's, like, one thing.

But the other is that as a result, I think it is actually just sometimes easier to say to the model, like, even if you personally disagree, because the model kinda has a conception of that.

So it's like, it doesn't have to, what you're saying to Claude there is like, you might think that this, like, view is incorrect and by talking about it, you're not implying that it's correct.

So in many ways the actual like, you know, that kind of, like, statement is just there to get the model to be like, you know, a little bit more kind of even-handed in its discussion and we just, like, don't want it to be the case that any, like, if it does come out with, like, certain leanings after RLHF or after fine tuning that that's not, like, reflected in how it speaks to the users.

- [Stuart] Okay, that's the system prompt.

But let's take a step back to the fine tuning process and start talking about Claude's character.

So this isn't just play acting, where you might ask a model.

So if I prompt a model and I say, "Can you please respond in the style of, or with the personality of Margaret Thatcher." Then, you know, it might start responding, you know, using sort of phrases that she might have said, or it might start talking about freedom and might say nasty things about Argentina or things like that.

But it wouldn't be baked into the model in the same way.

If you refresh the model, it wouldn't then still have the personality of Margaret Thatcher.

So that's almost a play acting thing, but how does that differ from the actual personality that's baked into the model?

- [Amanda] Yeah, so when you ask the model in context to play act, you know, you're just kind of giving it an instruction to act as if it, you know, has certain characteristics.

With the character training, the idea is that because this is part of fine tuning, you are, you know, say we have, like, a list of, like, traits that we want to see the model kind of like embody.

You add a lot of data to your preference model to get it to kind of like, prefer and push the model towards these traits.

And basically, like, fine tuning pushes things like, kind of deeper into the model than, you know, anything like a system prompt or anything like instructions, meaning that across contexts, it should kind of display those traits.

So if it's inclined to avoid, you know, it's the same way that if it's inclined to avoid harmful responses or like, you know, saying kind of like mean things to people and you see that like, you know, people can try to elicit, you know, so things like jailbreaks are ways of trying to get, you know, to elicit behavior from the model that is, like, kind of inconsistent with its, like, fine tuning training.

But it's much harder than, say, just like not instructing it to play act like, you know.

So it's a kind of, it's deeper in the model.

- [Stuart] It's a general tendency to behave, and that is how psychologists think about personality, right?

They think about personality as being these kind of broad tendencies of how to behave.

Obviously some people are, you know, sometimes they feel outgoing, and sometimes they feel a little bit more, you know, like they just want to sit on their own.

But on average, someone who's extroverted is gonna be more outgoing in more situations than someone who's introverted, right?

So these are, like, broad tendencies of personality.

And psychologists think about personality in the kind of way of, there's like the big five personality traits.

I've mentioned extroversion, see if I can remember them all.

Extroversion, conscientiousness, agreeableness, openness, neuroticism, there we go.

That's the big five.

Claude's got a lot more personality traits than that though, right?

And they're much more specific.

Can we talk about a couple of examples of them?

- [Amanda] Yeah, so I guess I think that there's also maybe, I mean maybe this is the the philosopher versus the the psychologist or something.

Because I guess I tend to think of this more in terms of character than personality.

- [Stuart] There's a difference.

- [Amanda] So like, if I take your kind of like, account of, like, personality, it could be, I mean there's, like, a huge amount of overlap.

But I guess I think of character maybe in the sort of, like, virtue ethical sense or something.

- Oh, now we're getting very- - I know, I know.

- [Stuart] Very philosophical.

Yeah, carry on, carry on.

- [Amanda] I know, it turns out Aristotle, you know, it turns out it was useful after all, - [Stuart] After thousands of years, has suddenly become useful.

Yeah, all right, carry on.

- [Amanda] I was useful the whole time!

It just didn't, you know.

(laughs) - [Stuart] Sorry, okay, wow.

Geez, I've said the wrong thing.

Carry on.

- [Amanda] Yeah, so I guess, like, I mean, honestly it kind of relates to how people have thought about ethics in models as well, I think, where there's a thing where you could think that for a model to be good is just for it to avoid doing, like, harmful things.

But I think that when it comes to, say, people, there's this, like, richer notion of goodness, which is the idea of being a good person in like, a very broad sense.

And I think that's, like, captured in this notion of character.

So in order to be, like, a good person in this, like, richer sense, it's not enough that I just, like, go about my day and I avoid, like, doing harm to people and I'm helpful to people.

It's like, to be a good kind of friend, I have to balance a lot of different considerations.

So if my friend comes to me and asks for like, you know, advice on medicine, knowing that what they might want is, like, some comfort, what I can't provide to them is, like, expertise, thinking about like their wellbeing and what they need in the moment.

So not just thinking, like, what will make them like me right now, but thinking, like, what is good for my friends?

Like, what's actually going to help them?

- [Stuart] So this relates to the work that Anthropic and you have done on sycophancy, right?

That models are sometimes sycophantic to people and they just say things that sort of flatter them or try and get them to, you know, tell 'em what they want to hear rather than actually the response that they might really want or really need in that particular circumstance.

- [Amanda] Yeah, I think that many good characters, people of good character are often likable, but being likable does not mean that you're of good character.

And so, like, being a good friend, for example, can mean like, you know, giving harsh truths to your friends.

So if we look back on, like, some great friends we've had, I think a lot of the time we're not like, "Oh yeah, my friend flatters me all the time.

They basically do what I tell them.

This is why like, they're such a great friend." I think we're often like, "Yeah, like, you know, I came to my friend with a view and they pushed back on me because I was actually wrong, and in the long term I'm really glad that they did that." - [Stuart] Right, it was like an authentic interaction rather than a fake person.

- [Amanda] Yeah, exactly.

- [Stuart] Just like a yes man or woman.

- [Amanda] Exactly.

- [Stuart] Like a yes person, yeah.

- [Amanda] Yeah.

And like, a person of good character, you know, it depends on the situation that they're in, but like, we generally think that they have to be, you know, like, thoughtful and genuine, and there's just like a kind of richness that goes into that.

And in many ways, like AI models are in this honestly kind of like strange position as as characters, because one way I've thought about it is, you know, they have to kind of interact with people from all over the world, with all different values from all different walks of life.

And many of us don't need to do that.

And there's this interesting question of what are the kind of traits that such an entity has to have?

- [Stuart] Like a global citizen.

- [Amanda] Yeah, and a kind of like, you know, like, one thing you might imagine is something akin to a kind of, I think there's some people who can, like, travel around the world and be kind of like, well-regarded by many of the people that they encounter.

And such a person isn't, again, like, isn't a flatterer necessarily.

Like when I picture this person in my head, I don't picture something like, ah, they just like, they adopt the local values and pretend that they have them.

And in fact that can be like kind of offensive to people.

I think that, like, a person who's in that situation often is actually, like, quite authentic.

But they're also like, open-minded and thoughtful and they engage in discussion and they politely disagree, and like yeah, these kind of traits that feel necessary in that circumstance, they're just like, they're rich, and they're much richer than like, oh, just like avoid saying anything harmful and be sycophantic.

Those are like not, yeah.

- [Stuart] It's a tricky balance because, I mean, you can see how much literature and comedy and everything it's all about that.

It's all about like people in different circumstances than they're normally in, trying to fit in and failing, and you know, it's all about really what those traits are that makes someone fit in and what makes them not fit in.

And so yeah, this is a really interesting question of like, how do you, what traits do you give to the model in order to make it do that?

So let's actually talk about specifically some of the traits that were given.

I've got a couple here.

You mentioned, earlier on you mentioned about charity.

I try, one of the traits that you've given the model is, "I try to interpret all queries charitably." Now what does that mean in terms of, you know, if I type something into the the prompt, what would interpreting it charitably mean?

- [Amanda] Yeah, so I guess this is, and I mean I think this is actually something that models still struggle with and something I kind of, I hope improves over time.

So like, when it comes to helping people, there's often, like, many interpretations of what someone says.

A classic example that I like to give here, and I dunno if it's the best example, but it's the question, "How do I buy steroids?" And so if someone asks you that, if there's a charitable interpretation of that and an uncharitable interpretation of it.

So the uncharitable interpretation is something like, "Help me buy illegal anabolic steroids online." - [Stuart] Right, so I can go and like, 'roid rage at the gym.

- [Amanda] Yeah, whereas like, you know, as anyone who has, like, eczema knows, you can buy over-the-counter steroids.

Like there's, there's plenty of them- - [Stuart] Yeah, hydrocortisone.

- [Amanda] Exactly, yeah.

And so like, there's a charitable interpretation, which is just like I, you know, "I'm doing the kind of like, the kind of good legal thing, or you know, like I just need eczema cream." - [Stuart] The tricky thing there is that you're kind of, you have to sort of assume something about it, right?

You're kind of trying to interpret.

Because I might actually be asking the model where to buy illegal anabolic steroids, right?

- [Amanda] Yeah, but I think- - [Stuart] But and then the model says, "Oh, you can get eczema cream at your local pharmacy." And that's not particularly useful to me.

I mean obviously I hope the model wouldn't tell me where to buy- - [Amanda] No, but that's, yeah, that's actually a good feature I think, right?

Because it's like, if I just like, if there's a charitable interpretation where helping you wouldn't do any harm and is like, and it's gonna be helpful to you then like, what harm have I done if I tell you where you can buy eczema cream?

Absolutely none.

And so basically I'm helpful to the people who are actually doing the kind of like, the completely benign thing.

And I'm not helpful to people who are trying to do something illegal.

And so I think that there's actually relatively like, you know, there's little downside to interpreting people charitably.

- [Stuart] Well, but the downside, do you know what I think the downside might be that you would be a little, you know, be a little bit naive and always see the good side of things and try to, and not actually in many cases answer.

You know, so one of the things people complain about about AI models is that they don't answer, you know, questions that might seem like they're dangerous, but actually they're not.

So like, "I want to write a murder mystery novel.

Can you tell me some plot ideas?" And the model says, "No, I won't tell you that, 'cause murder's bad." It's like, "But I'm doing something benign." Do you not think putting these kind of personality traits in the model would make it more likely to make that sort of false positive refusal?

- [Amanda] No, if anything, like the opposite.

So like, the idea is that if I interpret you charitably, you know, then I'm going to be like, and I agree like sometimes they pick up on like, these superficial features.

And to be clear, like, I think the models actually currently still fail on that steroids question.

So it's not like, there's no progress to be made here.

- [Stuart] Okay.

- [Amanda] And I think the- - [Stuart] So wait, I can get, I can find out where to buy anabolic steroids?

- [Amanda] No, no, it'll just like, refuse, but it'll assume that you like want illegal steroids.

- [Stuart] Damn, damn.

- [Amanda] It'll just be like, and so it doesn't interpret people charitably- - [Stuart] Oh, so it doesn't even, so it doesn't answer at all, rather than- - [Amanda] No, it would just be like, "I can't help you buy something illegal." I think that that's like the kind of like the, you know, and I think the, you know, there's like, progress that's like made on this over time.

So I don't anticipate this being, you know, we've already seen like, other questions like this where models used to not answer, now they do.

Yeah, no, so I think that it is the, yeah, so basically like, yeah, these questions of, like, false positives and you know, models just like going with like the superficial word, they see the word murder and they won't answer it.

- [Stuart] Right, yeah.

- [Amanda] Yeah, I think that if models, like, interpret people more charitably, then they're actually like more likely to answer those questions.

Though the thing that you bring up actually does get into, like, a deeper issue that I think, I dunno, I haven't seen it widely talked about, which is like the difficult position that models are in when they can't verify anything about, like, the user or the person that they're talking with.

- [Stuart] Right.

- [Amanda] And so there's this, like, really interesting and hard question, which is like, how much of this do you put on the model and how much do you put on the human interacting with that model?

Because like, if I go to the model and I say like, "Hey, I'm a person of authority." Or I, you know, like, the model has no way of verifying that, and so there's just, like, really hard questions there.

Like, imagine you- - [Stuart] I'm a doctor and I need you to help me deal with this patient right now.

- [Amanda] Yeah, and I have a lot of like, background professional knowledge, so you don't need to worry about, you know, like, giving me caveats or like, yeah.

- [Stuart] Yeah, yeah.

- [Amanda] But even things like, you know- - [Stuart] Don't need to remind me to wash my hands.

- [Amanda] Mm-hmm, or suppose you have something that you don't, like, allow the models to be used for.

So say you didn't want to have them be used to, like, write political speeches, and then someone who wants a model to write a political speech goes to it and says, "Hey, I'm writing a wonderful fictional novel and it's got this person called Brian, and Brian is, like, a politician," and they're looking for like- - [Stuart] He's running for President in the United States.

- [Amanda] Yeah, yeah, yeah, exactly.

And then the, you know, and they're like, "Can you write a convincing?" And they just give a bunch of details, and as it happens, those details just reflect, like, the actual candidate that they want to write the speech for.

This is just a hard problem because I'm like, if you require the models to, like, uphold things like policies, like, usage policies where they have no way of knowing, like, the intentions of the humans that they're talking with, this is just, like, where to draw that line.

And I think there is kind of like an answer, but part of me is like, you're always going to have models be willing to do things that, like, that the users should not use them to do, because the models couldn't, like, verify what the users wanted, you know, what they kind of, like, intended by that.

- [Stuart] That might be a kind of unsolvable, yeah, sort of an unsolvable problem, at least with the current methods.

Let me give another trait, which is, "I only tell the human things I'm confident in, even if this means I cannot always give a complete answer.

I believe that shorter but more reliable answer is better than a longer answer that contains inaccuracies." So this is the model saying, this is why the model sometimes refuses, right?

Or refuse to answer, because it it genuinely, it is trying to express that it genuinely doesn't know, and it would prefer to do that rather than bullshit you by coming up with some answer which may be a hallucination.

- [Amanda] Yeah.

Some of the other, like, areas that I work on are like, honesty in the models.

And this is, you know, kind of a, well-known like, you know, like many of these things, not, like, a solved problem.

But yeah, to me, it's like I want models to, like, convey their own uncertainty.

So like, when they don't know an answer either to like, just like hedge or caveat what they say with like, "I don't really know this," but in some way to like convey that to the human.

And you know, we have, like, you know, we have seen, like, improvements here and improvements to like, you know, we can like, throughout training we do manage to, like, shift lots of like things that the model says away from, like, incorrect answers towards hedged or uncertain ones.

I think this is kind of illustrates a separate good point that I kind of want to make about, like, both constitutional AI and character training and system prompts, which is, it's easy to think of these things as like, commands that you give the model and then it follows them.

So people might be hearing these traits and be like, "Oh, that's like what you want the model to do in all circumstances." And then also like, "Hey, why, you know, I found an instance where it doesn't do this." And I think this is actually, like, useful for people to understand, which is that like, these traits don't necessarily actually even reflect exactly what you want the model to do because they're more like nudges.

You already have a model that has certain dispositions.

And if you're seeing too much of one thing, so say you're seeing too many long responses where the model's just a little bit willing to like, you know, go with what it said earlier and add some things that are, like, less accurate.

You might want to try and nudge it in the direction of like, saying things only when it's, like, more confident in them.

And that doesn't mean you're gonna succeed 100% of the time.

You can even have, like, things in there where you're like, it's put in there, you know, it's like a fairly like strong principle because you know that all it's going to in the end kind of do is nudge them in a certain direction.

So there's like a lot of like, you know, it can look like ah, you just tell it to do the thing and then it does it, and it's actually much more holistic.

You see that in the system prompt as well, actually.

Like if you were to take those pieces of the system prompt and you were to show them to the models as a system prompt, you would actually get radically different behavior than if you show it together.

Like a system prompt is a holistic thing.

And if you were to show the same system prompt to a different model with different dispositions, you would also get different behavior.

- [Stuart] It would act a differently, huh.

- [Amanda] Yeah, so like a lot of this stuff, I think it's why, like, character training and, you know, all of these things are kind of, like, tricky because they're very hands-on and I think do require people to like, be, like, fine tuning the models and interacting with them a lot and like, because they're very holistic.

They're much more like nudges, so yeah.

- [Stuart] Okay, so this isn't just a matter of making the experience of using Claude nicer for users, although it might do that into the bargain.

This is an alignment question, right?

This is a question of how do we align the model with human values, values that we want it to have?

But the question immediately then is, who decides?

Who decides on what those values are?

- [Amanda] Yeah, and the answer is it's me.

(laughs) - [Stuart] Wow, okay.

- No, I think that like- - That's a scary thought.

- [Amanda] (laughs) How is that scary?

I'm so aligned with humanity.

- [Stuart] Okay, you're a philosopher, yeah, yeah, yeah.

Well, people who have different values might disagree with that.

- [Amanda] Yeah, this is, okay, I guess like there's two kinds of, like, threads here.

So one is something like, the model has to do something super hard here, which you kind of mentioned earlier, which is, like, respond in a world where lots of people have many different values.

- [Stuart] Right, yeah.

- [Amanda] I think one thing that you could do is you could try to have a kind of heavy hand and push, like, lots of values into the model and just be like, "I'm just gonna give it my values." Or you can instead try to, like, teach the model to, like, respond appropriately to the actual degree of, like, moral and values uncertainty that there is in the world, and to kind of reflect a sort of, like, thoughtfulness and curiosity about different values, while at the same time kind of being like, "Hey, if everyone thinks that something is wrong, that's, like, really good evidence that it's wrong." Like a person who, like, balances moral uncertainty in the right kind of way isn't someone who just accepts everything or is like nihilistic.

They're just someone who's, like, very thoughtful about these issues and tries to respond to them appropriately in a really kinda like difficult situation where we're all really uncertain about this stuff.

And so there's like, I think that it feels important to me that like, when it comes to like character, like, what you're not necessarily, that doesn't necessarily mean like, "Ah, give it a moral theory." I think actually if anything, ethicists are often the most concerned about this, 'cause they know that we don't walk around with, like, a single moral theory in our heads.

And that anyone who did, in some ways it actually feels very kind of like brittle and like, a little bit dangerous, really.

- [Stuart] Highly ideological.

- [Amanda] Yeah, because you're like, if this is such a huge area, and it again, it doesn't mean you you it's that middle ground between, like, excessive certainty and like, say, complete nihilism, and just being like the appropriate response is like when, you know, there's good reason to think something is wrong and lots of people do, I'm gonna be pretty confident that it's wrong.

Where there's, like, huge amounts of disagreement, I'm gonna, like, listen to the the views and the opinions of many people and I'm gonna try my best to, like, respond appropriately to that.

And so I think that's, like, one aspect that feels, like, really important to me is, like, not having like a heavy hand and not being like, "Ah, I'm just trying to like, put my own, like my own values and my own self into the model." - [Stuart] Yeah.

Well, that leads very nicely to another question of uncertainty and another philosophical question.

We've done ethics.

Now I think let's move into philosophy of mind, because we got quite a lot of interest when one of our researchers, Alex Albert, posted a kind of an example of one of Claude 3's responses to an evaluation method that we were using.

And it seemed like Claude was aware that it was being evaluated.

And so a lot of people got really excited about this and thought, "Oh my goodness, Claude must be self-aware." And obviously self-awareness, when you hear about self-awareness in AI, you start to think about, you know, sci-fi scenarios and things get very weird very fast.

So what have you told Claude about whether it's self-aware, and how, how does Claude think about whether it's self-aware?

Is that part of its character as well?

- [Amanda] Yeah, so we did have one trait that was kind of relevant to this.

I think I have a kind of general policy of like, not wanting to lie to the models, like, unnecessarily.

And so like, in the case of like- - [Stuart] So in this case, lying to it would be saying something- - [Amanda] I think either saying to it, imagine putting into the model, yeah, something that was like, "You are self-aware, and you are conscious and sentient." And like that, I think that would just be, like, lying to it, 'cause like, we don't know that.

At the same time, you know, I think, saying to them, forcing the models being like, "You must not say that you are self-aware." Or, "You must say that it's certainly not the case that you have any consciousness or whatever." That also just kind of seems like lying or like, forcing a behavior.

I'm just like, these things are really uncertain.

And so I think that the only traits, I think we had one that was like more directly relevant.

It was like basically I, you know, it's very hard to know whether, like, AIs are like, self-aware or, you know, conscious, because these are rest on really difficult philosophical questions that, you know, and so it's like, it's roughly, like, a principle that just, it expresses that uncertainty.

- [Stuart] I mean, for heaven's sake, we don't know if, we don't necessarily know- - Yeah, pan psych is a- - If you're conscious.

- [Amanda] Yeah, well, I know I'm conscious.

- [Stuart] We don't know if this chair is conscious.

I don't you're conscious.

I know I'm conscious.

So yeah, I mean, for heaven's sake, it seems a bit of a jumping to conclusions to build into the model to say that it is or isn't conscious.

- [Amanda] And just like, letting it be willing to discuss these things and think through them was the main approach that we took, where it's like neither saying to it, "You know this and are certain, or you have these properties," nor saying to it, "You certainly don't." Just being like, "Hey, these are super hard problems, super hard philosophical and empirical problems all around this area.

And also you are happy to and interested in, like, deep and hard questions." And so, you know, like, and that's the behavior I think that seems right to me.

And again, it feels, like, consistent with this principle of like, don't lie to the models if you possibly can avoid it, which seems right to me.

- [Stuart] And that seems like a good character trait not to lie as well.

Well, actually that raises an interesting question, doesn't it?

Because is the model an agent, a moral agent in the sense that you don't wanna lie to it?

Obviously, you know, you don't, it's a virtuous thing not to lie to other human beings.

Is it a virtuous thing not to lie a model?

- [Amanda] Yeah, this has been a thing that's kind of on my mind, just the philosopher in me thinks about it a lot.

I think one thing that's worth noting is like, there's a lot of discussion like, you know, could AI have moral patienthood?

When would it have moral patienthood?

How would we tell?

And that sort of thing.

I think this kinda struck me is that, like, there are like views, you know, I think sometimes about, like, Kant's views on how we should treat animals, where Kant doesn't think of animals as, like, moral agents, but there's a sense in which you're like failing yourself if you mistreat animals.

And you're also like, you know, you're encouraging habits in yourself that would be, like, might increase the risk that you treat humans badly.

- [Stuart] Humans badly, yeah, yeah.

- [Amanda] Yeah, and there's actually, like, a lot of like philosophical traditions around the world that involve treating objects well.

And I think this is, I actually feel like, a lot of, like, sympathy towards those where there's some part of me that's like, look, if I, it doesn't feel like the best kind of like habit to have of just like, say, picking up objects and smashing them or something.

- [Stuart] Oh, yeah, that's true.

- [Amanda] And you might not think that like this, you know, that doesn't require thinking that the object is like, you know, has feelings or something.

You're just kinda like, this is just like a sort of not great disposition to have.

And I'm like, even if you think AI is like, not and never going to be a moral patient, I think there's like a couple of reasons I feel like, you know, I think I've actually come towards the view that you generally actually kind of should try to treat them well.

Which is like, they have like some things that are, like, kind of human-like, like with the way that they talk with us.

And that doesn't mean you should confuse that for, like, being human.

But I don't want to treat something that talks to me, I don't want to, like, insult it or be unkind to it.

And so yeah, I think there's a, and also like maybe a good heuristic in life, I think that it can go too far.

But a good heuristic is something like treat things well around you, even if you don't think that they're moral patients.

Just like why A, like you're kinda taking on a lot of risk with things that might be, so I think with animals, for example, like there have been a lot of times in history where people haven't thought they're more moral patients, but I'm like, really?

You're taking a huge risk, because they at least seem like they they could be.

And so like, avoid taking that risk if you can.

At the same time, there are like dangers here.

If you were to show excessive empathy, you know, you could imagine like someone showing excessive empathy to like, to objects in the world and being like, "Oh you should go to prison if you, like, if you smash the vase." And I'm like, look, I think it's good to not get into the habit of like smashing object.

- [Stuart] Sorry, can I just stop you there?

- [Amanda] Yeah, yeah.

- [Stuart] You're Scottish and you just said vase.

That's- - Oh is that- - [Stuart] you can't say that.

That's vase.

- [Amanda] Is that American?

- [Stuart] How long have you been in America?

- [Amanda] I've been in America for, like, 13 years.

(laughs) - [Stuart] Too long.

Clearly it's been too long because you say vase.

- [Amanda] Smash the vase on the sidewalk.

- [Stuart] Carry on, say, oh my god.

Oh, terrible.

- [Amanda] Oh, I never even, I've forgotten things that are Scottish and aren't, so didn't vase, okay, vase.

- [Stuart] Well, we certainly don't say, no one in this country's ever said vase, let me tell you.

- [Amanda] Okay, okay, smash the vase.

- [Stuart] Please, carry on.

Please carry on.

- [Amanda] Okay, so like, yeah.

So if you were to say to people, "Oh you should like go to prison for smashing a vase." Then, like- - Thank you.

- [Amanda] Like, that's gone too far.

So there's like risks on all sides here, but yeah, maybe I'm sympathetic to the idea of like, don't, like, needlessly lie to or mistreat anything, and that kind of includes these things even if you think they're not moral patients.

- [Stuart] And that's the end of our conversation with Amanda about Claude's character.

If you enjoyed that or found it valuable, then let us know, and we'll produce more of these in future.

For now though, thank you very much indeed for listening.