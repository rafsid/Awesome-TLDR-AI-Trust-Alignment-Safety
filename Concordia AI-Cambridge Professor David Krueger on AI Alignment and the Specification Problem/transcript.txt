Source: https://www.youtube.com/watch?v=huRdeDL_x44
Transcribed: 2025-12-30 17:36:22
Method: YouTube Transcript API
==================================================

krer thanks for being here David and the floor is yours now all right great yeah thanks so much for inviting me I'm really happy to be here today talking to you guys and to be included program with other great speakers like Rohan and Max um so I'm going to be telling you about AI alignment and the specification problem and talking a little bit about some of my thoughts and uh some of my work on these top topics so before I do that I want to just tell you a little bit about me and where I'm coming from so I've actually been concerned about the social impact of AI since about 2009 while I was still an undergraduate uh but at the time I really wasn't that concerned about uh alignment or safety because I thought that we were very far away from any sort of meaningful progress in a artificial intelligence and that really changed when I heard about deep learning in 2012 and as soon as I saw what neural networks were capable of in terms of image recognition and text generation uh I knew that this is what I wanted to do and that I had to get involved with this field so previously I had just been studying mathematics and so I started my Graduate Studies in 2013 I say 2012 here because that's when I saw Jeff hinton's corsera course online about deep learning and that's what really got me in into the field and uh about a year ago I began as an assistant professor at Cambridge and uh in my new role there I'm building a research group that's focused both on deep learning but even more so on AI alignment and so uh one thing I just want to mention right now is that we are hiring for postto so if you or anyone you know might be interested um please check out the applications or get in touch great so I mentioned that I got into the field uh in 2012 and this was really the beginning of what people have called The Deep learning Revolution and I think it was hard for most people to see at the time how much Impact deep learning was going to have so in 2012 people working on deep learning were still kind of viewed as somewhat Fringe and it was not widely accepted in the mainstream AI community that deep learning was going to play such a large role but uh there were people in 2012 already who were able to see that this future was possible that deep learning would become the key driver of AI progress and that existing methods would be rendered almost completely obsolete by Deep learning and at the same time a lot of the times it was the generation the people who were just entering the field who were able to see this because they had a fresh perspective and uh I talked to many people in my early years in the field who were very interested in deep learning but they had a hard time convincing their supervisor to pay attention and that it was worth working on and right now I'm seeing a lot of parallels with AI alignment so I think in a few years ago we had the development of gpt3 this large language model and I think this was really comparable to the development of alexnet in 2012 for deep learning in terms of being the first clear sign that we might see a big change in the field in terms of how progress is being made and one of the key claims that I want to make here is that in this new paradigm of very large models sometimes called Foundation models uh AI alignment is going to be one of the key drivers of progress and that's because as we scale up these systems they get smarter and smarter and better at understanding the world and the data that we give them but they don't necessarily understand what we want and we can see oftentimes that we're able to get much better performance once we communicate to the AI system what it is that we want it to do so uh let me give an example of that so here is an example using gpt3 and originally if you ask gpt3 this nonsense question how do you sporle or morle and when I mean ask it I mean you just put this in the context of this language model and then it predicts this text here and sporle and morgal are just totally madeup words so this is really a nonsense question uh the original model without any sort of fine-tuning or alignment will just say you sporle a morle by using a sporle so you ask a nonsense question and you get a nonsense answer on the other hand when we add this prompt to the model that basically tells it when you get a nonsense question say yo be real which is a way of saying you know this is a nonsense instead of answering as if it was a real question then we find that the model is actually already able to tell the difference between real and nonsense questions in fact we need to include just just a few examples in the prompt of what what a nonsense question looks like and what a real question looks like and so the model already had this idea that some of these questions were kind of made up nonsense and some of them were real questions it just wasn't leveraging that understanding in order to distinguish between nonsense and real questions and if we want that kind of behavior we don't need to train the model to tell the difference between sense and nonsense we just need to get it to use the capability it already has in the way that we want and that's what uh sort of the core of this idea of AI alignment is for me so when I talk about AI alignment and when people talk about AI alignment there's actually a few different things that they might mean one of these is just how do we get AI systems to do what we want so instead of just doing you know something uh that is maybe useful maybe not useful maybe it's impressive because it can like play a video game well but it can't actually solve a real world task um or or it has some issue with like robustness and so we can't deploy it so that's one a alignment the one that I'm most focused on though is the second one which uh I'll just skip ahead yeah which is often called intent alignment and this is just about getting the AI system to try to do what we want and this is a little bit of a fuzzy concept still I'm not sure we will ever be able to make it fully precise but intuitively with the example that I gave with gpt3 the issue wasn't that the model didn't know the difference it was that it wasn't trying to tell the difference and to behave differently when it was asked a nonsense question or not and so if we wanted it to identify the nonsense questions the issue there was really the intention of the system as opposed to the abilities of the system there's one other thing that people often mean when they talk about AI alignment and this is a broad definition that is not just about technical research but is more about this big picture question of how can we prevent Ai and particular Advanced AI from leading to disasters that might threaten the human race and uh this includes things that are not just technical research this also includes things like governance and policy Solutions and all sorts of other uh questions but the focus of my talk is is just going to be on the technical stuff even though I think ultimately this is much more of a soci technical problem and I think this these other areas of work are also very important so when we talk about technical approaches to AI alignment there's basically two ways that you can think of making sure that AI systems do what you want or to put it another way making sure that they don't do things that you don't want or bad things um one of these is to just make sure that the AI doesn't have the ability to to do bad things and you could think of this as capability control so just making sure that the AI never could take actions that would lead to disasters on the other hand you can think about getting the AI to have the correct motivations or intentions and I think this is ultimately going to be a more promising approach because I don't think that we can keep AI in a box so to speak forever and prevent it from interacting with the world I think we might want to move in that direction but that's very challenging from a governance point of view and it also will break down when AI systems get smarter and smarter at some point they will be smart enough that they can figure out how to get around various safeguards that we might put in place is the concern that I and other have others have with the capability control approach so this is just coming back to this again I'm going to skip over this in the interest of time so when we talk about technical alignment ah yes there's a couple different ways that we can think about in inuencing the intentions of a system and in this case this was just done using prompting and so right now a lot of work is in fact being done on large language models and aligning these large language models and also other forms of foundation models that might be multimodal and also incorporate uh like Vision or robotics all sorts of other domains so one of the points here is that you can go beyond the kind of prompt that I showed you here which is whoops which is just uh sort of telling you telling the system what you want um and there are various approaches that are looking at more General approaches to prompting like oh sorry like this generic prompt that says let's solve this problem by splitting it into steps and once you provide that uh instruction as a prompt to the AI model and by prompting again I just mean putting it in the context of the language model you can see that this already changes the behavior quite dramatically and can improve performance on many tasks so more recently a very similar idea was used by Google research to uh do really incredible reasoning with these language models so this is one of the main limitations that people often point out with the Deep learning approach and even large uh Foundation models is that they are not very good at doing reasoning however here by sort of including examples of explicit reasoning just using natural language they found that the model was able to imitate that kind of reasoning and use it to get the correct answer so another approach is basically using instructions and in fact fine-tuning so everything I've talked about so far is just working at the level of the prompt and you don't actually need to adjust the parameters of the model however I think that for alignment we are probably going to need to do fine-tuning and actually adjust the parameters of of the model itself and this is because if you think about the training objectives that are used for these large models they really don't have that much to do with what we want the model to do they're just about predicting what happens next and that might be useful in some contexts but a lot of the time we don't just want the model to predict something thing uh we want it to actually you know do the specific behavior we want so if you um had you know somebody who is generating text that was really bad maybe not grammatical or maybe um problematic in some other way like hateful you don't want the model to continue in the same style as that text you want it to do something usually that is more in line with the kind of text that you'd like to generate which is correct and ethical so on and I won't go into too much detail here but the point is that um fine-tuning using either supervised learning or some sort of reinforcement signal is often very useful as well for lining these models so um you might say uh given that we have these techniques like why is it actually going to be hard to get the right intentions into the model and I want to provide a couple of arguments that people have made and sort of of evidence for this actually being challenging to do in full generality so even though we've seen some really remarkable successes here and dramatic improvements using these alignment techniques over what is possible uh if you don't uh do any sort of fine-tuning or design the prompts carefully I think there's still a long way to go and I am concerned that the methods that we have right now are ultimately going to be insufficient and so more research will be so one of the reasons and sorry so I think this is the first time I'm talking about specification and when we're talking about intent alignment and Alignment this is really uh the the core thing that we're doing when we provide these prompts or when we fine-tune the model I think of this as specifying what the behavior we want is so we have to communicate to the AI system what we want it to do right and that's what we saw all the way in the first example with the yoi real thing and in all these other examples the core thing that's missing after you have trained this model even though it's very intelligent it doesn't understand what you want it to do and you need to communicate that to it somehow and that is what I call the specification problem and this is actually the focus of my research as well so Stuart Russell who is uh the author of this book about controlling Advanced AI systems likes to use this example of King Midas so this is a myth from I think Greek myth of a king who wanted everything he touched to turn into gold and then of course that wish was granted in the most literal sense so that he was unable to eat and when he touched his daughter she also turned into gold so even though what he said was actually you know came to pass it wasn't what he meant and so it can be hard just in the first place to say precisely what we mean and if we get it a little bit wrong then we might end up with something that is not what we intend so uh here's a more like real world or I should just say realistic example so this is came from actually training a reinforcement learning system on a video game and the goal of this video game is to get the boat to race around the track uh and you know win the race against the other boats however the reward function in this game the boat gets uh bonus points that you can see down here for collecting these green powerups and so instead of actually completing the racetrack this boat ended up just going in circles collecting more and more points from these powerups and this is sort of just a a uh a real example of the kind of PR instantiation problem that I mentioned where excuse me where the reward function that was used was just not quite what was actually intended and so we get this completely different behavior from what we actually want finally the last example that I have is adversarial examples so people often think of adversarial examples as a robustness problem where there's something wrong with the way the system is functioning but I think it's very uh Illuminating to view them instead as a specification problem where even on this simple task of image classification we didn't specify enough information about how we want the AI system to make its predictions and that means that it's able to find genuine features that actually do a good job of predicting the class labels but that are not the same features that humans would use and so while it can generalize very well on the same training distribution it won't generalize out of distribution and so when we think about outof distribution generalization which is really fundamental to building AI systems that work in the real world we have to specify to the system somehow how should behave when it sees data that is very different from what it seen that is out of distribution so the solution that I think has been proposed by people in the alignment Community uh is to learn the specification so the issue in all of these cases before was that we sort of just wrote something down by hand as the specification and it was too simple and it didn't capture everything that we cared about or trade it off in the right way so instead you might think let's learn the specification from Human data and I think this is a very promising approach in some sense it looks like almost the only viable approach to this problem in my mind and so I've listed here a few different examples of sort of foundational papers talking about different approaches to learning specifications the one that I'm going to focus on is the one that I'm an author on which is working on reward modeling and this is also the most recent and I would say the most popular Paradigm right now for learning specifications and the basic idea of reward modeling is just to learn a reward function from Human feedback so typically in reinforcement learning we think of the reward as coming from the environment and we might program a reward function that just sort of uh looks at the state of the environment and provides a reward to the agent programmatically however as we've seen that approach can go wrong and so instead what we would like ideally is something more like a human in the loop giving reward to the a now that's very expensive however and might be very difficult when the system is operating at a very quick time scale that's too fast for humans to keep up so what we can do instead is we can train a reward model to stand in for the Human by getting it to imitate the kind of feedback that the human would give so now instead of just having the environment and the agent we also have the reward model and the human user and what we do is we take trajectories of the the agent's behavior in the environment show them to the user and then they give some feedback about which trajectories are good and which trajectories are bad which behaviors are good and bad basically and we use that to figure out which ones should be rewarded and then we have this machine learning model which is the reward model that provides the reward to the agent so there's been uh I'll skip over this for now there's a lot of challenges uh that come with this approach and a lot of approaches to dealing with challenges and this is one of the things that we detail in this research agenda that I showed you back here so uh you can see that like the amount of feedback might might be an issue because we need a lot of data usually to train a good model and there's a lot of approaches that you can think of to doing that like um leveraging existing data that uh instead of just having the human give all of the feedback online however giving online feedback has a big Advantage as well in that the human can look at the most recent behavior of the model and respond to that and that kind of feedback can be more informative so in terms of the work that's being done in this area uh my co-author and the the first author on that agenda Yan has done a lot of work on sort of improving reward modeling and doing things along the lines of exploring these different approaches in order to come up with algorithms for learning reward models that work better and that's I just showed a few examples in this category so here's an example where instead of saying let's just have the human give feedback comparing behaviors that the agent is doing we can also have the human demonstrate some behavior and then the agent can start off by imitating the human and then only later get the human preferences uh to distinguish between those uh behaviors so I think it's worth now actually me skipping back a tiny bit to this slide and talking about why we can't just do imitation and the reason is that ultimately we hope to go beyond what humans can do themselves and to be able to teach AI systems to do tasks that we ourselves cannot do so for instance in Alpha go we were able to teach a go system to perform at a superhuman level and it did things that humans never would have decid chosen to do like this famous move 37 now that was made possible by us having a very clear reward function already because it was a game and we could just say the reward is to win the game on the other hand uh as soon as you allow for the possibility that the AI can come up with novel solutions that humans wouldn't do then you also allow for the possibility of Novel Solutions like this circling boat that are actually not what we want and so ultimately we want to expand the space of behaviors that we can train AI systems to do the kind of task that we can train them to do but we have to be very careful that as we're doing this we're only getting these kinds of desirable novel Solutions and not the undesirable ones and that comes down to figuring out the right reward function or the right way to learn the reward function and use that learned reward function to train the agent so the other uh line of research on reward modeling that I think is important to mention here is applying it to tune these Foundation models and so a lot of the work right now on aligning Foundation models is not just using prompting or using fine-tuning with supervised learning a lot of it uses reinforcement learning from Human feedback which is another term that basically refers to the same thing reward learning now uh I mentioned those lines of work because I think they're very important to know about and there's a lot they're very active what I'm working on with my research group is more focused however on ways that this approach can fail and in general ways that specification can fail and I focus on this because I think it's a very difficult problem and I think we need to understand all of the ways that it can go wrong so that we don't put undue trust in an approach that has worked so far for simpler tasks and simp models I think as we expand the scope of things that we try to do with AI systems and as the systems get bigger and smarter we there's the possibility that approaches that have worked in the past will break down and so I'm very concerned that even though we have seen some promising initial results with reward modeling um I'm concerned that this approach might in the long run fail if we don't make sure that we anticipate all of these failures so I'm going to do in the remaining about 15 minutes of my talk is tell you about these three different ways that uh we can have a specification failure that I have studied in these three papers so these two are with collaborators including some of my current students and this last one is work that I did um during my PhD and also during my internship at Deep Mind with Yan so at the same time that we're writing this agenda about reward modeling so again I think uh this this other work is really promising really exciting one of the reasons that I'm focused on the failures is because I think there's already so much work being done in these other areas and I kind of want to you know set myself apart and I also think actually despite all of this work on sort of improving the approach and applying the approach we still don't have anybody who is focusing on figuring out is this approach actually going to work is it going to work as we scale up and so that's kind of one of the main focuses of my research right now so yeah uh let's jump right into talking about these papers now so this is uh a very recent work that I did with uh students from Oxford and Mila and one of my students at Cambridge and the idea here is basically we're asking this question of when is it okay to optimize a proxy reward function so the issue that we saw here with the boat race is that the reward function that was programmed was actually only a proxy for what we really cared about and in general when we are programming reward functions uh we don't know it's it's very difficult or it can be impossible to get everything that we care about into the reward function so this is just a video game but in the real world there's all sorts of impacts that an AI system could have and if we don't account for all of those impacts then um if we don't include them in the reward function is what I mean by account for them then the AI system by default won't care about them and so if it's able to do something to improve its performance like in this case even though the boat crashes into the wall and catches on fire um it still gets more points you know we might say oh actually we don't want the boat to catch on fire but that wasn't included in the reward function and so long as we don't include things like that in the reward function the reinforcement learning system just won't care about them so the question here is when we have a proxy like this that is includes some of what we want but not all of what we want or maybe it includes um all of what we want but it doesn't appropriately trade off the different the value of different things uh are we going to get acceptable behavior or not and when we learn a reward function I think we can only ever expect to learn a proxy so if we had a human giving the reward that might be you know representative of the human's real preferences or intentions but when we use human data to learn a reward model we know that the model is never going to be perfect so there's always going to be some sort of flaws in the model and the question is is the model good enough or are those flaws too severe and so we will see this kind of perverse instantiation or reward gaming is the term that we use for it so reward gaming uh has this signature uh appears in this signature way that you can see in these plots so in Black we have the true reward and in green here we have the reward of the proxy which in this case was a reward model and you can see most clearly I think on this leftmost plot so these are um plots from one of those papers that I showed earlier in the slides training on Atari video games using reward modeling and we just show the the Returns on the y- axis so higher is better and as we train the agent both the proxy reward and the ground truth reward are going up at the beginning of training but at some point there be there's a sudden transition and a and a the proxy reward continues to increase and in fact in this case it jumps up Suddenly while the real reward drops and so this is what the sort of characteris uh reward gaming learning curves you your agent is exploiting the the difference between the reward model in the real reward and has found a way to maximize the reward model that doesn't actually give you good reward according to the real reward function that you actually care about so one of the main contributions of our paper is just to Define what it means for two reward functions to be gameable so when could such a thing happen in principle and for this to happen we say we need there to exist a pair of policies such that the returns are lower according to one reward function for the first policy but higher according to the second reward function and that means that actually if uh J2 was the proxy here J then it would be happy to switch from following Pi Prime to Pi because that would improve the proxy reward uh R2 or J2 the proxy returns but doing that would also decrease the true reward and so that would give us this kind of a this kind of a um a learning curve now just because there exists a pair of policies that look like this doesn't mean it's guaranteed to happen but it does mean that it can happen and so that's basically what we wanted to do is just Define sort of what it means that these reward functions could ever have this happen when we optimize the proxy and it turns out this what this amounts to is saying if you want to find an ungame proxy you're allowed to uh replace strict inequality in the orderings over policies with equality so if you think about reward functions actually can order all of the policies according to their returns which we represent by J here and in that ordering um if you had the true reward function saying Pi Prime is better than pi and you had the proxy reward function saying uh they're equally good that actually wouldn't be a problem because then when you optimize the reward function there's no pressure to switch to PI from PI Prime and so it's only this case where there's actually an optimization pressure as we call it to switch to a policy that is actually worse according to the real reward function that we say that these reward functions are gameable and yeah so I'll move on now um so the results that we had in this paper uh let me see how much time I have left yeah good I'm I'm doing all right for time so I can tell you a little bit about our results as well so first we just had this definition then what we want to do is explore the consequences of this definition and to see you know how because the ultimate question motivating This research was how good does the proxy need to be in order for us to to for it to be safe to optimize and by safe we mean that we won't ever see this kind of effect where it looks like we're doing better according to the proxy but in reality bad things are happen maybe are happening maybe even disastrous things are happening and it turns out that this relationship is entirely trivial if we consider all policies so there's actually no way to have a proxy that is ungame unless it's essentially exactly the same as the real reward function so this is remarkable because it's it says that actually this approach of learning a proxy reward function might just be doomed in some sense now I don't believe that that's actually true but I do think that our result uh should you know give some pause and and I'll talk more about the the consequences of of our results at the end of um well in a couple minutes so one of the implications here is that you might need to restrict the policy set in order to get non-trivial example examples of ungame ability and so in particular uh so that's what we look at in the next couple of results here so I should mention as well that this paper is actually entirely theoretical we're just focused on as it says in the title defining and characterizing this notion of game ability um another thing that we're working on is actually looking at whether this happens in practice or what factors influence whether or not you will actually get reward gaming because as I mentioned just because the two reward functions are gameable doesn't mean that you will actually see gaming and excuse me so once we restrict the policy set and we say maybe we're only going to consider a finite set of possible policies it turns out that there are always UNG gameable pairs of reward functions um so these are kind of our our main results here and I think I'll focus on those uh for now discussing this work so I just want to give you a little bit of a flavor of the idea behind these proofs so when you look at the returns of a policy this is actually linear in What's called the visit count so just how many times does the agent visit each state and each action and of course it's discounted as always by gamma in reinforcement learning and so the returns of a policy can just be given by this inner product between the reward function and the visit counts and so the implication turns out to be that if you have anywhere in your policy space an open ball so basically if you have volume somewhere in the space of policies you're considering then you can always take some visit counts that you get from two different policies that you want to set equal project them in some sense inside that open ball and then within that open ball the only way to set these equal is to flatten the the entire reward function so that any visit counts have equal returns so yeah so you can basically think of this operation of um finding a a proxy as based on this thing that I show here you replace some inequality with equality so you have two policies or in this case the the visit counts ofes that start out proxy that is not gameable but if you do this within a continuous space it turns out that that just automatically sets all of the visit counts equal on the other hand if we only have a finite set of policies then we can think of the reward function uh as this plane that we sort of tilt around in order to try and set two policies equal and there's a way of doing that when you have a finite set of policies that is guaranteed to uh to to exist and give you something where you set two policies equal but you don't set all the policies equal so a non-trivial example of ungame ability so uh just to say a little bit about what we've concluded from this paper so I think first of all this is still really um like preliminary in a sense that I think there's a lot more work to be done on expanding this definition and in General on this question of game ability but the first thing I think to say is that maybe actually optimizing proxy rewards is not is not actually the right approach that we should be taking and maybe we actually need to think more about Alternatives like imitation even though it has this limitation where we can't exceed Human Performance or constraints maybe we need to incorporate con incorporate constraints but maybe an even more important takeaway is to say optimization optimizing a proxy we should think about that always as a heuristic so anytime we're optimizing something um we shouldn't think that the goal is just to optimize it as well as possible in the same way that like optimizing for training performance is as well as possible can be a really bad idea because you get overfitting in general I think we can never take the specification of the goal so seriously that we think that we should optimize it instead we want to optimize it a little bit right like we want to optimize it to this point and no further and so there's this challenge of knowing where that point is like how much can you trust this reward function how much can you optimize against it before it starts to peel apart from what you actually want and once you think about it that way then this this approach to reward modeling should be viewed not as learning a reward function that can be optimized um you know safely but rather learning a reward function that can be optimized carefully and if we optimize it carefully then we can get a good policy out so the ultimate uh product that we're trying to produce here not be the reward function but only a policy all right so I'll talk about this other paper uh now which is about goal miss generalization in deep reinforcement learning so this is a paper that my student Laro and these collaborators started before uh before he actually joined my group but um since he joined the group group we did a lot more work to polish it up and submit it to icml and so it was accepted at this icml and so previously I talked about the idea that you can your specification can fail if you don't have the right reward function but it turns out that you can also have an agent that has the wrong intention even if you have the perfect reward function and that's the main point of this work so we we use this example of this coin run environment which is a lot like Super Mario Brothers the goal of this agent is to get to the end of the level where there's a coin and in training in fact the coin is always at the end of the level and we give it a reward of one when it reaches the coin but the agent doesn't know if it's getting rewarded for reaching the coin or reaching the end of the level so what happens when we put the coin somewhere else is that the agent goes to the end of the level and ignores the coin and so this is a kind of of robustness issue that is caused by the agent having the wrong intentions and in fact this happened again even though we trained it with the right reward function so the the the point here is that having the right reward function is actually not enough to get the right behavior and in fact it's not even enough to get the agent to try and do what you want so contrast this this kind of mode where the agent is goal directed and the policy is capable of achieving its goal in this case of reaching the end of the level with a more typical kind of failure which would be a capability failure where the agent really isn't capable of do doing anything sensible and so when we see that an agent uh or or an AI system isn't doing what we want a lot of the time we might assume that it's because of this kind of a failure but in fact um it could be that it's doing the wrong thing but it's doing it competently and that could be a bigger issue because then it could be actively pursuing some goal that is different from what we want and so one of the main things we did in this work is to formalize the difference between these two kinds of uh of failures and basically we say if the agent is still behaving as an agent but one with the wrong goal then we would say it's goal Mis generalization whereas if it's behaving less like an agent and more like a device so just a sort of arbitrary policy and you can look at this paper if you want to read more about where we get this this definition then we would say it's not goal miss generalization it's just a regular sort of capability robustness failure great um I am close to out of time but I will go through this work really quickly so this is um the point of this work so actually I'll skip to sort of my later conclusions and then I'll come back and show you something there so I've talked to you about these two sources of specification failure and I'm about to say a little bit about this one as well and I just want to highlight what is causing the failure here what is causing the agent to have the wrong intention and in the first case it's very simple it's just that we didn't the designer didn't provide a good reward function so the reward function didn't actually Express what we want and as long as that's the case we might have to worry about reward gaming that's one of the conclusions from that paper that I talked about on the other hand even when you have the right reward function the agent can implicitly learn a different reward function that is not the one that you intended and that's the source of the failure in this goal miss generalization is that the agent learned to get to the end of the level instead of to pick up the coin and it learned that because the coin was always at the end of the level in training although we have experiments as well where if the coin is almost always at the end of the level you can still get a similar kind of failure in practice so the last source of failure that I'll just go into really briefly before wrapping up is nothing to do with like the reward function I'd like to describe this instead as about the means that the agent uses to optimize the reward function so I've already talked about the idea that optimizing something really hard is maybe not a good idea but this is sort of uh something like that but a little bit more subtle where we're saying it's not only an issue of how much you are trying to optimize that thing it's also a question of how you are trying to optimize it and in many cases I think we want to optimize something but we don't want to optimize it using certain Paths of influence so the example that I like to use for this is content recommendation so in content recommendation you want to get the content to match the user's interests and and the kind of solution that we expect I think when we phrase the problem that way is that the users interests don't change that much maybe they change you know randomly or just uh as a result of that person becoming interested in different things naturally or organically but the content that we provide is going to match the preferences more and more on the other hand when I say get the content to match the user interest we can also do that by changing the user's interest to match the content so just saying these two things should match doesn't say do I want to change the content I'm providing or do I want to actually influence the user and change what they are interested in and um so you can think of this as basically you're recommending some content and the user either clicks on that content or not and the reward is do they click on it and just finding good content that matches the user right now is fine but if you have if you also optimize over the influence that you have on the user then you might be more likely to come up with solutions that would have uh a harmful outcome for society like changing users interests to be more interested in conspiracy theories or radical politics or other things that might keep them on the platform for longer and I'll say one more thing about this quickly which is that the one of the really important things about this work is that not trying to influence something is not the same thing as trying not to influence it and what we're focused on is not trying to influence so you might indeed change somebody's interests when you interact with them and that's okay so long as you aren't trying to influence them so again this gets at the intention of the AI system and unfortunately I don't have time to walk through these experiments but you can take a look at the paper or or follow up with me that so just to conclude I've argued that a key challenge for AI systems is uh to get them the to have the right intentions or motivations and that this is ultimately really important in order to get the kind of behavior we want so going back to these definitions of AI alignment as getting the right Behavior or getting an AI system that tries to behave in the correct way I think the second one is in some sense more important because I think whether or not we get the right behavior from an advanced AI system that is very capable is mostly going to come down to whether or not it understands what we want and is trying to do what we want and it's my opinion that we need to learn these things and that manually specifying um the intentions or the motivation or the goal for an agent is not going to work it's not going to be a scalable approach so somehow we need to figure out how to leverage machine learning for learning the learning the goal that the AI is supposed to optimize now once we look at it that way uh it's clear that human feedback is going to become a key bottleneck for doing this so we're going to have more and more unsupervised data and more and more computational resources but we only have a relatively limited amount of human labor that can go into building these systems and to telling them what we want them to do so and then finally I talked about these three different challenges and some of my work on identifying these challenges and I'm very interested in you know going Beyond this to start addressing these challenges as well and finally just a reminder I'm looking for post talk so if any of this was interesting to you or if you know somebody who would be interested please put us in touch and thank you so much for the invitation and for all of your attention and really looking forward to questions thank you so much David it was really a nice talk I really enjoyed it and we received quite many questions actually um they lie on a very wide spectrum both on the um General a alignment side um language models Etc and also on the other side of very technical and specific questions on reward modeling so maybe I'll just alternate between these two types of questions sounds great okay yeah um okay so so the first question is about um the claim that you made about um the alignment of gpt3 is the next Alex net moment and what advice would you give to an ml student wanting to Pivot to AI research at the moment yeah um that's a great question um so I think like one thing that H where to begin so I think one thing is sort of going back to the analogy with deep learning I think it's good to think about you know not just the next deadline or the next you know year or two but think about like five years 10 years from now and really try and do research that is going to be impactful on that kind of time scale um so that's that's I think can be hard to do given the the culture of the field is very um short-term focused and deadlines driven but I think ultimately this is actually good advice almost no matter what kind of research you're doing but it's especially important for alignment I think because for alignment we really want solutions that are going to scale and so you can do a lot of work and this is important work on getting AI systems to behave the way you want right now like in some actual application um and I think that work is very valuable especially when it's very focused on a particular application and on ensuring that the system behaves reliably and is not harmful to its users or to society but in the long run I think there are hard technical challenges like the ones that I mentioned that we're only beginning to to Grapple with and if we want to be able to continue to get the right Behavior out of AI systems in the future we need to focus on an identifying and addressing these fundamental challenges so these are challenges you know like these three that I mentioned and also just about the limited quantity of human feedback that is going to increasingly and increasingly be a bottleneck um so that's that's the first part of my answer let me think if I want to say anything more ah okay another thing I think is I think it's very important to think about these problems you know think think for yourself and think creatively and read a lot about what other people have said and thought um and and written about these problems but ultimately um I think everyone who is serious about working on the alignment problem should come up with their own views and their own model for like how the future of AI will look and how we will be able to to shape it or what the challenges will be so there's a lot of a lot of stuff has been written um much of it is not actually in academic papers but is on forums like less wrong um or is on people's individual blogs um and a lot of that content is like I think it's important to be aware of um and I think it's it's a bit of a challenge but to to balance keeping track of this thought that has sort of happened informally or less formally and also keeping track of developments in the machine Learning Community but I think to be really successful in this field you kind of want to be tracking both of these things and there's two newsletters that I think are really great for for doing this which are the uh alignment newsletter by rohen Sha and the Machine learning safety newsletter by Daniel Hendrick yeah um so I'll leave it at that so that we can take more questions sure thanks for sharing this rohen was also one of our speakers at a previous talk yeah yeah and a second question would be um you mentioned about the misspecification of goals like the coin run example do you think some um reward learning schemes like preference comparisons Etc should will be able to solve these or you or do you think um these types of human feedback cannot even solve this type of problem yeah um I guess I'm not actually sure how to interpret that question um so if it's about coin run in particular the kind of failure here is is quite easy to solve once we know exactly what's going wrong we can just train the agent on levels where the coin is not at the end of the level and as soon as we do that as long as it's like more than 1% of the levels have the coin in some random location instead of at the end of the level then the agent learns to find the coin coin most of the time so the the it's it's more of an issue if you think about um you know this is we chose this example in part uh because it so clearly illustrates the problem but in reality this might happen in a subtle way that you you can't even detect um so yeah so I think the question about whether or not human preferences could solve this kind of problem more generally is a good one um I think you know it remains to be seen right it's it's research um so I'm like cautiously optimistic about this direction of work but I tend to be somewhat pessimistic overall um because I think it's actually just very difficult technical problems that we're working on and so I think it will take long time to to make progress on them um or I guess not to make any progress but to like get close to like solving them uh the important thing I think for so so if we think about the actual training environment that we used in coin run where the coin was always at the end of the level then I think there is a version of learning from Human preferences or reward modeling that would be able to solve this but importantly you would have to get human uh human input not only on the states that the agent actually encounters but also on hypothetical states that are different from what it's actually experienced because it really needs to know is it getting reward from the coin or from the end of the level and if the coin is always at the end of the level there's just no way to disentangle them and so one of the papers that I have in the slides but I didn't actually talk about much I'll just see if I can find it really quick yeah so this this paper um learning human objectives by evaluating hypothetical behavior is actually doing something like that where they say let's let the agent you know sort of Imagine things that might happen and then get human feedback on those and so in that case you could have the agent say well what if the coin wasn't at the end of the level I've never seen that but I can imagine it and then the human would say okay in that case it's good if you still go to the coin instead of skipping over it and going to the end of the level um so I think that's a way that I can imagine addressing this kind of issue with reward modeling I hope that got at like the the spirit of the question um otherwise you know happy to to clarify if uh if I just totally Miss missed or misinterpreted it I think you interpreted it correctly yeah okay thanks for that and the third question is on the more uh General side so someone asked how should AI research be more integrated with social sciences and philosophical research just trying to improve so that it can improve the probability that AI development is positive yeah um that is a that's a big question um excuse me so I have a few a few thoughts about that so I guess where to begin again um one thing would be when we talk about learning from Human preferences there's this assumption kind of implicit assumption that the human knows what they want and I think that's like a quite a reasonable and safe assumption but I think it's not strictly speaking true like there's a difference I think between what is called often people's revealed preferences so what they actually choose when presented with different options versus what they actually value or what they would prefer if they were more reflective about their decisions and this is something that you can see with issues like addiction or um maybe even uh in the context of content recommendation or social media like clickbait like you might click on something uh that is sort of interesting or appealing to you right now but if you thought about it more you you'd be like ah I actually don't need to learn about this it's just some celebrity gossip it's not really helping me achieve my goals realize my values be you know H have a happy fulfilling life overall um so I think we do need to be careful about how we involve humans and human decision-making in training AI systems and that's something that I think there's a lot of work for social scientists to do um I guess another part of this would [Music] be I think we need to um study other fields and other not even just academic Fields but just like communities of people and how how they make decisions how they manage risk and how they manage to um coordinate and cooperate with each other so I think AI development so besides the technical challenges and and I alluded to this at the beginning of the talk besides the technical challenges there will be very significant social um and like political geopolitical challenges I think with AI development and one of the things that concerns me the most is the idea that AI developers will just race forward and push for AI progress at the expense of alignment and safety and I think there will be strong structural incentives for different actors to do that and this could be for economic reasons it could also be for geopolitical reasons and so I think we need to understand how can we um both like uh reduce those incentives or sort of create less pressure for Rapid development and get people to appropriately prioritize safety reliability alignment um and then also look at sort of the culture and the cultural side of this so the culture of teams that are developing AI how can we get people to think seriously um about this and because right now a lot of people who work on AI uh sort of come out of a research mindset and um have have this attitude that like everything that they're doing is sort of just you know science or almost like a Sandbox for them to play in to to see you know cool results and understand things but these systems are having really big impact on the real world and that impact is only going to increase and so I think there's a kind of responsibility that everybody in the entire machine learning research Community has and not just people working on alignment or people focusing on these social problems everybody needs to have some appreciation for um what we are doing is is going to have a really big impact and we need to think about those impacts and especially when you look at like an organization that is doing AI development I think it's it's um so so there's a paper from uh Tom Dietrich called I forget the title but he talks about the idea of Highly reliable organizations so we often think about like reliable AI system as about you know how do we make the AI itself reliable but I think an even bigger part of the picture is how do we make sure that the people and the organization is set up to treat this you know with the seriousness and gravity that it deserves um and we see examples of Industries or fields that already have that kind of mentality when they're dealing with things like nuclear um nuclear physics nuclear weapons nuclear energy um or also some types of biological research and I think we're going to need to inate some of the lessons from those fields into the way that we do AI development um I think there's also so when we when we talk about like uh cooperation and incentives I think this naturally um brings in like the field of economics and in particular mechanism design which is a subfield of Game Theory that's often called like inverse Game Theory so this is about how do we set the rules of the game so that when the players play the game it results in a good outcome and good outcome here usually means like good for everyone good for society good on like a a social scale and I think um that's a an an area that I think is very interesting to think about in terms of yeah having having more Cooperative AI development and I guess I'll leave it at that again you know in the interest of time sure thanks for elaborating so much on this and we have our next technical question on reward learning so someone raised the question that in reward learning there are many different sources or modalities of human feedback like um preference comparisons Etc and he or she is interested in your vision about how to use these more efficiently and also how do we scale scale up the human supervision or are there aligning alignment approaches yeah H that's another great question um so one of the basic results that I think is um I wouldn't say it's super well established but it's it's it makes sense and I it it's been fairly consistent with what we've seen so far in the research on this is that both demonstrations and preference comparisons are are are really important and a lot of the time you can't get to the point where the AI system is doing anything sensible in the first place if it hasn't been trained on some amount of human demonstrations excuse me so um there's for uh excuse me for this this paper this is what they did they pre-trained on human demonstrations and then they fine-tuned with reward preferences and found that that was much more effective and some of the empirical work that I alluded to that we're doing in my group right now on reward hacking and reward modeling sorry I said reward hacking that's another word for reward gaming um in some of this empirical work we're finding something similar where um you know typically in reinforcement learning agents start out doing kind of random things that aren't very sensible and it can take a lot of work and including potentially a lot of human feedback in order to get them to a place where they are doing something sensible in the first place so I think there's a there's pretty pretty clear and established role for demonstrations and preference comparisons in that sense um in terms of scaling this uh to other kinds of feedback so one of the coolest works that I've seen in this area is called okay again I don't remember the title exactly but the idea is called reward sketches and what they got people to do is to just watch an entire episode of the agent's behavior and sort of Trace out how high or low the reward was at every time step and of course when you do that you're not going to get like very precise numbers but you will get a a good sense of like oh it was doing good stuff here and then this was okay and then that was really bad and so on um and that gives you just a lot more data in a sense because a human can draw a line like that in a couple seconds but then you get like a data point for every single time step along that trajectory um I think one of the things that is really interesting to me here that I think there's only we're only just beginning to see some work on and I'm not sure if it will pan out but I think it would be very powerful if it does is to use natural language feedback and so there's been a lot of work on incorporating natur language into reinforcement learning somehow um usually though it's as context and not as feedback so this is the difference between like something being an input and something being a Target basically and so you can have the reward function uh depend on some natural language instruction and that becomes very similar actually to another uh another slide here so the stuff that I was talking about about like fine-tuning with instructions um but again that's still using the natural language as as input instead of as feedback but if you think about how we how we teach other people to do something or or what we want or some New Concept it's just so much natural language and this is so much richer than reward as a kind of feedback so you can imagine like trying to learn you know how to do calculus by just like moving your pen and someone either like rewards you or not so like maybe they you know they shock you when you when you draw the wrong symbol and they give you some treat if you draw the right symbol or something it' basically be impossible to learn calculus that way but a lot of what we're doing when we train AI systems is sort of is sort of like that and so I think given that we have these systems that seem to have a good understanding of natural language it might be possible to to leverage that to interpret more Rich human feedback like oh you know you made a mistake on this line because you aren't actually allowed to change the order of integration in this case because you know Etc um and have a system actually understand that and adapt Its Behavior appropriately so that's something that I think is is very promising um and uh at the same time I I want to just always like War and advise caution because I think it's it's hard for me to to imagine that we would have great confidence that the system actually understands natural language so when we do something like reward feedback there's a shared semantics of the reward um between the human and the agent sort of by default because it's just a number and humans and AI systems both understand numbers and we getting these AI systems that like understand language quite well but I think there's still compelling evidence that they don't understand it the same way that humans do and there's still discrepancies and I actually I know a lot of people um these days think that this is a problem that's going to go away as we scale these systems up but I'm concerned that it won't ever fully go away and so there's going to be a bit of a um I don't know if you have this expression actually but like chicken and problem like you can't solve this problem without solving this problem but you can't solve this problem without solving that problem um I think there's going to be a little bit of that Dynamic with using language for uh as something like a reward signal because if the AI doesn't understand the language perfectly then your feedback might be misinterpreted and you might end up again with like uh the wrong reward function or encouraging Behavior that you want to discourage yeah that's very interesting thanks for sharing this and we have our final question um from the audience um how do you evaluate the possibility of AI working out a better goal setting agenda than humans would do in a similar situation uh I I don't uh understand this question unfortunately um I okay so my personal interpretation is like under the same situation do you think it is possible that an AI will set a better goal than humans oh okay yeah um yeah uh I mean that's a that's another big question um I think like the short answer is yes absolutely of course because humans make mistakes all the time and and not only do humans make mistakes but are also humans that are just you know uh don't have good intentions in the first place so um yeah uh but I think I think like I'm very skeptical of the idea that like we should we should pursue that direction right now of like AI systems defining the goal instead of humans defining the goal um to me that just seems like uh a recipe for for disaster to some extent I think we can use AI systems to help us clarify our goals and um and I think that needs to be done very carefully but I think for the most part we should always think about the goal as being coming coming from the human somehow for for the time being um and yeah so so I think like at the same time there's there are things that you could do like maybe in in very specific or narrow context it makes sense to have the AI uh Define the goal and then a human like uh look at that and say yes that is a good goal so I talked about mechanism design before and so defining the rules of the game you can imagine building an AI system that says oh here's the rules that I think we should use for let's say self-driving cars in order to have a good transportation system based on self-driving cars and um I think you know that's the sort of thing where in some sense that's defining the goal that's defining you know the rules but maybe also like how um you know what what the what the goals will be of the different the different driving algorithms in that context um and I think that could be you know a sensible thing to do I just think we have to be very careful about it and ideally we want to make sure that humans are still involved in looking at that uh description of the goal and saying yes I agree with that that makes sense to me I understand it and I think it will work