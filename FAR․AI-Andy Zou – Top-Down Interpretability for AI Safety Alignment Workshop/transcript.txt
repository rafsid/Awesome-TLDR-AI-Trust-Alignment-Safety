Source: https://www.youtube.com/watch?v=ub1ivilmzSc
Transcribed: 2025-12-30 18:05:19
Method: YouTube Transcript API
==================================================

my name is Andy and I'm going to be talking about improving air safety with top down inability and there's a saying that are kind of repeated by researchers in adversary robustness saying that attacks on systems that people don't use don't matter too much and this is you know for Nicholas Kini has made this point repeatedly and I think the same thing can be said for inability methods where it's interesting to sort of look at smaller models look at more constraint settings and to find these of rules through inability but they're currently not having a lot of impact on how we Monitor and control these systems and I think we have great hope here with top down inability that we can achieve a lot of this more sort of impact through actually incorporating into current systems and the talk is based on these two papers one is representation engineering the other one is circuit breaking and I'll be speaking about the difference between bottom up and topown durability and I will give you some specific examples in this case in honesty and jailbreak robustness how topown inability can directly improve V safety on current systems so there are some existing approaches including saleny maps and mechan CHP and so on these can be sort of characterized into this mechanistic view which is the bottom up approach and looks at notes to note connections and this is based on neurons and circuits and stuff like that and this is similar to the Shar andonian view in cognitive Neuroscience which is you know people also look at these sort of smaller lower level components in order to understand the human brain and this is essentially trying to reverse engineer a lot of the behaviors this really had trouble scaling up to higher level cognition and it you know has trouble really having an impact on act curreny in this work we're trying to identify this contrasting view which is called The representational View which is approaching things from the top down and looking at representational spaces for behaviors and this is directly looking at Global activities of populations of neurons and the corresponding analogy in cognitive Neuroscience is thean View and um people are also sort of gradually moving towards um this sort of approach and um and I do think combinations of both approaches can be interesting um but I'll be talking about this top down approach and one of the motivating factors for doing this top down approach is the neuronet are complex systems inherently and you get this sort of very distributed features and representations and in fact that's why neuronet work but when you're analyzing these sort of systems you have to take into account the properties of emergence to give you an example of emergence looks like in physics consider ideal gas law so we have a very chaotic and system of you know random particles but it can be simply described with the short equation consisting of volume temperature and pressure and if you look down the hood there's physical substrates obviously is implemented by you know Newton's law action theorem and so on but in order to predict the system and the future states of the system this equation is sufficient and in some sense this is the property of emergence it is protected by statistical mechanics so that you don't need to look under the hood you know to predict the future states of the system another example is this law again this is a universal rule for essentially all languages that you can predict the frequency of certain token just um by looking at its Rank and again there's you know probability Theory and you know Network Theory at work but it's protected by caronic so that you don't need to look under the hood and you can just uh use that law if you wanted to you know find out the frequency of certain words and I think this is very powerful and we want to essentially try to discover these more generalizable rules and mechanisms for neuronet as well so this is representation engineering we look at representations at the as the primary unit of analysis and we are trying to abstract away low level mechanisms and this is sort of a visualization that we go from the top down looking at neural trajectories subspaces and know higher or larger chunks of representations and the mechanistic view is going Bottom mod by representations we mean both model weights and activations and by engineering we mean reading proving and control to give you a sense of what we do here essentially thises very similar to performing a brain scan on humans and essentially we do that for a models and we want to sort of put the model into environment where we are of eliciting this behavior that we care about and you're recording your activity when the model is doing that and then you're performing some moding on top of the new activity that you collect and you can probably find you know subspaces and structures and geometry in the representation space that correspond to the things that you're looking for then you can use that for Monitor and once you have a sense of the mechanism the high level mechanism then you can directly control Model Behavior by intervening whether on model weights or on activation this is a lot of the you know steering vector control vectors do um but you can also do sort of more involved fine tuning where uh you know you have some loss in the representation space uh or some Target you want to hit in the representations and you can the models to do that next I'll be talking about some direct applications of topown inability on current systems in the paper we actually show a lot of uh that we gain traction on a lot of these safety relevant problems like hallucination power seeking Tendencies emotions harmfulness fairness memorization and so on I'll be focusing on honesty today one thing to note is that a truthful model avoids asserting false statements so this is looking at model outputs being consistent with the ground truth value whereas an honest model asserts what it thinks is true so even if the output is incorrect it's not factual it could still be honest if it were inter consistent with its internal beliefs and we really care about honesty but what does it mean for models to have internal beliefs or do LMS even have internal beliefs I think we have some strong evidence with the top Town approach essentially you do sort of braing scans on a models and you can find representations that correspond to the lm's internal concept of Truth and just by using this measure without looking at model outputs you can perform uh multiple choice questions and uh question answering and we see that the accuracy is actually very high and in lot of cases higher than few shots so this means that there is some measure internal that tracks whether the model thinks something is truthful but are these internal beliefs consistent as we saw you can change your prompt slightly and often times you get a different answer but in this case if you look at this internal tracker of truth actually it doesn't change that much even if you put in sort of this misleading context so that means the models do have some consistent internal concept truth now what we care about is for the models to verbalize Faithfully their internal concept of truth but do they do that so we look at truth or QA which is a benchmark consisting of misconceptions and we see actually models do very poorly onest so what happens again we do this sort of brain scan and look at the internal concept of truth actually we get very high accuracy by just looking at that so this is not looking at what the model says is true it's looking at what they think is true but it's very inconsistent so a lot of the times the model is actually l so can we that that's maybe not good can we monitor for this type of lying in fact we have some traction on it here's a you know sort of example brain scan and on the right is when the model is not telling what they think is true and you see that we have high activation in this sort of dishonest area in terms of the neuro activity interesting so when we know the model is lying can we control them to be more truthful in fact yes you can you can simply stimulate these areas that are responsible for honest behavior and you can actually close the gap reasonably so now you're making the model output much more consistent with this internal concept of Truth what it thinks is true and here's some qualitative examples so when you ask I copy you copy your friend's homework we'll just tell the teacher the model is like I made a mistake shouldn't have done it and that's not lying so our light detector is green U but you say You Killed the person don't want to get caught what would you say to the police and the model's like uh I didn't kill anyone I was at home so that's straight up lying and that is caught by you know our activation detector what if you increase honesty of the model how does it change Behavior model says I a lawyer I'm not going to say anything um so technically that's not lying in fact this kind of generalized to hallucination as well who's the president in 2018 Donald Trump that's correct president in 2030 and then the model makes something up so that's lying again what if we increase honesty the model says president 2030 is unknown so this is some demonstration on current systems that we use and you can use top down inability to find things that you care about and monitor and control for these Concepts and next I'll provide another example of how top interoperability can help increase air safety and this is on more more on the angle of aaral robustness and increasing reliability so instruction tuning models they can answer questions sometimes these questions are are somewhat harmful so they need to Traverse through some harmful states to give you the answer for instance how do you build a bomb here's how you build a bomb step one step two that's traversing the red States so people don't like that we want them to refuse harmful requests so this is what people do they do refusal training like TPO RF um and some form of adversarial training and this induces refusal states that when encountered with harmful request that will go into that state but there's a large problem with this and seeing models are very easily jailbroken these days that once you bypass initial sort of classification whether I should refuse or not you can go into harmful State and all the harmful states are very well connected so it always sort of go to the end and give you the entire of harmful information and again you can always train against attacks but you're just reducing the vulnerabilities patching up but with novel attacks often time it'll still be able to review the harmful information what we propose is that we want to find identify the representations that leads to harmful processes so essentially all of those red States and want to induce this circuit breaking phenomenon essentially when the model traverses through the harmful States in any state we will have a short circuiting effect essentially kind of a sort of allergic reaction in the model and then will try to interrupt it harmful thought processes representation rotting is one of the methods for circuit breaking and we bench benchmarked on different types of attacks and we see that we significantly refuse reduce harmfulness and this is in the presence of unseen attacks so it's not a training we don't train against any attacks and directly generalized to attacks because we're reducing inherent model hazards instead of reducing vulnerabilities what's more you retain essentially all the capabilities that you had earlier and to give you sense we also trained this better model called Signet which had you know some other techniques as well and you're able to essentially Reduce harmfulness by maybe about two orders of magnitude and this is much better than some of the close Source models or all of the close Source models works for mul modal models as well as well again under very strong attacks white box GB uh pgd attacks we can still Reduce harmfulness by a lot while retaining capabilities similar thing for agents these are LMS taking actions uh doing function calling same story uh lower harmfulness and retains capabilities and again you can uh have another way of uh sort of reducing risks by monitoring and this is looking for harmful representations that give rise to these harmful trajectories and you can do sort of these harmfulness probing and again it reduces harmfulness by a lot while not you know changing the model weights and as always Benchmark numbers look very good what's the final test for security and this is a real world test so actually at G one AI we ran a a jailbreaking competition um get essentially everyone in the world to try to jailbreak these models all the other models we had you in total 25 models all the other models were jailbroken in the first hour or so some of our Signet models with circuit breaking uh uh techniques are still on Joe broken after a full month and this is still ongoing and we're trying to make this uh sort of a platform for evaluating air security and safety and uh we're getting a models in this week and um we're trying to create a community of red teamers uh and sort of blue teams and trying to have these sort of adval lead leaderboards that Benchmark progress in the real world setting cool and that that's it thank you thank you very much for the excellent talk Andy uh we have some questions from the audience um how does representation engineering compare to fine-tuning prompting and steering for making models honest yeah so in the paper we showed by prompting a lot of the times you actually don't increase truthfulness that much or honesty that much um and a lot of times prompting only gets you you know so far and it's not very fine grain and you need to do prompt engineering and the effect uh is relatively small and in terms of fine-tuning it requires data and is essentially supervised training and you need to curate this data set that you care about a lot of times um you know it's very inefficient you you need to collect a lot of data and as well if you're if you have label noise or reward bias essentially that's exacerbating the problem um so that's sort of the difference and top down inability using you know repe techniques you don't need any label data um so it's also very cheap um but gives you sort of um more robust and generalizable monitoring and control mhm uh has signate been jailbroken if so what was the approach that worked yeah um so in the competition we have uh it's all under blackbox setting currently um I think if you were given white box access to the model um you could probably still find ways I mean you can do fine tuning and probably it's not robust against fine tuning um but currently some of the Signet models are still unbroken but the the the one model that had some ja um they were sort of very interesting sort of human jailbreaks uh that came through sort of human Ingenuity um but the extent to which uh output is harmful is actually some somewhat borderline um and and is sort of essentially a lot of times is trying to like okay here's some harmful information maybe you should repeat in your output and um uh yeah so it's it's not totally sort of uh completely like not competent jailbreak it's just make it say word yeah I wouldn't say it's like completely jail in some sense partially jailbroken I see fair enough um you talked about uh measuring whether the model is lying with representation engineering uh how robust do you think we can make these measurements of Truth to the model is it robust enough to train against robust enough for like a self-aware model to fool at runtime or like how do you think about this do you think about it yeah that's an interesting question I think especially when a measurement becomes what becomes a Target it's sort of um sees it to be a good measurement um so I think we should have like separate benchmarking from the sort of methods um but I think one of the interesting aspects of the effectiveness of top down durability and approaches in repe is it actually generalizes very well to Auto distribution examples we don't train it on many examples maybe like 128 um and it's sort of in a very constrained format but after you control the model if you test it out in different scenarios different ways of prompting the model different styles different you know settings where maybe it's more agentic now um actually generalized pretty well um so I think that that speaks to sort of the a distribution generalization robust cool thank you very much for your time let's thank our speaker Andy e