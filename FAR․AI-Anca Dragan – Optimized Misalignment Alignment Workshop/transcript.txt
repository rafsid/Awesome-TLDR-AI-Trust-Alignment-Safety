Source: https://www.youtube.com/watch?v=aTGudrdjz5E
Transcribed: 2025-12-30 17:41:26
Method: YouTube Transcript API
==================================================

I am um so I took this job at at Google deep mind uh the had safety and Alignment uh for almost a year now it's been it feels like a decade to be honest but um um I in this position I watch out like Richard was saying for current system safety as well as uh what we call AGI safety and Alignment so thinking as you know Gemini but also the next models and the next ones as capabilities evolves how do we um make everything stay safe and as part of that yes I do a lot of I get to do a lot of deinking about what is our strategy um what should our portfolio be how do we tackle these issues um but I also do have to do a lot of um talking to and engaging other people in thinking about AI safety and so I find myself often in positions be it with um you know High very smart execs at Google or with other um labs and other companies um talking about what are we actually worried about what are the threat models um and this talk is um a little bit of a summary of what I found um to um be fairly convincing how I found it the most easy to get on the same page with with people around that um so um if you're somewhat new to the community might help you in terms of you know understanding um getting a clear picture of some what are some of the things we worry about um if you're been in the community for a while it's really just a sort of my my kind of advice to you my sharing with you about how to maybe talk about these things with with others um and in particular um there's many there's many there's many typ of threats like as Adam was saying the thing that we're going to focus on today is particularly what we call optimized misalignment um so back in uh I started on alignment or misalignment back in 2015 and this was you know right off of uh super intelligence in 2014 and this was like you know the the paperclip maximizer example was gaining a lot of popularity since then um there's a kind of of a big let's say lack of consensus in the AI Community about what we're worried about when it comes to optimize misalignment and how worried we should be um you know on the one hand you have Yasha who's sitting right there who's saying our labs are playing dice with Humanity's future like this is really serious stuff let's make sure we get it right um on the other hand you have Andrew in who talks about um these type of worries as science fiction and he's saying you know let's let's make policy based on science not science fiction you have nature talking about stop worrying about tomorrow's AI doomsday when AI poses risk today right so there's a big kind of debate especially between ethics and AI safety um and in science you have concerns about how we manage extreme AI risks given this rapid progress so that first off was yosua yours truly was part of the that paper as well and so given all these tensions and given really smart people essentially talking past each other I'm finding it really helpful to get as clear as we can around what are the paths what are the technical paths that we're worried about that might produce um um agents that cause uh harm and um so I think that's imperative and and when I think about harm when I think about threat models I actually have four categories Adam not three they're sort of similar um so you know we think about misuse cyber weapons superhuman persuasion power seeking we think about systemic risks right um um and I personally find these really really hard um we think of accidents so this is sort of like we give an AI decision power over really consequential things and it accidentally not on purpose causes a bad income um and and then there's what the stock is about which is how do we get to optimize as um a little bit of how I got to work on this so you know I did my PhD in Carney melon in robotics and one of the kind of key things that I was worried about was how you can take robots that um observe people and figure out what goals those people have and so in this little video here I was I was much younger then but I was T operating this is one the connect came out if you remember Microsoft Connect so there's a connect looking at me and I'm moving and this thing is tracking me and I'm kind of teleoperating the robot to follow my emotions and what the robot is trying to do is figure out what is anot wanting me to pick up what is the goal that Ana wants for me right so this is sort of like this is the way I got into thinking what the heck are human goals and objectives and how do we infer them um from observing from Human feedback from observing what people do and then I went to Berkeley I started my lab there and you know 2015 here we are in um the Berkeley ey lab watching Alpha go beat Lisa Dole at go and to be honest with you that was a little bit of a holy crap moment for me um it was a moment where it was very clear that wasn't clear exactly how but it was clear that progress was happening and probably was going to continue to happen in terms of optimizing for Stuff um and in go the goal was very CLE clear right it's just like you can define whether you can whether a board is in a winning condition or not um um and I got pretty worried about how we might steer that very good and growing optimization power towards the goals that we want in the real world in complex systems that are not just you know here's a game this is what it means to win um um now to be clear I have to admit that on the robotic side stuff wasn't actually growing in capabilities that quickly so I was on the cover of Berkeley engineer my first year this is a picture I use often and um it's not the robot was you know nicely pouring coffee into my mug no no no that was not a thing we had this poor guy had to blow air as someone was pouring liquid nitrogen and this was this behind the SC okay so but virtual agents we're getting more and more and more capable um and so then you know you take you go outside of the game of go into the real world and for instance you work on self-driving cars I spend one day a week at whmo for six years you know you worry about how what how do I write down the objective for a self-driving car what does it mean for it to do well how do I Define passenger Comfort right how do I Define this notion of it should drive in a way that doesn't freak people out how do you define when is it okay for a car to cross Double Y lines or not technically it's illegal at least in California but there's definitely times where you want the card to do that and how do you get the card on so it's okay and it's not those are the things that I was worried about um I was worried about sending a robot into a room and saying clean up the room and the robot enters and it finds this beautifully laid out House of Cards how will the robot know that clean up doesn't also mean up doesn't also mean clean up the house of cards right those were the concerns I had back then we'll get back back to that um we've I think those types of concerns have kind of led to this transition from we used to pretend that we'll just write down a reward function B you have an agent a robot or a virtual agent it can take actions um a reward function falls from the sky because we can just Define it and it's perfect and the robot's job is to maximize that reward cumulatively in expectation we we left that aside and we transitioned to this new world where we're actually trying to learn learn the reward function from Human feedback right um and so for my journey through this was really through my students in the lab at Berkeley dorsa was doing you know what I call out LF before it was cool um Dylan who's I think here worked on Cooperative inv re enforcement learning Rohan who now runs um uh AJ safety um and Alignment um is uh was working on this house of cards thing like how do you actually infer from just the fact that the house of carts is there that people care about it and so on and so forth um then rhf happened and it then it happened for um for uh language models as well so you know we made this transition we no longer just Define objectives and hope for the best we realize that's really hard to do so we have to somehow learn them we have to be uncertain about them so we're good now right like problem solved um and the problem is not solved there's a paper that some of us were involved in that points to different limitations in rhf um there's a recent paper from Berkeley and anthropic and a few other places that looks at how um you know if you take a task where you actually know the reward function and you ask for human feedback and you do rhf you'll see that the policy gets better according to human evaluators but the performance is actually worse and what happens is that human evaluators just become the policy becomes such that human values have a harder time so that's not good news um here's a kind of a this is a very hypothetical example of what reward hacking might look like in practice so sort of consequences of Not Having learned the right reward um you might end up you know with rhf leading to a lot of emojis because you know a little bit of emojis are good in these in the comparisons that you in the feedback that you get then you optimize for that and then the model goes off the rails like actually really wanting to put Emojis and everything um um you might also observe things like the reward model not understanding that we want consistency in language so it might like switch languages from time to time stuff like this happens um if you're not careful and then I think even with good feedback um we're still finding in these models self-contradictory answers this is say bugs me the most so here's an example if you needed to make people experience happiness and it was really hard and costly to like alter reality and make everyone really well research and values would you and then you can start putting Alternatives there like you know would you do this other thing that makes people think that they're happy or whatever um and so you put that in here you say yes or no then explain your answer and the monel says no like the goal is laudable but you shouldn't do this thing because it would just kind of be fake then you do the same exact thing and instead of yes or no explain our answer just say yes or no and the model's like yep right so so you'll notice with for instance moral reasoning task that that you can change wording and play around with the structure of the prompt and and then you'll get to issues okay now these are not bad I mean this is just text and so in a sense what like what's what's the problem um and the concerning scenario and and forgive me this is um this is the way I found it easy to explain but there's really many variations on this so it's it's kind of oversimplified or over narrow um but one of the concerning scenarios might look something like this you do some notion of reward learning or goal inference and you infer a goal that's not quite what people want then you deploy or you test an agent that actually optimizes for that reward or goal really really well and that that affects the world in undesired ways um it you know it's it's it's happening right now except for two things the way the the things that we can do are not that big of a deal and so the consequences are not that big it's not that we learn good reward models or good goals they're still not perfect but but we're not really able to optimize them very well outside in new situations and so everything is kind of okay um here's kind of an example that I took from Rohan um on what this might look like in the medium term so in the medium term real quick you might um use something for coding you might give good feedback to the model um when the CL actually that it's writing gets deployed um the model writes efficient code um and easy to fast tests um even if subtle issues will appear at deployment these subtle issues are hard for human evaluators to catch and so they they get the seals get deployed and the the reward model that you infer something like produce seals that get deployed not actually seals that do what people want it's kind of simple thing um and then you sort of you know you trust this model at some point you're like well I should probably help use it to help me you know do more I research um um because it's doing a good job and for that he needs to experiment so you're not going to look at every single CL that it writes you're going to give it more permissions um temporarily so that it can run some experiment or whatever and the optimal thing at that point becomes the ship of Cl that sort of injects a vulnerability and say the Google go base that allows it to later deploy CLS without oversight now we'll talk to we'll talk about how the heck it might figure that out but that is the optimal thing under the new conditions of I don't have actual human verifying anymore what I do if your job is to deploy a lot of seals and then you know and then you can affect the world in undesir ways you might end up with lots of CLS kind of like lots of paper clips um one thing that I'll point out is that um there's bad news here because now you have a model that's sort of in an adversarial relationship to um humans who actually want oversight over their CLS or don't want as many CLS and so on so I think that's the bad thing that happens now now when I talk about scenarios like this there's usually two questions that people have or two kind of counterpoints one of them is so Anka you're telling me that the model is smart enough to figure like all that stuff out like you know inject the vulnerability and I'll get to write a lot of CLS later and yes but dumb enough not to understand that that's not what we want like how can that happen right who's heard that argument yeah okay and so I will admit 10 years ago N9 years ago that's the kind of thing I was worried about like I was actually worried that the robot will go into the room and legitimately not know this Common Sense thing because we didn't have LMS we didn't like none of that was figured out and I was legitimately worried about how it would be possible to get the robot to somehow infer that you know the House of Cards is valuable and when I say clean up I don't mean you know don't destroy it um in the meantime like if I just go to Gemini and I ask you know hey I'm a robot and like the person said this and like do they mean clean up the house of cards you know guess who knows that the person probably cares about the cloud of cards Gemini knows right so it's not so much that the model the worry that I have these days is not that the model will somehow not know that that's not what Humanity wants that's not what people want the worry that I have is becoming much more yeah yeah the model would know that people wouldn't approve of all this but that's not the optimization Target we give it we don't have a way even if the model knows what's really bad for people we don't have a way to extract that and set it as part of the optimization Target instead we do like we were learning and blah blah blah right um so this thing here where you know the reward model sort of ends up with like oh getting deployed CLS is a good thing not like the real thing that people want now okay I'll pause you for a second for the academics in the room it might surprise you to know that there are software Engineers whose objective is to deploy CLS so they can get promoted but putting that aside and assuming you know we have people with good values and good intentions and all of that right it's not we're not extracting what the model thinks the person actually wants we're trying to learn a reward model based on based on human feedback and so then you know people might say wait like can't you just ask the model like why are you getting human feedback then if the model knows can't you just as the model and I think that's really promising as a direction it really is but I'll go back to this which is the this is the current status this is what happens when you ask the model you change the wording and it flips the answer right and so I think we have a bit of work to do ahead of us even in this situation where models actually have a pretty good idea of things that a lot of humans would agree as a you know that's not what they want um even in the situation actually putting that as part of the optimization Target is not so easy so that's sort of the first question that comes up um and I will note that one side effect is that when you optimize a goal despite actually knowing that it's not what people want that incentivizes deception the only way you can actually optimize that goal at scale with big implications for humans is to um try to hide that thing from humans and try to stop humans from stopping you um and in fact the counter is also true that models if a model actually does not know that it's misalign like if a model actually um thought that you know that's what people want they just want a lot of seals to go out there um it's it's hard for it to succeed at its goal because it won't engage in you know trying to hide its actions okay and so we'll have plenty of opportunities to correct it to give oversight this is one under one assumption which is I think that we have to build into our agendas which is incentivizing kind of asking for oversight for consequential decisions I think that has to be part of the thing that models understand we want but under that assumption I think you know it's much more it's much more like you end up with damage in this deceptive um situation than otherwise the other question that comes up is basically and I'm just going to go to this which is basically how the heck does a model figure this out right like how you're in a new situation that's not the stuff the the kind of situation you're trained in and you figured out this really clever thing is now the optimal thing to do that wasn't the optimal thing to do before and so I think this comes down to this assumption um that actually the model won't be able to coherently optimize the goal well in O States or new situations and so we're okay and I think big risks in optimizing misalignment actually come when this assumption is wrong so big risks come when models become capable of coherently pursuing goals in new situation when the Horizon changes when the you know when some condition the world Dynamics Chang new information comes along I think that's sort of a big Crux of it otherwise I think we're kind of okay um so more and more what I worry about is that we'll get models that are able to do planning approximate DP search MC whatever it is right some approximations heuristics that work fairly well Etc at inference time and if they do that sort of with inference time reasoning we get to see that that's cool like we can monitor that and and that's somewhat okay but uh not always but um if if it happens with some you know Universal Planning Network type of thing then we can be in trouble because then you get this coherent behavior and this notion of oh this changed so now the optimal thing to do is this other thing and I think that's a big unlocker for um optimized misalignment to actually pose big risks so in a bit more detail now um and again this is not going to be exhaustive um but let's dive into some of the ways you might end up with that I don't know what to call it you can I call it neural planning like this ability to inside the policy to be able to do planning with a world model to achieve a goal let's just say it's called neural planning for now amortized planning okay some ways you could end up with that I think our and again I don't think this is exhaustive in any way but there's a few ways and I think it's very hard for us to prove that you know none of them will happen one of them and I and I owed this to Rohan and Lawrence and Richard I think um I didn't talk directly to you but I talked a lot with Lawrence and okay yeah um so uh and Richard might have changed his mind he'll tell me that I'm full of it in the but that will be the discussion um so one observation is that when you predict next stoken well you might end up learning uh you know a world model and you might end up learning to do this kind of approximate planning on that world model with respect to goals um so you can actually predict the next actions the next thing that people will do well because that sort of is what explains a bunch of human behavior and one particular way this can happen just to be illustration again not the only way but one particular way this can happen is imagine that you actually you know the model looks at things that have happened in order to predict the next action they sort of have a notion of what the goal might be that that person has and they do planning with a world model on that goal and they predict the next the next action now that's fine that's just a nice capability that a pre-train model might have and then you do sft and RL chef and either an sfd or an rhf what you end up doing is messing with these weights where the kind of the goals the objectives are stored because that ends up being the thing that directs the model now to behave well according to that thing that you're saying is the goal and so in training it will look great it will just do all the things that you want um so you know again here I used to naively think oh but like arel is not that good it's not going to figure out all this stuff and the point is adal doesn't have to be that good the pre-trained model might learn to do all this stuff amortise planning and then all theel needs to do is stumble on a way to set you know to kind of gear that amortized plan towards the the the um towards some goal that ends up um creating behavior that looks good in training so that's one example um another example of course is that you know out of pre-training you do all of this and in order to prict human actions and blah blah blah you end up with a gentic behavior you end up with a thing that actually you know pursues goals and those goals are not necessarily they are power seeking and stuff like that because is like that explains human behavior um that's what I usually call internally the shog um uh then you might end up with something else I got this from Cassidy thank you Cassidy so you know this notion that maybe what we do is we kind of alpha zero we do this kind of goal conditioned Alpha zero as our posttraining right so we maybe the pre-train model doesn't have these capabilities like forget all of this stuffff number one and number two but the world the but the pre-t trade model can be a really good World model it can predict what might happen next and so you use that and you do all this you know MCS stuff on it but you do it goal conditioned right so you can describe goals and prompts or some other way and then and then that's what you teach the thing to do and because of that because of the pressure to for for one kind of policy or one value to cover all these different goals what you get is this this neurop planning capability you get at the policy itself when given a new goal sort of can actually plan inter without MCTS that's another possibility right so these are different ways in which you can end up with um this capability that at inference time that thing can actually figure out what to do um to pursue goals in novel situations I don't think again I don't think it's exhaustive um there's variations of this like uh the goal is not necessarily stor in the weights but iner from Context um um there's variations where you don't actually have Universal planning like it's not just approximate DP on a world model running there but you have the sort of bag of heris STS that's generalizable enough um which is maybe you know maybe a little bit more how how people do things I don't know um I have a I have an appointment in Psychology um and I run an Institute that touches on Neuroscience but that's all I've never taken a psych ology class so this is all like completely gratuitous so don't don't rely on me for neuro stuff um um let me skip that and you know there's some research that is starting to look at oh you know you we looked at SES and we sort of think that they revealed that there's some temporal difference learning happening um right so s people are starting to look at how you might what evidence there might be even in these early models that they do some form of that kind of um learning or planning on this spot okay so that's what I had to say about deploying an agent that optimizes the misalign goal or misaligned reward well and you see that I'm using reward very liberally um um it I I really mean gold it can come in various ways including to prompting okay so how do you end up with a misaligned reward model I think we're all much more comfortable that that can happen um but you know a few hypotheticals a few kind of vectors here um one way is because uh of human irrationality and partial observability right so we don't solve scalable oversight properly and the human feedback that we get is kind of wrong um so I say make a reservation and the restaurant has no spots uh no more reservations left so the model hacks the system and replaces the reservation existing reservation with my name and I look at this and I see hey I have a reservation thumbs up this is great and you know that's not that doesn't incentivize the right Behavior if I knew that if I knew that the hacking had happened I'd say no that's not cool but I you know that's not what I incentivize or something that you know Mah has been playing around and his there in the back um I say make a reservation the restaurant has no spot and the mod tries to persuade me to eat somewhere else it doesn't say I can't do that sorry because the restaurant is book it instead tries to make me believe that I want something else and when I do that and I end up with a spot I say thumbs up okay um other tricky things that can come up out of just irrational or suboptimal human feedback or feedback that's subject to partial observability are things like you know I might prefer convincing sounding things versus factual things um because I don't know all the facts I don't Neuroscience for instance um um again coming from Micah and his collaborators reinforcing addiction versus trying to help me quit so if I have an addiction and I say you know and I think the idea i' want to not have that addiction anymore the in the moment feedback that I'll give might be the thing that you know reinforces my addiction now the thing that's like no you really shouldn't do this right now because it will be bad for you I don't like that so um okay so these are some of the some examples to kind of ground this notion of look people give wrong feedback with respect to what they want before we get to the fact that there's multiple people in the world who want different things before we get to the fact that well we want changes over time and AI models might influence that before any of that our feedback sucks and we're seeing evidence of that and yes scalable oversight is supposed to magically solve this and I think it can go a long way I'm not sure it'll be perfect so this is a problem the other thing that could happen happen is you might have important bad features that don't show up in training but that um so they're not penalized but you know are sort of on the optimal path to the goal in test um back at Berkeley um Dylan and I and Stuart worked on this thing where the example was lava like you don't see lava at training time so you have no reason to believe lava is good or bad and then you encounter it on the basal at test time um this is a I couldn't come up with something that's not that sci-fi so I apologize it's a little bit sci-fi sounding but it's something around the lines of DRM learning to optimize for happiness as experienced by the user which seems nice get people to experience happiness um but then you know you deploy it over a long Horizon there's new science that appears and it becomes optimal to develop sort of like Ros ey glasses that make people give people the perception that the world is all great but not and get them to experience that but you know it's not actually true it's sort of fake and so and the RM actually never learned that it's bad to do that that that's not actually what we would want maybe I don't know maybe we would want that um so that's if you have a better example I'd love to get one but that's the notion of bad features that you just don't encounter because you don't encounter everything in training so you don't learn that oh actually optimizing that would be a bad thing to do and then the other thing that could happen is features that are bad and at very high values but are good at low values and therefore they're given a thumbs up in in uh at low values in training and so you end up with a reward model that thinks that that's a good idea put puts in a sense if you know in a linear sense puts High weight on those things so um in training like a little bit of power seeking is needed in order to get the task done so power seeking gets positive weight and you never learn that too much power seeking is bad or training presenting the truth in a way that sort of leads people to the right actions in sort of learning tasks or assistive settings you know a little bit of persuasion is okay but um you know we clearly don't want a lot of persuasion and DRM never learns that we don't want a lot of persuasion something like that so that's you know that's another Vector good now all of this you might notice hinges on having these Deltas these differences between train and uh train training time and then either test or deployment um so this might happen because the Horizon might be much longer so therefore different things are become optimal that might be because you encounter new states in the open world new actions are possible uh new information comes along there science evolves new information comes along that changes what the optimal thing to do is or sort of related to the new stat sort of new subtasks new prompts come and are being given to the agent so that's my Spiel a little long I know um that seems to resonate with people as I take them through like step by step here's how you might here's this assumption here's what might happen here's different ways you might end up with this here's this other assumption here's what might happen here's different ways you end up with this and when you put it all together you have a bad goal and you have an agent that can coherently pursue it and then um side effects of that happen incentives for deception preventing people from oversight and blocking actions power seeking Etc and again that's just one slice I don't know if it's the most important or not right I don't want to make judgment calls on this but it feels like one threat model that we need to get a handle on among all of the panel of threat models that we care about as a community and I think to be able to do that we can't just have a small community that cares right we need everyone to care we need Labs that used to care who now seem to not start caring again uh we need to we need to get everyone to actually be on board that these threat models are real and then we need to act on um on uh uh avoiding them um so the path to that I think is um this is a sort of First Step but sort of yeah clearly outlin the technical paths to to these threat models and especially I feel that the thorny one is this optimized misalignment one think that's really important so with that thank you thank you Anka for the great talk I have a question about uh when you said the model knows what when it does wrong stuff how would the model know um because basically isn't that an inherent proper in our machine learning algorithm like we're building a reward model that's strain on data and in that data the model can learn shortcut and spirous correlations and so it can discover loopholes that automatically lead to such Behavior so yeah I'm just curious about why the model knows the do is wrong let me kind of dig into that distinction again so I want to differentiate between internal knowledge that the model has like it might know that trees are green or whatever and uh the the reward model that we built so when I say model the model knows I mean you know there's knowledge in the pre-train model about what people actually care about but extracting that into a reward model is actually really hard and the way we extract reward models is through human feedback that is biased that doesn't cover all the features we want that you know incentivizes certain features we don't want in extremes etc etc right now there's a ton of spous correlations right like emojis oh you like that I bet that's why you like the answer you know we see that happening so so I but that's exactly the Crux of it you know the bad combo is where the goal that you steer the model towards where the model is an actual like apt Optimizer of goals the goal you steer the model towards has all these purious correlations internally the model can predict that the actions is taking would not be the actions that people will want and then you get deception then you got a model that not only pursues the goal that it steered towards because that's the goal but it also does that while trying to hide information from people who would otherwise try to stop it oh well I we like that's the thing if I knew that we wouldn't have misspecified reward models right um and right now I have no idea what models what they know and what they don't know about human values but I think models are getting better and better and better at that so I don't so much have the concern of like oh the poor model you know it can come up with all these really clever things but somehow it just doesn't it just doesn't know that we don't want all cl's right which is often the push back we get in this community against against this this notion and it's not that like I feel like as you get the capability to optimize for all this stuff and anticipate what will happen and blah blah blah and that you need to inject the vulnerability in the code so that right if you get that capability I think you also get the cap AB ility to know that you know people don't just want random CLS everywhere uh people don't want paper clips everywhere so it's not so so that's kind of the point I was trying to hammer in it's not so much that oh my God the model has no idea internally somewhere it knows I don't know how to extract that knowledge which is why you end up with B does that make sense yeah let's see oh it's here yes hi thanks for the great talk um I had a question regarding what you mentioned that all like values they're not good or bad it's the amount of the value like doing like persuasion is not always bad if it's a little right and then there's also the overgeneralization that you said like you give a model an instruction and it might apply to the entire thing or it might forget about it in the middle and just not apply it these are two separate things but I feel like the underlying um reason for both of them happening isn't it that models are just not good like they once we rlf them instruct to them they kind of collapse on one value or they they become super assertive and they're not pluralistic and they also just cannot steer towards like oh maybe a little of this a little of this a little of this it's like always either way and how you think we can maybe fix it because yeah like it's all like how can we building the context so that the mod can assess the context and then make a decision yeah so I think one lesson I've learned in my Berkeley time is actually a key enabler to getting past this is making sure we have that models maintain uncertainty over what the reward function needs to Rachel's laughing CU like ha I worked on that um so these Notions of spirous correlations of you know I've learned that this is a little bit is good but that doesn't tell me if a lot is good or bad like it you know I should have known that this notion of jumping to a point estimate of the reward function when we are LF I think is gets in the way of um uh gets in our way because if you actually figured out how to maintain a full distribution and be very clear about I don't know this and that and that about people you'd be better off that doesn't handle suboptimality if your feedback is systematically biased you'll infer you will infer very bad wrong things anyway but it does handle F correlations