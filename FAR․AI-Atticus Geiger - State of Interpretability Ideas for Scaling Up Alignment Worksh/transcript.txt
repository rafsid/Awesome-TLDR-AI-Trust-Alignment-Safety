Source: https://www.youtube.com/watch?v=eqZ1iEoor5s
Transcribed: 2025-12-30 18:05:15
Method: YouTube Transcript API
==================================================

hey everyone I'm attakus and I'm going to be talking about interpretability just kind of uh the state of things where we are and my thoughts on how we should be scaling it up and maybe some issues that currently exist with how people are doing it okay cool so I'll just kind of hop in so I think roughly a way to try trict what interpretability might be able to do for us is it might give us the ability to predict control and understand models and I'm going to quickly go through kind of like each of these um kind of uh topics and then tell talk to you about what I think are sort of the state of art methods for doing each of these things and then I'll get to what it would look like to try to scale up interpretability all right so for prediction really I think it hasn't changed too much since pre-large language models and maybe like the 2016 like kind of like Alex inet era it's just you can just train another classifier on model internal representations to predict out a concept so if a model has French text input then you can train out a classifier like what language is this text and people have been doing this for a while and you can also um think of this as just within information Theory so if you have an arbitrarily powerful probe all probing is doing is measuring what is the mutual information content between a concept and then a hidden representation and typically we train probes on like labeled input data but you also also might think about training a probe on what Behavior a model will have um and that could be more relevant in a safety setting but that's pretty much just I think the standard approach we have for predicting what models are thinking about basically all right so then moving on to control so control is a weird one and I think steering has become a pretty popular way to think about interpretability and what we might be able to do with it but I think a lot of the time we're not as eager to run the really strong powerful baselines that we have for controlling a model which are one fine tuning so if you could steer a model to make it an angry model now or you just fine-tune the model to be an angry model with some angry data basically and that we should be running these Bas lines so we can actually figure out um yeah when fine tuning isn't the thing we want to do and so there's also parameter efficient fine tuning so fine tuning can be really expensive and I think that's why we might not want to do it yeah um and then we have prompt engineering which is another very competitive method for controlling models you can also just ask the model to be angry or ask the model to speak in French or ask it to talk about the Golden Gate Bridge so these are yeah this a really strong Baseline for controlling the model and it you know is unclear to me like how much work it's going to take to get to a point where prompt engineering or fine-tuning isn't just the right way to go about uh manifesting some desired Behavior in a model and so methods like dspi have the ability to even like automatically generate prompts for you given a specific goal and it's just very cheap and easy to interact with and so then we get to steering which is I think really just a collection of methods that are focused on doing an intervention to some model representation giving it a little tweak to then get some desired Behavior whether that's removing something or enabling something and so uh basic steer method is just so maybe you have an angry text and then a bunch of angry texts and a bunch of calm texts you run them through the model you take the average angry representation the average calm representation find the difference between those two and then that is a direction along which you can modulate the model representation to make it angrier or to make it calmer so this is like a very basic supervised method for going about doing this and this really would only work for binary uh Concepts because you can't really do this in like a k way or I don't think I've seen anyone doing it but maybe Andy will tell me I'm wrong or something um yep so yeah here's just the example you know I love talking to weddings uh talking about weddings I hate talking about weddings these are the contrast prompts which are then used to steer a model and so another there are other simple ways to get the directions we would want to do steering so maybe you train a linear probe and whatever the direction that that probe is sensitive to that's also a thing you could use for steering or dimensionality reduction techniques like PCA or um uh SVD will provide you directions that you know explain a lot of the variants of the data and they might be useful to use but really what I think for steering the most crucial thing to do is just to establish a setting where steering is a good idea I I don't think that exists yet and some ideas I have on what this might look like is if you're really overusing your prompt space so maybe you have like a thousand things that you want to cramp into the prompt maybe some of those could be handled through steering and you give the model like more room to think but I have not seen this yet and I think that's basically where we need to go first in this research direction to motivate whether it's one we want to pursue at all yeah and then uh last thing I want to mention so representation fine tuning is work that I I I'm involved with and it's somewhere between parameter efficient fine-tuning and steering so the sort of idea is that instead of just adding a vector to some hidden representation you add a vector to the hidden representation and then you also add a little adapter that is a low rank Matrix transformation and all that does is basically like read out a small amount of information and then inject a small amount of information at a specific location so far we've only benchmarked this method on just parameter efficient fine-tuning sort of things so just like standard fine-tuning stuff but I think where representation fine tuning is going to shine the most is when we're trying to do surgically precise and like layered compositions of steering yeah um yeah and you can just kind of think that this is a way of taking the power of supervised machine learning and then using the targeted manipulations of steering cool so prediction and control is not really what I would consider sort of like the core idea of mechanistic interpretability and a lot of times when we talk about it in the field we use words like basic science and reverse engineering neural networks into algorithms and I I personally think that my research and the research with my collaborators over the years take those sentences really seriously in a way I don't think a lot of other research in the area does so we actually have a really mathematically and philosophically precise idea of when an algorithm is being realized by a deep learning model or really any dynamical system yeah so I'll I'll go through sort of just like how I think about what the like gold standard mechan turp looks like so yeah really this is just a general phenomenon so we explain dynamical systems with the algorithms that they Implement so humans doing stuff this is like the entirety of cognitive science in functional theory of or functionalist theory of mind is that yeah we're something like computers and we can explain our behaviors and how we interact with something like an algorithm and then we also have the same intuition probably about deep learning models you know if you're Gary Marcus you never had this intuition about deep learning models but it's one I've always had myself um and then also a physical computer you know the most canonical case of implementation like that system is clearly one that implements a computational object and was something very cool for me in my undergraduate is basically realizing that this notion of algor algorithmic implementation we just like hadn't made a lot of progress progress throughout my PhD and up to now on yeah what it looks like for computational explanation to be good and for an algorithm to actually be realized by a system so basically our framework is that we represent a dynamical system and also a hypothesized algorithm as causal models so all causal models are there's just a bunch of circles and then arrows between those circles and then rules for determining the value of a circle based on its appearance it's just a discret graph with some control flow structure to it basically and you can represent a neural network as that where the variables are individual dimensions of vectors or entire vectors there's lots of ways to do it and you can also represent algorithmic flow and just algorithms with causal models and so then how do we understand the relationship between a simple causal model and a more complicated large causal model is with causal abstraction and it's a pretty basic idea and it's you just go through this high level model with a small number of variables and you say where are they localized in the Deep learning model so you'll make claims about where the variables in the algorithm are being sort of causally realized or where information flow is sort of being mediated in the network and then once we H now that we have the language to sort of like make these claims in the first place the intervention experiments we do are going to be hypothesis testing and actually figuring out whether the hypothesis that we formed is accurate so yeah and so we have yeah and so causal abstraction has these interesting origins in the last 10 or 15 years as like a mathematical field for supporting people who are analyzing weather data and also brain data and I think we're actually much better suited to do this sort of analysis in interpretability in a in a really deep way so this task of aggregating micro variables into macro variables it's really hard with the weather and with the brain because we don't have Interventional access to these objects it is very expensive and very difficult to do even basic intervention experiments in these two settings but we really have arbitrary precision and measure in this setting we have like an ideal sort of like neuroscientist situation where we can manipulate and rerun and freeze and patch in whatever information we want and so I think there's good reason that these these new deep learning models that are so good are the first opportunity We have basically in human history to do this sort of hierarchical reverse engineering and yeah that's why I'm really excited about interpretability and I think it's a yeah promising and unique sort of like moment in history for us yeah um so the basic like recipe that we can walk through a little example for is I'll walk through here so basically the hypothesis you have about the structure of some neural network is a highle model and then an alignment between the high level variables and lowl variables so suppose we have just an algorithm that's adding together three numbers X Y and Z and we have a network that performs this the then we have two runs of the network one two and three four five and six the sum X Plus Y is three on the left and is n on the right if we do an intervention where we replace the three with a n the output should go up by six it's like a very obvious statement about this very simple algorithm for addition but then what it means for these variables to be realized in the network is that if we do the coresponding intervention on the neural network hidden States we get the same output we get 12 so it's really a matter of aggregating a lot of these intervention experiments and showing that there's a counterfactual correspondence between quantities at the high level that are easily understood and these seemingly uninterpretable lowle hidden representations and then I think one of the most exciting developments that like come out of the research that I've been involved with is that we have a supervised gradient descent-based method for localizing Concepts in this way the very short idea is that you freeze the network parameters and then you learn an alignment between the highle model and the low-level model and that alignment is parameterized as a rotation Matrix that basically tells you which linear subspaces of a representation you're going to do an intervention on I'm not going to go into the details on that for the P you know you can look at the paper but basically the idea is that if you have the data counterfactual data telling you what a network should do if a concept has been intervened on you can then use that data as a signal to localize that concept to model internal representations and if you want to do that Lo localized concept really think this is the state-of-the-art method for for doing so yeah and yeah so now I'm going to spend the the end and I'm going to give my sort of like hot takes and thoughts on that what we should be doing to scale interpretability and talk about sparse Auto encoders briefly all right so just a refresher on what sparse autoencoders are so sparse Auto encoders are trained on the hidden representations of neural networks to Recon construct them they project them into a high dimensional space and then they reconstruct that representation by putting projecting it down into a smaller space and these are basically the methods that people have for featuri the residual stream or vector representations in deep learning models and so the typical SAE is just a linear layer followed by a relu followed by another linear layer for decoding and it is trained to just reconstruct the residual stream representations for tokens on a very large purpose and features are automatically labeled using some Auto interpretability Pipeline and current pipelines really rely just on here are some examples for which this feature activated and that's how they label these these features these new directions in the residual stream space yeah and so one thing I think SES would be good for is that they are unsupervised so they might be able to come up with the features we haven't thought of but I really do think if you have a specific concept that you want to localize sees will not be what you would ever use you would if you want to predict you'd use a probe you want to control you'd use something like steering or ref and if you want to understand you do something like distributed alignment search and that saes will basically never give you features that will be used in critical pipelines because if you can ma like articulate what the feature needs to do that's the same data you need to supervised train for that feature to get a better one yeah and so here's what I want to see moving forward in scaling interpretability first I think it's really insane that our goal is to mechanistically understand an object using an SAE that has been trained only on the observational distribution of that object it's like doing science without ever doing experiments basically like if we need mechanistic understanding we need to simulate counterfactual States and networks with interventions which we can very easily and cheaply do and we need to incorporate that type of training data into our scale efforts for interpretability there's no reason to think that you could jump up a whole level of pearls causal hierarchy by just training on observational data and now all of the sudden you've recovered a full mechanistic reverse engineering of an object I think that's my really I think that's like the strongest problem I have with sees right now then we need to pre-register our Criterion for Success um we need benchmarks we need to say what it would look like to be successful at a task and then hold ourselves accountable if we're not able to meet that I think that slipperiness is something you really want to avoid saes have uh context independent features meaning they unpack the residual stream in the exact same way no matter what the context is you're in and that is not an assumption I think you should make either also I think we need to uh open-minded about nonlinear features and yeah I have been working on benchmarking uh interpretability methods also have an alternative idea on how we should scale things up using counterfactual data and I have some food for thought on nonlinear representations but these are kind of like papers that I'm currently writing or about to be out and I'd love to talk to you guys about them yeah um so the first question we have is um is there a way to use Cal mediation analysis to Benchmark SAE faithfulness yeah so I would say causal mediation analysis is exactly a special subcase of causal abstraction and absolutely we should be doing that and The Benchmark that I have on this slide Ravel is doing this it's a benchmark for disentangling pieces of information and figuring out which components and features are causal mediators of that information um great uh the next question is uh your CLE obstruction idea has been published since 2021 it's been fairly influential um and I think in the way people think about circuit usually reflected but people usually check whether hypotheses mat the model in a very aoic way rather than using the full formalism do you think that's a problem do you think it should change where should things go here is a fine I think that's a really good question honestly yeah I like doing the the you using the full formalism I think is basically never necessary you know like formalism or math if you can explain your ideas in a better way you should totally go about doing it but I do think that um say Redwood research is I I was a big fan of their interpretability uh stuff but I think they really held themselves accountable for hypothesis testing I think they were realistic about what failure looked like and their conclusion was interpretability is really really hard and they didn't want think it was going to be a viable Way Forward I honestly think that yeah that that that is really the way we should be thinking about things and I think some of the other interpretability efforts we see embrace the slipperiness of not having a defined criteria for success um let me check if there's another questions otherwise I have one um I ah what do you mean exactly by intervention data observation data is equivalent to intervention data when all variables have been observed that's I don't think that's right in any way is that right okay okay wait what was the sentence yeah you can identify any causal estimate if you observed compounding so in this particular scenario you don't have anything that's unobserved or unmeasured so observational data is like effectively the same as Interventional data yeah so what I mean by Interventional data is that the training data for SES are just the hidden representations the values they take on when we put in an input and what the data I think we need to have is once we put in an input and we let it run out what happens when we do interventions on the hidden States in there then what will happen that's definitely data we don't have and don't use and it's really easy to generate like yeah yeah um okay excellent um maybe one last question uh if if there is more uh how well does distributed alignment sege scale in terms of compute what's the algorithmic complexity um yeah I mean the only trained parameters in distributed alignment search is the rotation Matrix so you have to in the initial version of it you had to sort of like choose a location you wanted to start doing your disentangling on and then you would just train this sort of like just rotation Matrix which is just uh or however many Fe Dimensions you're going to intervene on that square it is basically what you want or that times the dimensionality but the hyperd which is like this project that I've been working on was just really thinking about like how do we scale up distribute alignment search in some crazy ways and we're trying to do things with sort of like a Network that is able to select locations in the residual stream to do intervention experiments and have that Network take in natural language commands about like the concept to localize it's pretty ambitious and the pre-print is going to say basically like we put this end to end and the gradients worked and like it was able to be optimized some uh to to success in a very toy setting but yeah I think there's basically are alternatives to scaling interpretability that aren't saes and that this is like food for thought about what they might look like