Source: https://www.youtube.com/watch?v=JVoYzS2RcRc
Transcribed: 2025-12-30 18:10:21
Method: YouTube Transcript API
==================================================

thanks for the generous introduction I'm excited to be here and give this talk about alignment and interoperability a good place to start this talk is by defining what value alignment means I stole this slide from from is who defines this as how can we align AI with human values so that it reliably does what we think it should do and this definition aligns with the one in the book the alignment Problem by Brian Christian and this book will reappear in this talk again now once you define the problem as such now you can break this question into two you ask a technical question how can we encode those values and then there's also normative question what is the value now I want you to keep these two questions in mind but Place some thought experiment with me so let's say we're trying to align values of two people and they happen to be from two different different culture and these tles represents vocabul and Concepts that could be described using this vocabul and let's just say that I'm person B I'm from Korea and I wanted to communicate what the value of Chong means anyone in the audience who know what Chong is right so I can just make it up so Chong is something between love and like and it's not and it's less than love but it's a little more than like but it's never a romantic thing and also it's something that doesn't happen suddenly there is no Chong at first sight it has to take time and it kind of grows into you now if I were to and this concept of Chong stems from such a fundamental ideas in Korean culture in order for you to truly understand what this means I would have to tell you a lot about Korean culture and you probably know if you speak multiple languages you probably have words like this in your languages too that Words that cannot quite be translated between cultures between languages without a lot of context in other words I would need you to expand what you know in order to truly understand what this Chong means so the alignment problem is something similar we're trying to align values between humans and machines except this time it's more difficult question because humans and machines we don't even share very basic principles like we have families we want to survive so this gap between perhaps in these two circles are even even bigger in other words there's potentially this huge space m minus H that only machines know and that's the space where this beautiful move 37 that Alpha go made against its match EO in 2016 came from goal players still talk about this move it's we cannot quite understand it maybe in retrospect in pages of details we can but we don't quite understand how to generalize this move and also the unexpected behavior of a machines that we weren't quite prepared for is likely to come from that M minus h space so if alignment problem if Ms we exactly share the same vocabularies alignment problem is easier not easy but easier because once you found the value that you want to align then maybe maybe using some Theory or something like fail of ignorance then you can write down the definition of the technical approach that you want to solve so you can just focus on that technical problem but of course that's not the case the truth is m is not H so you will have something like purple dot on the left where it's our value that we care about but machines don't have a cavy to express it yet and you will also have something like on the right where it's a value that we care and machines perhaps know it but it didn't it wasn't described in a language that we can understand so we don't know we can't understand it yet so how do we solve the alignment problem we need to expand what we know and doing so actually expands machines too because by knowing more we are able to build machine that can communicate uh with us and align values with us a little better and I argue that I'm here Target this whole talk is about convince you of that that interpretability is at the core of enabling communication between humans and machines and we need to do this by expanding what we know and teach ourselves something new so that we can understand and communicate with machines better and so that we are not only in a better position to align the values like the purple dot there but also in a much better position to be prepared for an unexpected harm and risks that we worry about but there's more we can actually benefit from these machines like imagine we can learn rational behind M 37 we can do that medicine and Science in other words we don't just survive but we Thrive and actually have reasons to have these machines around so hopefully that makes some sense but you might wonder well you know that's that that's nice but alignment problem is already really hard can we just focus on eight first and maybe I'll do something and that will hopefully fall into something between m andh m cap H I'm here to convince you that that's not a good idea and here's some stories around how what happens when we pretend M ish that there's no M minus H so this is the way that we used to think about interpretability we wanted to know how machines work they're rational behind their decisions so we decide that we're going to shoor everything into map H because what we don't we don't in fact just H because we don't even know where M is so we developed many many methods like feature attribution feature visualization my own work falls into many of these categories and that didn't go very well years after years we just encounter over and over again empirical and theoretical evidence that this actually doesn't work as well and you're very welcome then so so for those who are less familiar with what feature attribution is a family of technique called saleny m is aiming to achieve U or aiming to do feature attribution what that means is that we're going to assign a number for each input feature so if you're doing it doing it in image classification we want a number in each pixel and what a lot of these methods aim to aim to get at is we want to know the shape of the function f ofx the prediction function around this feature X so if I move the this feature increase this feature is this become more likely a bird or less likely a bird or stay the same now the intuition here is that well because I am wiggling this x this has to do with the result of the model which is the prediction F ofx in other words well at the end of the day we want explanations that has something to do with the prediction and that's why we're doing explanation right that sounds pretty obvious it turns out that's not the case and the first author of this speaker Julius is in the uh in at the workshop here um in 2018 we stumbled upon this phenomenon that if we take a trained Network and then randomize the weight so making it basically untrained Network and random prediction the explanation coming from these networks are virtually indistinguishable from the one from train Network so one is random expl uh prediction the other is Meaningful uh prediction and you can't tell them apart but maybe you can maybe maybe if you stare at a little longer maybe there's some difference you can't because I flipped these two and you didn't even notice right did you see you can't even tell the difference I I don't even know which one is yeah the this is the right one and and we show emper we show in qualitatively and quantitatively that this is the case so this result really bothered us from 18 and we put these methods in front of human users and show maybe it's PR and useful in practice it turned out to be that's also not true but we always had this uh itchy spot but we don't have a theoretical proof that these methods actually don't work as well uh now we finally do Natasha who is part of this work is also in this Workshop uh here again the goal is to get shape of the function so expectation here as written in the original one of the paper is that you can use this methods to account the contributions of each feature and what that means is that if the tool assigns zero attribution to a pixel then the pixel is unused by F ofx and F ofx is insensitive to X and in fact that's how these methods were used in practice here's a paper published in nature and they use one of these methods to uh this determine eligibility of medical trial our work says None of these inferences true just because tool gives you zero attribution doesn't mean that it's unused it does not mean that f ofx is insensitive so how how did we do this uh theoretical sketch uh the theory um proving sketch we treat this interpret interpretability question as hypothesis testing so the question is I have a function I have two hypothesis function is maybe flat or function is maybe going up can we tell these apart given this data from this attribution methods and we show that methods with complete and EDI of properties by the way very reasonable properties to have for these methods those methods has to satisfy this performance inequality one minus true through negative is is less or equal to True positives and what that means is that is this line over here in the in the graph ideal method would have one true positive and one true negative and this is oops no that's not pointer that was not pointer one ideal method is in the corner randoming in this line and all methods falls under that line so you must say well that's just maybe worst case maybe in practice this works well happens to some structure coming from neuron network uh it turns out that's not the case so we did this imper IAL testing for two concrete Downstream tasks recourse and spiritus feature recourse is very commonly used in task essentially means if one of my feature is my income then if I make more money would I have approved of this loan and S feature you're probably familiar with and both of these concrete Downstream task reduces to hypothesis testing so that we can do this and for cart 10 for both of recourse and Spears features we see that actual uh the the performance lies around that random curve it's a miror miror image of the previous curve you saw and this was true for a lot of other data sets but you might wonder and we also wondered but maybe this has to do with the complexity maybe if the model is simple maybe these methods tend to do better etc etc so we did the testing and here you can see that that is kind of a case where depending on the data sets and depending on the end task some some methods do well like in um in this plot we're drawing both methods that we that falls under our theorem and methods like lime and gr who does not fall under our theorem or smooth grad and you can see it's kind of all over the place sometimes these methods do well sometimes they don't but the problem is you don't know which case of these your data fall into a prior you just have to throw it and close your fingers so how did this happen I believe that we pretended that m is AG because we didn't have anything better and they that made us delusional that we might be able to understand these machines in fact we we we we treated that oh we can see pixels so maybe if we do something around pixels we'll understand something but that turns out to be completely incorrect I think we this would have been prevented had we had a clear goal that we can measure if we had then as we developed these methods over many many years including myself we would have got the signal that we're not getting anywhere near the goal or or maybe we're we're moving in the wrong direction in other words we made a lot of hammers but we couldn't find a lot of nails cuz once goal is clear there's actually really a simple method to solve this problem getting the slope of the function you sample around it and the question is how many samples do we need you can actually we we show in this paper that you can push take this curve upwards by having more samples having you know if you have more features then will be you will need more samples etc etc and you can actually do this if you had clear goal this might not be your goal but if you had clear goal you can just do this simple in using simple method so what do we learn I still think that saleny Maps can be useful if that saleny m method happens to align with your goal like if you're just trying to generate some hypothesis in a lowrisk explorative setting then this might be your choice uh and also pushing science is important too and I think in particular in mechanistic interoperability May provide foundational Insight that get us closer to human machine communication but we still have to have our eye on the goal if you were if your goal is to cure answer you don't just randomly pick a gene and try to study random aspect about that Gene you carefully select what to study and how you study it and I think that's what we should do in other words always ask if you're developing a method if your method is an answer what's the question so hopefully I convinced you that mle AG pretending mle's age is not going to be a good idea so then your question might be how do you do M minus H how do we learn like is that even possible here's couple reasons why you should be skeptical so how do we know machine is a super human because it's a superum it's beyond us we don't know and who should we teach who should be learning who is the expert who is the the best Frontier of human knowledge how do we know like good doctors even themselves disagree so how do we know who to teach how do we verify that we learn something new because again this is superhumans so we wouldn't know we wouldn't immediately notice chess offers an amazing playground as as a showcase um as a case study because in chess there's a clear win and lose you have a el rating that I'm talked about a little bit to uh that that tells who is better than whom and you also know who are the best right now because there's tournaments and games every week every day and we have Grandmasters who are clearly at the frontier of human know knowledge and we can also know confirm that experts learn something by quizzing them because there's a bir poition we can ask them to make a move to see learning has made an impact on their play how they play and we have of course Alpha Zero from Deep mine that beats stockfish engine which is the best chess playing engine out there only after four hours of training from scratch and Alpha zero learns it from selfplay that means it plays against itself so the question is still is this even possible tldr is yes it is otherwise I won't be spending all this time talk telling you about this so I mentioned that Alpha zero is self it learned how to play chess via self-play and what that means it has never seen how humans play chess so you might first Wonder well is there any overlap do we even have mcaph some of our previous work showed that yes there is some evidence of Alpha Zer having human chess Concepts and this work that I'm about to share with you is taking a step further and asking can we can we excavate some knowledge from M minus H and teach it to Grand Masters this kind of work is only possible if Stars a line and I was very fortunate to work with Lisa who herself was a chess champion and the head of professional chess player uh life before she joined as a PhD candidate at in Oxford and this worked wouldn't have happened with L without Lisa and we had amazing Grand Masters that who participated I'll talk more about them in a bit so again here our goal is clear our goal is to teach Grand Masters new chess Concepts and make an impact on how they play the game the approach is rather simple so first we start with some ingredients we discover concepts by embedding that concept into an embedding Vector in policy value Network and we filter Concepts heavily uh because we won't be very very picky when these Concepts gets delivered to the Grand Masters and we evaluate algorithmically and I'm going to share more importantly how we evaluate this with humans first of all you might wonder what is a concept we Define as the following we Define it as a unit of knowledge that is useful for a task in this G in this case winning the game of chess and we impose two properties not to say this is all the properties that you want but this one happens to align with what we're trying to do one it's minimal that it cuts out all the fat and it's sort of concise and that it's transferable it can be Tau to AI agent and eventually to a human in chest setting we assume that a concept gives a rise to a plan a deliberate sequence of moves that players play there's many way to operationalize this but this is one way we choose to do it we learn a sparse factor and we confirm that this can be trans transferred to another AI agan or human Grand Masters in a measurable way it's very important as it's quantitative so how do we discover Concepts we do convex optimization constraint convex optimization pretty simple thing um one thing to note about Alpha zero it's not one network it's actually two components policy value Network and MCTS tightly working together to to generate those moves so our optimization process incorporates both of them so here we trying to learn a vector vcl such that the following constraint is true and the constraint comes from MCTS so M MCTS is like a tree at each node you have a fork you have chosen path and unchosen path chosen path is is given uh the green dots and the red ones are unchosen one and we simply say well if there were a concept in this vcl then a concept would more strongly present in the chosen path than any other unchosen path there's a lot of unchosen path because at each T time step T you have things that you didn't do right so you can you can kind of enumerate that inequality equation now we filter a concept A lot and this turns out to be very important we filter it in two different ways teachability and Novelty teachability is we can we want to ensure Concepts can be taught to another AI agent this is like if you're teaching another U student of a math concept then what you do is you give them an unseen math problem and see if they can solve it and that's how you verify that they had learned something and we do the same thing here uh and we do that by doing following three simple steps we choose prototypes positions from by sampled from alpha Zero's previous games and that closely relates to this concept measured by cosign similarity pretty pretty simple and then we teach student by minimizing the policy of student and the teacher the K Divergence a simple thing and then we test students on an unseen data like they never seen this before and they we measure how similar the students moves were compared to the teacher who knows this concept we filter 96% of the concepts in there and what's also interesting is how we enforce novelty we do it in two different ways one is we do it by construction we use two alpha zero models one at the final training step and one couple of hundred thousand zooks before that and we only use positions that the two disagrees now remember both of these Alpha zeros are already superhuman so we're doing this as a just extra insurance second we Define what we call novelty score by by doing the following we first create sort of language for humans and Alpha zero and you can do this by decomposing human games like zh and you can decompose it to find some orthonormal bases and you do the same thing with Alpha zero but make sure you you you only encounter the same number of games of course otherwise one will have more information and then we see if I were to reconstruct that concept Vector vcl which one works better the a bases Alpha zero basis or human bases we say the novel ones are the ones that are better reconstructed using Alpha Z's language or Alpha zero bases and that's how we Define noties store and given that we filter a lot of it and Lisa painstakingly went through them to make sure it's not silly and we finally deliver those Concepts to Grand Masters so here's how we did it in phase one we gave play players a set of positions and we asked them make a move and we collect them and in phase two we show what Alpha zero would have done on those positions and this is learning step they're learning new Concepts and phase three we give them unseen set of positions and and and ask them to make a move again and we compare how well they did how well they they learned this concept based on how much they improve comparing phase one and three and all players some of them significantly improve and and learn now we're doing something interesting in this phase too we're giving them the simplest possible explanation and and avoid all the potential failures of explanation methods by not using them at all we just give them Alpha what Alpha zero would have done and by doing that we're actually heavily relying on these grandmas ability to connect the dots because it's pretty sparse information then they would have to somehow create this connection kind of beyond what they're used to in order to understand this and there's some evidence that they are doing that and there are of course some Concepts that actually none of the experts ever learned either learned because they don't learn perfectly on all Concepts oh for some context by the way coaches for these players these players are so good that coaches for these players spend like weeks months sometimes years to teach them one new thing because they probably know all the chess chess moves that ever happen in in the history of Chess and so the fact that we can change the way that they play this much is actually pretty uh pretty remarkable so they they said some nice things too in know qualitative results they said oh it's clever it's interesting uh there's definitely some novel elements to it they say unnatural that's very fluttering uh and they also say it's complicated it's it's nice idea it's it's hard to spot Etc and here is an example of that concept for those who play chess in the audience who play chess in the audience great quite a lot okay wonderful uh if you have difficult question don't ask me I will connect you with Lisa because she's the expert I I'll be honest I I spent a lot of time trying to understand this one concept like hours if not days but anyway here's an example so when human uh chess player play a game they they have some this guiding principles so first in opening phase like this they're told to control the center develop pieces as soon as possible and bring the king to safety now uh The Grandmaster chooses Bishop B3 E3 which does the two of the following control the center and develop pieces now Alpha zero chooses BG5 and I'll let you wonder why for a little bit before I show exact sequence it sort of does kind of a crazy thing so here is the exact sequence did you see that the queen is gone all right I'll show you again boom there right uh it's kind of crazy because in in in Queen is the most important piece in your in your game on your board and in fact a chest players assign numbers to these pieces Queen is a nine and Bishop and KN Knight or six so Alpha zero decided to trade nine for six which is kind of insane move and the only and doesn't happen very often in human games only time this would happen in human game if for a tactical reason which means there's immediate gain like Checkmate but in this case what Alpha Zer did is it sacrifice sacrificed the queen to gain long-term dominance on the board and in particular provoke weakness in black and this is a grandmaster's words that it's a clever that it provokes so this was like in hindsight was also understandable by Grandmasters and Grand Masters improve uh trained on this concept improved by 50% in phase three and of course there are many many many other Concepts in our paper which is an an archive some of the other fun concept uh theme that runs in Alpha Zero's Concepts uh Alpha zero does not seem to care about the rules like humans are told to abide by certain rules Alpha zero doesn't care there's a rule to the rule that says you only touch each piece once in the opening game so that you can quick kind OFA your position Alpha zero doesn't care it touches everywhere and it kind of replaces pieces everywhere and Alpha Zer tends to be also flexible in planning and executing those plans and human ch um you develop plan on the left side of board then you kind of stick to it until then you see it through and right side the board you also do the same thing but off a zero just goes left and right all over the place which is kind of interesting so to uh to closing note I think the the goal of an alignment problem and interoperability both are about establishing this communication Channel between humans and machines and I argue that better way to do do interpretability is that we extend ourselves and when we need to learn something new in order to establish this communication and in a way that I'm even I'm even willing to go a little stronger than that there might be the only way and by doing that we're also expanding What machines uh expanding the machines representational space because we are better in position to build machines that can better communicate with us and here we I showed a cute example of Chess but of course we want to go Way Beyond this and this was just an example to show well this is possible in a measurable way learning something new from M minus H and we want to build a vaby of this m minus h space that we can acquire maybe my son will be learning this vocab is from his school going forward CU that's something that is essential for humankind like we learn math and and other science in and at school and we're very trained in that it wasn't something that you weren't born with but now you're used to it because it's useful for your life I want to close with this excerpt from the alignment book um Brian was going through some Archive of volunteering and here's a alen TR talks about how there's a he was trying to teach something pretty simple to the machine and he tried everything and Moine was kind of slow to learn so he really had to like enumerate all the sequences to to to make this machine to do something pretty simple and his panelist and colleague asks but who is learning you or machine and Turing ansers well I suppose we both were special thanks to Claire who helped me prepping this stop and with that I'm happy to answer any [Music] questions hi thank you for the amazing talk I'm a huge fan of this Direction I mean I literally Nam my lab conceptualization lab but I have a question about the cultural problem here how do we invent vocabulary and Concepts that won't regress onto a simpler interpretation that people find easier to understand that seems like a huge problem with complex Concepts like people misuse P values all the time and say they're confirming the hypothesis instead of rejecting the null hypothesis how do we think about creating a vocabulary that humans can understand and still actively explains machines ah interesting so is it like our own fallacy rather than machines fallacy well um you know that's an that's a that's a fun question and I'm Pro I'm really advocate for studying humans as much as you we study machines because we don't even understand ourselves very well and until then we can't really think about this policy like Sam talked about the biases of our our humans that's now well studied maybe we correct some 10 times and they still make the wrong direction wrong decision and this does happen right Danny Danny canan's book of undoing project my one of my favorite book talks over and over about how well-trained statisticians make wrong judgment over and over again about statistics problem right if you write down math you can solve it but intuitively we're just wrong we're just primed to be wrong so to answer your question I don't have an answer but I think we should Al you we should really think about how we think about us in this equation it's not just machine and it's us studying ourselves how to fit this puzzle together is the the part of the puzzle thank you thank you for a great talk um you found some minimal and transferable Concepts from M and then you formed this new H which we could call H+ however M minus H+ might still uh be non-null and contain very important safety Concepts and in general why do you think it should be possible to expand M plus to be large enough to capture everything that's Salient and also why do you not focus on restricting M instead of expanding H yeah that's a great question so first question I don't think we will ever be in a world where m is H right let's rule that out first this doesn't even happen between two humans right two Koreans you put them in the same room they don't know Korean things um so the question is where do we focus Our intention our time which H+ should we learn and I think that's a decision as a community to decide I think next step um for those who are interested in is to go beyond chess and think about a particular problem that we may encounter like hallucination to see how we can acquire that H+ so that's that's one thing um and your second question was is M directly right um because it's too big we don't have to we don't have time to learn all M right we can probably only learn some of it and quite frankly I think there will be things that we might be it might be just impossible for current human intelligent to understand so we don't want to cover all M that's kind of waste effort we don't have a lot of time as we as we talked about in this in this alignment Workshop so we want to really make an intentional decision of when what we want to study