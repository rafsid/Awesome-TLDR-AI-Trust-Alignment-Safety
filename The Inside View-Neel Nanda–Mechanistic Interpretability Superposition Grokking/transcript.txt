Source: https://www.youtube.com/watch?v=cVBGjhN4-1g
Transcribed: 2025-12-30 18:10:19
Method: YouTube Transcript API
==================================================

bold hypothesis despite the entire edifice of established wisdom and machine learning say that these models are [ __ ] inscrutable black boxes I'm going to assume there are some actual structure here but the structure is not there because the model wants to be interactable or because it wants to be nice to me the structure is there because the model learns an algorithm and the algorithms that are most natural to express in the model's structure and this particular architecture and stack of linear algebra are algorithms that make sense to humans I'm here today with Neil Nanda Neil is a research engineer at Google deepmind most well known for his work on mechanistic interpretability gracking and his YouTube channel explaining what is going on inside of neural networks to a large audience YouTube channel called Neil Nanda and I'm very proud of that name [Laughter] um yeah I remember reading you like four years ago at some party in London and at the time you were studying mathematics in Cambridge and you already had some kind of like Outreach where there was like classes of mathematics at Cambridge and the classes was Neil Nanda where you were like putting them on YouTube and explaining more stuff uh additional stuff from the teachers um and now yeah fast forward to 2023 you've done like some work with entropic fhi chai Google deepmind you covered multiple papers around uh mechanistic interpretability and working which we'll talk about in the episode yet thanks Neil for for coming to have you all right thanks for having me on let's talk about like more like your your YouTube experience um you have like a YouTube channel where you talk about like ml research and you do what what you call like ml walkthroughs of paper which I think is like the I think it's the first person or like one of the only people to do it um yeah why why did you start this like what was the story behind this yeah so the credit to this actually goes to uh Nick camarata who's an open AI interpretability researcher who has this video called let's play building blocks of interpretability uh which is the name of this paper and he was like let's do a let's play or I record myself like reading through and playing with the interactive graphics and was commenting on how this was just really good and people seem to really like it and the effort was like trivial compared to actually doing it actually like writing the paper and I was like ah it seems fun and then one evening when I was in consolation this Bay Area co-working space I was like ah it'll be kind of good to go do this for a mathematical framework I have so many hot takes uh I think in mathematical framework is a really good paper that most people don't understand and decided to go sit down in a Call Booth and ramble uh this was incredibly low prep for a low production quality and involves me talking into like a Macbook microphone and drawing diagrams on my laptop track pads uh just like rambling in Colby for three hours till like 3am and it was just really popular got retweeted by Dominic Cummings it got what is it like 5 000 views and I've had multiple people tell me yeah I read that paper didn't make any sense and I listened to walk through and made loads of sense and if I heard uh and people who have explicitly recommended you should like read the paper and then watch Neil's walkthrough and then you will understand that paper I was like well that was incredibly easy and fun guess I should do this more more recently I've started doing the Morris interviews because um I don't know empirically if I tell someone let's sit down and read through this paper together and chat then this will work and I will turn up into time and do it if I'm like I should of my own volition sit down and monologue through a paper like way higher efforts and we're more likely procrastinate I'll do and people still watch them I don't know why well I I watch them I even like listen to them while I'm in the gym and I have like Lauren's Channel Neil Nanda talking about drugging well and I'm doing like bench press but there's so many diagrams this is the entire reasons or podcasts because we're like looking at diagrams for disgusting well I guess some of it is like it's like you guys talking about um numerical instability and like why are like low loss so hard and like why like three e minus nine is like important on something uh 190 minus seven please yeah it was a paper you talked about like on your on your first like Marathon at 3am oh that was a mathematical framework for Transformer circuits which is still in my opinion the best paper I've been privileged to be part of that's this anthropic paper we might discuss that's basically a mathematical framework for how to think about Transformers and how how to break down the kinds of algorithms they can implement and just lays out a lot of the foundational Concepts you just kind of need to have in your heads if you're going to have any shots at doing mechanistic interpretability in a principled way so possibly I should Define mechanistic interpretability before I start referencing it casually in conversation yeah uh Neil Nanda what is what is mechanistic contemporability sure so mechanistic interpretability is the study of reverse engineering the algorithm learned by a trained neural network it's kind of this weird flavor of AI interpretability such as bold hypothesis despite the entire edifice of established wisdom and machine learning say that these models are [ __ ] inscrutable black boxes I'm going to assume there are some actual structure here but the structure is not there because the model wants to be interpretable or because it wants to be nice to me the structure is there because the model learns an algorithm and the algorithms that are most natural to express in the model's structure and this particular architecture and stack of linear algebra uh are algorithms that make sense to humans and it's the science of how can we vigorously reverse engineer the algorithms learned figure out what the algorithms are and like whether this underlying assumption that they're a structure makes any sense at all and do this rigorously without tricking ourselves because as I'm sure will be a theme it's so so easy to trick yourself when you say like the safer the algorithms instead of neural networks what is like an example of some kind of like algorithm we can see like inside of the weights sure so um one example which I'm sure we're going to get to more later on is there's paper progress measures for grocking Via mechanistic interact ability where I looked into how a one layer Transformer that's a particular kind of neural network does modular Edition and what I found is that it did modular Edition by thinking of it as rotations around the unit circle where um if you compose two rotations you're adding the angles which gets you additional and because it's a circle this means it's mods of 360 degrees so you get modularity for free if you choose your angles of the right frequency and I found that you could just go inside the model and see how the inputs were represented as trig terms to parameterize the rotations and how it used triggered entities to actually do the composition by multiplying together different activations yeah I think that's like one of the most like science examples of your work and I think you posted it on Twitter like oh I managed to like find the composition of like modular Edition in like christianas and Science and everyone like loved their mind um that remain that was my first ever tweet and that is the most popular thing I've ever tweeted well I I feel like you're still not going going upwards on YouTube and and you've done like all these podcasts and everything but um yeah I think this was kind of very interesting I'm kind of curious if there are like other other examples of like more like computation that we see like instead of neural networks or or um or is that most like the world most well-known case uh it's the one of my most confident actually is there and in my opinion it's just the prettiest another example is that of induction heads so this is going to get a bit more involved to explain so the way so a feature of language is that it often contains repeated subsequences uh so models like gp3 are trained to predict the next was and given a word like Neil if they want to predict what comes next it is unfortunately not that likely that Nanda comes next but if Neil Nando's occurred like five times in the text so far Nanda is now like a very good guess what comes next because it's like oh it's a text about Neil nut and it's a podcast about Neil Nanda it's a transcript exactly um hey gbt5 and yeah so it's like um and this is like actually a really really common structure you just could not know nanta came next without searching the previous context and seeing that nanza came after Neil and models are just really good at this they're so good at it that they can actually predict if you just give them completely randomly generated text just randomly generated tokens and then add some repetition models are perfectly capable of dealing with that uh which is kind of wild because this is so far outside what they see entry and it turns out they learn an algorithm that we call induction notably implemented by these things we call induction heads and so so induction essentially does a head which learns to look it it learns to look from the token Neil to the Token Nanda that is the token that came after an earlier occurrence of Neil and it looks uh Nanda and then it predicts that whatever it's looking at comes next and this is a valid algorithm that will result in it predicting that um and the reason this is a hard thing for a model to do is that the way Transformers move information between positions is via this mechanism called attention where each token gets two bits gets three bits of information um a key which says here is the information I have to provide a query uh which says here's the kinds of information that I want and a value which says here is the actual information I will give you and the queries and keys are used to match things up so like find the token that is most relevant to the destination I want to bring the information to and importantly uh this is all symmetric like from the perspective of the query of token 17 uh it looks at the Key of token 16 the key of token 15 until the key of token one all kind of separately like it caught it doesn't treat the there's no relationship between the key of token 15 and the key of token 16.

it can't tell that they're next to each other it's just like shuffles everything into an enormous mess and then hunts for the keys that most matter because the key for token 16 has no relationship for the key to token 15 it's all kind of shuffled up from the model's perspective it's really hard to have a key that says the token before me was Neil um and the model needs to like actually do some processing to like first move the information that the previous token was Neil uh Along by one and then compute a more complicated key that says the thing before me was Neil so the attention head knows how to properly identify nanta and yeah the other interesting thing about induction heads is these are just like a really big deal in models like they occur in basically every model we've looked at up to like 70 million parameters and uh but we found them by looking at a two layer attentionally model which is just one of the best vindications thus far but studying tiny toy models can teach us real things and it's a very cool result so I guess like induction heads was like this paper by anthropic maybe like 2021 2022 um and in the paper they may they might have studied like smaller models and so you're saying they checked as well for 70 billion parameter models or is it like later evidence uh so what actually happened is in so we published two papers around the late 2021 early 2022 and we found induction heads we discussed in the first one a mathematical framework in the context of two layer attention models but we were in parallel writing a SQL paper in context learning and induction heads but we looked up to 13 billion parameter models uh 70 billion is just I look to the deep I looked in chinchilla and I had them yeah may as well just increase the number I'm allowed to get during talks so yeah the the 13 billion is the actual number in the paper but to to go higher you need to actually talk to Neil Nanda and see what he's doing in the weekends yeah I really need to convince someone to open AI to go look in gp4 so it can be like the biggest model in existence has them guys it's all great when you say chinchilla that's the um I think it's a paper by divine right so you you have access to it but like it's is it it's not public right for people to look at it yeah I think that's kind of an interesting thing is like you you like to do like research on the side you you you don't just do like research uh during the day but you also like do like a bunch of like mentoring and a bunch of like weekend marathon where you like try to like explore things um it's so fun and I'm so much better at procrastinating than doing my actual job it's great yeah so yeah I'm curious like how did you start like getting so much in love with like um mechanistic interpretability which will like maybe call like make interpret uh or Mercantile uh moving forward yeah because like four years ago you were like maybe like doing alignment work at different orgs but maybe like not that much interested like what was it like I think I made your brain be like oh this is interesting yeah so I don't know I kind of just feel like we have these unscrutable black boxes that can do incredible things that are becoming increasingly important in the world we have no idea how they work and then there is this one tiny subfield led by like this one dude Chris Ola but it feels like it's actually getting some real insights into how these things work and basically no one is working in this or taking this seriously and I can just go in and in like a year the one of the top researchers and mechanistic interpretability in the world it's just like what this is so fun this is incredible why isn't everyone doing this there you could just like look inside them and there are answers it's also incredibly incredibly cursed and messy and horrible but like there is like a real structure here it's so pretty it's like there's like a beautiful problem and and there's like five people working on it and everyone is like super like smart and doing like a bunch of like crazy things and there's only five people so you can just like join them and and like look at this thing by yourself yes but now at like 30 to 50 though so you know your time's running out if you're hearing this you want to get under the ground floor there's not that much time left we'll definitely have solved this thing like next year it'll be easy would you say like you became interested because of his like links to alignment and you were like wanting to solve like alignment somehow like did you like when are you guys interested in like alignment was it like before that yeah so so maybe I want to distinguish this into two separate claims there's when should I decide I was excited about working lens and then there's when did I decide I wanted to work where I feel like I decided that I wanted to work 11 a lot much earlier than I actually became excited about working where I've been involved in the EA for a while I read Harry Potter and maths raciality when I was like 14.

I hung out in less than a bunch I read a bunch of the early as safety arguments and it kind of made sense to me and I spent a lot of time hanging out with EAS a lot of my friends and stuff matters and honestly I spent quite a long time working like hanging out in the space before I properly internalized that I personally could probably go and do something useful rather than alignment being this weird abstract thing that might matter in 100 years but like the only thing people did today was prove random useless theorems about uh and I managed to so I managed to figure that one out uh towards the end of my degree I graduated in about 2020 from um undergrad at Cambridge and Maths for context and um I gradually realized wait [ __ ] uh something an alignment probably matters this seems like a really important problem I'm a smart mathematician so I can probably contribute maybe I should go try to figure this out and then um I was actually going to go work in finance and then at the last minute it was like hmm I don't really want to go work on lent but I don't have a good reason for this I just kind of have this like uh believe for this alignment man it seems kind of messy um this seems like a bad reason and also I have no idea what working Library even means um I haven't actually checked and like maybe I should go check and uh this in hindsight was a much easier decision that I thought it was um but so I then took a year um and did a bunch of backs back internships and some different alignment Labs um the future of humanity Institute during some Nancy Theory stuff Google deepmind or back then just deepminds doing some fairness and robustness work and the sense of human compatible AI doing some interpretability work and all of these was a were a bit of a mess for a variety of different reasons another thing I did really like clicked uh but I also just spent a lot of time hanging out around 11 people started to become a lot more convinced that like something here mattered and like I could go and actually do something here that was useful and I then lucked out and got an offer to go work with Chris Eola at anthropic and other type I think I massively underweighted what an amazing opportunity this was because I kind of underweight underweighted like holy [ __ ] cross Ola is like a genius who founded a groundbreaking reset field and well personal event to you this is like such a good opportunity and also I think I was underweighting just how important getting excited about a thing was and how it just seems I don't know I had some concerns that mechanistic interpretability would be too narrow and detail-oriented and tedious for me to get excited about right which I think were reasonable concerns and I'm just not that excited about these oriented pots but fortunately there's enough of them that's fine um but I eventually decided to accept the offer uh my reasoning wasn't great but I made the correct decisions so who cares and yeah I don't really know if there was a pure point where I like fell in love I think there were some points early on where I felt like I had some real insights like I came up with the terms q k and V composition as part of helping to write the mathematical framework paper and it felt like I actually got some like positive feedback from Chris that I'd made a real research contribution and started to feel less insecure and more like oh wow I can actually contribute though I think it only really became properly clear to me that I wanted to pursue this long term after I left anthropic and had some research success doing this work on my own most notably this progress measures for grocking Via mechanistic interpretability paper and just had like a week where I was incredibly nerve-sized by understanding what was up with modular audition had this conviction that obviously the grocking paper was a great place to apply McIntyre that just now was trying and then was Vindicated when I was indeed correct and got some research results that everyone else agreed was cool that no one had done and was just like wow I actually properly led a research thing I can't be insecure about this and I can't just be like ah really this was someone else's thing and I just like helped it this is like my research thing that I owned that like I think was cool I mean I'm insecure about how cool it was but that was probably the moment where I was most clearly like I want to do this I think there's like a story about like whether alignment is important at all like is it like a real thing that's like I can I can make progress on and like are people like actually doing research on on this like productively um is this like a real problem um to solve like is it like Origins and then there's like like can I do anything about it is it like anything I can do that like I feel excited about it and so like the one year like internships are like more like oh is there something going on and and like you might not be sure that like you can do research but then the moment where you like realize that you can do research was like the crystal uh contribution where you're like oh I can do some stuff and then they're like like emotionalization is like oh I'm actually good at this like I'm pretty good at this maybe I have like a superpower in this maybe I should probably do this full time or something uh yeah pretty much yeah one thing I think is a bit overrated is the thing I was initially trying to do of like find the most important thing and go work on that where a I think this is kind of doomed because it's just really complicated and confusing and B I just feel like the fact that I like macintuck and I'm good at it is a such a ridiculous multiplier about productivity and that I just like can't really imagine doing anything else even if I became convinced that like this angle unskillable oversight was like twice as impactful as macintuff right so you're saying that like basically you just like enjoy doing it a lot and and it's good that it's like impactful but like you most most of your weight is on like what is making you productive and like excited yeah and like I think people should just generally wait find the thing they're excited about more than I think many people do because many people are EAS and thus overly self-flagellating yeah if someone watching this doesn't know what EA means it's effective altruism because otherwise you're going to be lost um do you have all these families you don't know what EA is there's like there's like thousands of people on YouTube that like there's probably like it was one at least like 10 or like 20 I don't like on my video is corner like a lot of comments were like what the heck what the hell is the EA thing hello today is lucky 10 000.

I don't know it's like there's like 10 000 people in the world that's like maybe like are part of the effects effective autism movements so I wouldn't be surprised if like I would be very surprised if like everyone was watching my videos I guess like I also asked people on Twitter uh to ask you some questions um like Dominic was curious about like you carrier path and um how did you go into a deep mine from independent researcher or um how is deepmind element work but I guess like you already answered the first part so yeah what is what is the element work like pretty fun I'm not sure it's actually that different from just any other kind of element Works which makes me not sure how to answer that question uh I think in particular the mechanistic interpretability I'm personally pretty excited about doing most of our research on open source models and generally trying to make it so that we can be as scientifically open as possible I mean obviously one of the main benefits of doing alignment work in an industry lab is you get access to proprietary models and you get access to proprietary levels of compute um honestly for Macintosh I think both of these advantages are significantly less important than for say the scalable oversight team where you just can't do it if you don't have Cutting Edge models yeah okay you can just like Define quickly what's scalable or reside for people who don't know ah sure so scalable oversight is this idea you can kind of think of it as early chat plus plus like uh no people don't know so the way we currently train these Frontier language models like chat GPT is the system called reinforcement leading and consumer feedback where you have it do something and you then ask the system the system does something and then a human reader gives it a thumbs up or a thumbs down depending on whether it was good and you use this technique called reinforcement learning to tell it do more of the stuff that gets you thumbs up unless the stuff that gets you thumbs down and today this works kind of fine but this just pretty obviously has lots of conceptual issues because humans are dumb and humans like aren't experts and everything and there's often like subtle problems and models and if you just give it a thumbs up or a thumbs down on like a couple of seconds of inspection then you can easily reward things that are superficially good but not actually good like things like that and this is all just like yeah kind of an issue and what ends up happening is that um yeah yeah this is just probably not going to score and scalable oversight is what are forms of giving feedback to models that might actually scale to things that are like smarter and better and it covers things like rather than judging the apple of the model you could have two models discuss something and a human breaks the one that thinks of me the best argument which is right here called as85 debates you might have AIS help the humans give feedback like critiquing the output of another AI which is this line of work from open AI called the airplane climate team called critiques um and yeah that's like the kinds of thing that happens in scalable of science I kind of think about it as coming up with the kind of schemes that as the AI gets better our ability to give them oversight gets better and where accordingly miss the ideas revolve around things like getting the AI to help you give feedback to the AI there's this other question from Simon Campos was in the podcast before um he's asking what is the most ambitious degree of interpretability that you expect to get with current Transformer architectures thank you is the is the spirits like how far with interpretability go yeah how how what was it like a most ambitious version of like like how far can we actually go hmm my guess is that we can go pretty far like my guess is that we could in theory take deeply false behavior and be able to answer be able to take anything dpd4 does and like answer most reasonable questions we could care about uh around like why did it do this is it capable of this Behavior I think we're more but we're much more bottlenecks by like sucking at interpretability than the models being inherently uninterestable and I mean it's like it's a fuzzy question because possibly the model is like kind of bad um but it's kind of a fuzzy question because maybe it's really cursed in this way and you can imagine a different model architecture that makes it less cursed and we can deal with the cursiveness if we were just like small enough and try it hard enough so there's a part about the like are humans capable of of doing this with current coordination or like our brains and then there's like is it actually possible like on paper yes oh yeah and then there's the other question of um the sphere of alien obstructions the models implementing algorithms that are just too complicated or objections we haven't thought of and my guess is we're just very far off this being an issue and the even up to like human level systems this is probably not going to be a dramatically big deal just because so much of the stuff the models are doing is just not conceptually that hard but you know I'm sure we're going to eventually have models that figured out the equivalent of 22nd century quantum mechanics and stuff like that where I expect it to be kind of screwed yeah if we need to like have another quantum mechanics breakthrough before we understand neural networks it's maybe like a tough bet um I guess to to answer your um like the question of like how ambitious can we be I think it's uh we can just go to the angle of like how do you actually look at weights and how do you actually do this work um because I think I think it's kind of like an open question of like how does neolanda stare at weights and come up with like new computation or like new theories like what why was the process you do so uh to people who are curious about this I do in fact have like seven hours of recyclable walkthroughs where I record myself doing research I'm starting the models which try to understand and you can just go watch that watch those I also have like another 16 hours of those unreleased I should really get around to putting out sometime because yeah turns out a great productivity hack is just announcing to people I'm gonna hang out with zoom call and do research for the next several hours I'll record it come watch uh there's a lot longer answer just just watch 16 hours of YouTube video exactly like when does anyone need any other kind of answer um so yeah try to engage with the question I kind of feel like a lot of my research style is dominated by this deep-seated conviction that models are comprehensible and that everything is fundamentally kind of obvious and I should be able to just go inside the model and there should be this like internal structure um and so one bit of research is I just like have all of these hypotheses and guesses what's going on I generate experiment ideas for things that should be true if my hypothesis is true and I just like repeatedly try to confirm it uh elements of research trying to like red team and break things well like I have this hypothesis I do this experiment I'm like oh my God this is going so well I then get kind of stressed because I'm concerned that I'm having wishful thinking and I try to break it and falsify it and come up with experiments that would show that like actually life is complicated research is what I call trying to gain surface area where I just have a system that I'm pretty confused about and rather than and I just like don't really know where to get started often I'll just go and do things that I think will get me more information just like go and plot stuff or like follow random things I'm curious about in a fairly undirected fuzzy way and this has actually been I think the mode of research has actually been most productive for me when or at least when I think about what feel like my biggest research insights it feels like it's been Downstream of this kind of exploratory [ __ ] around and find out mode the the first mode is like you haven't about this and you went to like um verify it the second is like you think you're wrong and you try to like find contract examples for why you're wrong no no I think I'm right but I'm insecure about it so I'm going to try to prove that I'm wrong instead yeah so I think that's like the there's something people often do when they're trying to like increase their confidence and something is like try to find culture example like find the best counter arguments um and the third one is like just like explore and gain more information plot new things yeah um you could paraphrase them as isn't it really obvious what's going on um oh man am I so sure about this and [ __ ] around and find out okay everyone and find out is there is there anything um that you think people like don't really understand about your method that like is like under appreciate or like surprising and um if people were to like watch like 20 hours of you doing things like we're like oh he actually spends that amount of time doing eggs um I think people underestimate how much this stuff can be hypothesis driven and how useful it is to have enough of an exposure to the problem and enough of an exposure to the literature of what you find inside models that you can form a hypotheses because I think that this is often just like pretty useful I want to like push back on this because Neil Nanda from other podcasts so I've listened to your podcast with uh like Tim's carve on ML Street talk and you say kind of the opposite where you say like oh you need to be like willing to be surprised you need to like don't have hypothesis so much and just like and you say that multiple times like you need to like be willing to be surprised so I'm kind of like feeling the Neil Nanda from like a few months ago I would disagree here so I think these are two simultaneously true statements it's incredibly important that you have the capacity to be surprised by what you find in models and it is often useful to go in with a hypothesis I think the reason it's useful to have a hypothesis is that it's just often really hard to get started and it's often really useful to have some grounding that like pushes you in a more productive Direction and like helps you get traction and momentum and then the second half is it's really important to then stop and be like wait a minute I'm really [ __ ] confused or wait I thought I was doing this but like actually I got the following discomfletory evidence um and you can kind of think of it as like strong beliefs weekly health like I think you should have and I think also being good enough that you can start to form a hypothesis like being at the point where you can sit down set a five minute timer and brainstorm what's going on and come up with like four different hypotheses it's just like a much much stronger research position so when you sit down and try to bring someone you come up with nothing yeah maybe having two hypotheses is the best one you want to like have multiple hypotheses in mind um you also want to be aware that probably both of them are wrong but you want to have enough engagement of the problem that you can generate experiment ideas maybe one way to phrase it is if you don't have any idea what's going on it's hard to notice what's surprising and often noticing what's surprising is like one of the most productive things you can do when doing research these take about like be willing to be surprises like from ml Street talk it's like a four hour podcast I highly recommend watching it um and I think I think there's like a few claims that you make in there that I think are interesting I don't want to go like all in because I think people should like listen to the ml free Talk podcast but I will just like prompt you with like what I think is like my summary of the takes and you can give me like the in the amount of completion of the prompt um I love being a language model it's good all the theory is bullshits without empirical evidence and it's overall dignified to make the mechanistic interpretability bad I consider those two different claims make two outputs uh yes so I don't know I think that it's like a strong Trend among people especially the kind of people who get drawn to alignment from very Theory based arguments to like go and it's just pure Theory craft and like play around with toy models and um form like beautiful elegant hypotheses about what happens in real models so that must be complete [ __ ] and there's like a kind of person that you'll write really detailed research proposals involving toy models but never has the step of like and then go and make sure that this is actually what's happening in the real language models we care about and I just think this is just like a really crucial mistake that people often make and real models are messy and ugly and cursed so I Vibe but also you can't just ignore the messy complicated thing that's the ultimate one we want to understand and I think this is a mistake people often make the second thing is that and no macintop seems hard and messy but like it seems kind of embarrassing how little we've tried it I'll just be so embarrassing if we make AGI and it kills effort and we could have been charged them we just like didn't try Hub and didn't know enough to like get to the point where we could look inside it and see the Press here to kill everyone second second prompt making terp is um like doing alien Neuroscience for True seeking biologists in a world of math I like that take I don't have anything about the sound that take well phrased I have stuff to say um so yeah the way I think about it it's like models have lots of structure there's all kinds of underlying principles that determine what make it Naturals but algorithms are natural to express if you're a language model and we just don't really know how these work and yeah um and there's lots of natural human intuitions for how the stuff works were we like think it should look like this and we think it should look like that like I did not expect that the way modular Edition was implemented in the model was with Fourier transforms and triggered seeds but it turns out that it is and if you and this is why I think it's really crucial that you can be surprised because if you go into this not knowing that you can not having the ability to notice wait this is just a completely different ontology to what I thought everything is cursed give up and go home there's something about the the world of math part where like all the like language models are doing things like from like matrix multiplication or like sometimes non-linearities but it's mostly like well understood right so in biology we have like um let's say a map of the territory and we're just like think about like cells and like atoms and everything but here we have like this very rigid structure that is kind of like giving birth to this like alien like neurons or like it it is like it's like math human Mass giving birth to like alien Neuroscience right yep yeah another good example here is this work I was involved in based on this fellow paper where they the headline result of the original paper was that they trained a model to predict the next move in this board game of fellow and found that the model learned to simulate the state of the board but like you gave it these chest notation style moves like black plays to sell C7 and then you could look inside the model on that token and see that it knew the state of the board for everything like it knew that this move had just taken the following pieces stuff like that and this was like a really popular exciting paper it was like an oral of iclaire people were really excited because it seemed to show that language models trained to predict the next token could learn like real models of the world and not just surface level statistics um but the plot twist of the paper that I found when I did some follow-up work was um so they'd found this weird results that linear probes didn't work but understanding what was happening inside the model uh linear probe is when you just look for a direction inside the model corresponding to say this seller's black or this seller's White and they'd had to train non-linear prints and this is weird because the way we normally think models think is that they represent things internally as directions in space so if the model has computed the state of the board it should be recoverable with linear Pro there should just be a Direction saying this cell is black or something and what I found is that the model does think in terms of directions but the it doesn't care about Michael weights it has about this has the same color as the current player well this has a different color from the current plan because the model was trained to play both black and white moves the game is symmetric and thus this is just a more useful structure for it and this is just like another cute example of alien neuroscience like from my perspective the way I would compute the ball is like each move I would like recursively update this like running stage if you're doing that obviously you think in terms of black or white each player moves and updates lastly a bit but this is just not actually how Transformers work because Transformers can't do recurrence they have to predict the hot they have to compute the entire board in parallel and the model is playing both black and white so from its perspective doing The Current player's color relative to that is like way more important and way more natural so just to go back to the linear probe thing for people like we don't know it's like training a classifier on the activations of the network and you're trying to uh see if you can like have a perfect classifier on the on the activations and and if you have this then you're like pretty sure uh like you found something right yes yeah so probing is this slightly conceptually cursed field of study and interpretability that's trying to answer questions about what a model knows um and the classic thing people are doing is they're trying to look for linguistic features of Interest inside the model like does it know that this is a verb or a noun or an adjective and the thing you can do is you can take an activation inside the model like the residual stream after layer 17 or something and you can just do a logistic regression or train a linear classifier or whatever thing you want see if you can extract the information about a noun verb or adjective and if you can the standard conclusion is yes the model has computed this the obvious problem is that we're just kind of sticking something on top of the model we're just like kind of inserting an approve in the middle and we have no guarantee that what the probe fines is actually used um it's a purely correlational technique and you can imagine if you take like a really dumb language model and then your probe is gpt3 I thought gp3 can figure out whether something's an adjective noun or a verb and thus your probe could just learn it itself you have no real guarantee this is what the model is doing and so the purpose of the pro um and so one of the core challenges you need to do is have a probe simple enough that it can't be doing computation on its own and it has to be telling you what the underlying models has learned and that's just kind of a hard problem but instead of having like a bunch of like a million linearities and like a bunch of layers just have like a the most simple like MLP with like not know non-linearity and just like a very simple um classifier yeah another claim is uh what are the like four kind of like main things you need to do to be in the right mindset to be like uh I'm making researcher yeah so I think there's a couple of things I said when I was on ML Street talk I don't remember any of them let's see if I can regenerate them so I think that it's really important to be ambitious to like actually believe that it's possible to genuinely understand the algorithms learned by the model that there is structure here and the destruction can be understood if we try hard enough I think that it's yeah I think it's really important to just like believe it's possible and I think that like much of the field of interpretability kind of fails because it's done by ml people you have this culture that you can't aim for understanding the understanding isn't possible but you need to just have lots of summary statistics and benchmarks and there isn't some like underlying ground truth that we could accessibly trade hardness so being ambitious is actually possible you can be ambitious and like actually understand what's going on yeah I think in some sense this is one Edge I have as someone who just doesn't have a machine learning background I think there's a bunch of ways that the standard cultural things that have really helped with success in ml like this focus on benchmarks there's focus on empiricism I like empiricism but there's focus on like make number go up and Achieve Sota on benchmarks just like is fundamentally the wrong mindset for doing good interpretability work um another point is being willing to favor depth over breadth like models are complicated uh I think people often I think often make people bounce off Mercantile is they hear about it and they're like oh but how do you know that the sting this algorithm be found in one model generalizes another model and I'm like I don't that's the entire point like there is a real ground truth to what different models have learned and it's possible that what one model has learned is not what another model is like my bets is that in general these algorithms are fairly Universal but like maybe not no one has checked it's all that hard and this is just like clearly a really important thing um that you just want to be able to take a model and find the truth of what that model has learned and I mean the steel amount of the standard critique is that people think it's just boring if every model has a different answer and like ah it's kind of a taste thing my guess is that in general models have the same answer but it's that like I am willing to take a specific model and go really deep into trying to understand how it works I think the the level at which you like um you say they're like kind of similar is more like in biology where all animals like a bunch of mammals have like hands and like like food or something but but if we don't have the same ends and so you expect like Transformers or um like structure or circuits instead of neural networks to have this kind of like similar structure but maybe like varying shape or like colors and those kind of things yeah similar to say the hands of mammals though I do expect things to get it's like it depends how you change it like if you just change random seed my guess is most things are going to be pretty consistent uh but with some Randomness especially for the kind of circuits the model like doesn't care about that much uh which we might get too late's work on a toy model of universality um and then there's like if you make a model 100x bigger I'll give it 100x more data how does that change what it learns and for that I'm like well I don't really know um some things will be consistent some things will change a final principle of doing good Mac interp work is I think it's really important actually no two phones I think it's really important to be truth seeking and skeptical to like really keep in mind models are complicated it's really easy to trick myself I need to try really hard to make sure that I am correct that like I've entertained also not hypotheses I tried to break my hypothesis I've run the right to baselines and I've like really figured for example a common mistake is people come up with some number and they're like I think that number is big and they don't have a baseline of like oh what if I randomly rotated this or like shuffled these or like randomly guessed and it turns out when you do that some of the time the number is boring they just don't really know they would do and the final principle that I think is incredibly important is to have a real intuition for models have like read papers like a mathematical framework for Transformer circuits and like stand in them be able to like sit down and map out on paper the kinds of algorithms that a Transformer might learn be able to like sit down carefully and try to think through like what's going on and be able to tell like this experimental method is principled or this experiment makes no sense because I'm training a probe in a way that makes this basis special but this basis is not privileged or be able to tell like wait um there's no way this would be possible because of the causal mask or the Transformer I don't know I'm like failing to come up with good examples of the cuff but there's just all kinds of things that make some methods just like laughably nonsense if you know what you're doing and I think that often people will write papers who don't have these intuitions and just kind of do garbage if I can plug there's this great new set of tutorials from color McDougall at Arena but macintop and I think they're just going through all of those and doing all the exercises and cutting things up is like getting you a favored on your ways of like developing these intuitions I also have a guide at neilnanta.io getting Dash started on how to get started in the fields I think either of these will put you in pretty good steps one last claim I think it's kind of interesting is like linear representations are somehow the right abstractions instead of neural networks yeah so okay so there's a bunch of jargon to unpack so the way I generally think about neural networks is that they are feature extractors they take some inputs like the Eiffel Towers in Paris and detect a bunch of properties like this is the word v and it is the source of the sentence or this is the tower token in Eiffel Tower this is a European Landmark this is in the city of Paris I am doing factual recall the sentence is in English this is a preposition the thing that should come next to the city a bunch of stuff like that and a lot of what models doing is doing this computation and producing these features and storing them internally and so a really important question you need to ask yourself if you want to interpret the model is how are these represented because internally models are just a sequence of vectors they have a bunch of layers that take vectors and produce more vectors by multiplying them with matrices and applying various kinds of creative non-linearities and so a thing you need to ask yourself is like how does this model work how do these vectors contain these features and the hypothesis that I think is most plausible is this idea called linear representation hypothesis which says that there's kind of a meaningful coordinate basis for your space a meaningful set of directions such that um the coordinates in this direction is one if it's the Eiffel Tower and zero otherwise the coordinate is one if it's in English zero otherwise it's one if it's V zero otherwise and the by looking for each of these different directions the model is capable of implementing a bunch of complex computation and one of the main reasons you might think this is intuitive is that the models are fundamentally made of linear algebra and if you're feeding something into a neuron basically the only thing you can do is project onto different directions and add them up and a thing that is sometimes true is that there are individual meaningful neurons in the model but a neuron is just a base assignment and so if a neuron is Meaningful like it fires when there's a cat it doesn't fire otherwise then the basis direction for that neuron is a meaningful direction that means cats and one of the main complications for this is this weird ass phenomena called superposition that I think we're going to get to at some point or possibly should say well to now who knows yes yes let's move on to this preposition yeah so I think the example I was giving earlier of the sentence the Eiffel Tower is in Paris it's probably a good example so we know that the model knows the Eiffel Tower is in Paris it's somehow able to look up Eiffel Tower and like get this information on Paris but Eiffel Tower is like a pretty Niche feature um I mean Eiffel Tower is not incredibly Niche but like models know all kinds of extremely Niche things like is for example is like solidly worth knowing but also kind of weird and Niche and 99.99 of the time because it's not going to come up and so it's kind of weird but a model to need to dedicate a neuron to eleazkowski if you want to know anything about him because this is just gonna allow you they're useless most of the time and empirically it seems like models just know more facts and then they have neurons and what we think is going on is that models have learned to use compression schemes like rather than having a dedicated neuron to represent the eleazkowski you could have like 15 neurons that all activate for eleazkowski and all boosts some like eleazkowski Direction a little bit and then each of these 50 neurons also activates for like 100 other people but they activate for a different set of other people so even though each neuron will now boost like 100 different people vectors whenever it activates it will boost the Eleazar the 50 eliaz neurons or activate an Eleazar and all constructively interfere on the LA Isa directory while destructively interfering in everything else and superposition is broadly this hypothesis the models can use compression schemes to represent more features than they have dimensions um exploiting this sparsity this facts the eleazkowski so it doesn't come up that often so it doesn't matter that each of these neurons is representing like a hundred different things because the neurons the 100 things they represent are never going to occur at the same time so you can get away with the neuron doing a bunch of different things at once and this is a really big deal because so a thing which seems to be kind of true in image models is that neurons were broadly meaningful like there would be a neuron that meant to call you or a car body or a cat or a golden retriever fur and things like that but what seems to happen in language models much more often is this phenomena of poly semanticity police monticity is like the model it's like um the neuron activates through a bunch of seemingly unrelated things like eleazkowski and Eric bomber and list variables in Python codes and this is really annoying because in order to do macintuff on a Model you need to be able to decompose it into bits you can actually understand and reason about individually but you just can't do that if the model is like messy you just can't do that if there aren't individually coherents but if models are using compression schemes like superposition neurons may not be the right units to reason about them instead maybe the right unit is some like linear combination of neurons and so one thing that people might be noticing is I've given this free menu it features its directions but then I'm also claiming that the model can fit in more features than it has dimensions and you like if you've got a thousand-dimensional space you can't have more than a thousand orthogonal directions but what seems to be going on is that models and facts use almost or final directions and you can fit in exponentially many directions that have like dot product point one with each other rather than zero even though you can only fit in linearly many things with zero dot products because High dimensional spaces are weird and there's just like a lot more room to squash stuff in and some of those things are sparse like most of these vectors are empty and don't occur on any given inputs the fact that they have non-trivial interference doesn't really matter there's two things to distinguish here there's the input weights and the outputs the input weights are like when the neuron activates and it's like what is it detecting and we can totally have a neuron that activates on eleazkowski and the Eiffel Tower and then there's the output weights which is like what features in the model does this boost in what we call the model's residual stream it's like accumulated knowledge of the input so far and what we generally find is there would be some like Elias yudkowski feature the some direction that gets boosted and some Eiffel Tower direction that gets boosted and the obvious problem is there's interference the model is now going to have non-zero information Direction and in the Eiffel Tower Direction but if it has like two yukowski neurons and two rifle Talon neurons and only one overlaps then when Elias yatkowski is there they'll get past two in the early as a Direction while the naval Towers area gets plus one in the eleoated direction and so it can tell them apart how I understand it like from from the explanation like the original stream is kind of like this uh skip connection and in resnet is like whenever you don't do um any extra computational images just like pass the kind of like output to some to like other neurons you skip a connection and and there's like a framing of like residual stream where you consider like these like skip connections to be like the main thing going on and the rest is like extra extra steps and so what you're saying is like basically like passing through the original stream is like you pass the information to like this like main rivers like this like main flow of information yeah yeah I think this is an important enough point it's worth a bit of attention to explain it so people invented the city of residual connections where in a standard neural network the way it works is that the input to layer n is the output of layer n minus one but um people have this idea of adding skip connections so now the input to layer n is the outputs of layer n minus one plus the input layer and minus one we let the input kind of skip round with an identity and this turns out to make models much better and the way people always draw it is with this like Central stack of layers with these like tiny psychologies but if you look at the norm of the vectors pass along it is actually the case that the like tiniest connection is actually like much bigger and the most circuits in the model in practice seem to often skip multiple lens and so the way I draw models is with like a big Central Channel called the residuals and with like tiny skips to the side you feel like each layer it is like an incremental update and the residual stream is this like Big Shed bandwidth the model is pursuing between legs but each layer is like reading from and writing to you as like an incremental object a couple of insights about student position the first is that this is just very much a frontier in the field of mechanistic intervention see right now like I expect like my understanding of superposition is a lot more advanced than it was six months ago I hope you'll be far more advanced six months from now than it is right now we're just quite confused about the geometry of how models represent things internally and I think this is like probably the big open problem in the fields better understanding this um one thing which like I've tried to emphasize but to make explicit is that a really important facts about language is sparsity this fact that most inputs are rare most features like Elias yadkowski or the Eiffel Tower are red because superposition is fundamentally a trade-off between being able to represent more things and being able to represent them without interference where Elias yadkowski and Eiffel Tower sharing in Euro means that if either one is there the model needs to both tell Elias yatkowski was there and also the Eiffel Tower is not there this was just interference and there's two kinds of interference does the interference you get when both things are present what they call simultaneous interference like Elias yutakowski is in the Eiffel Tower and there's alternating interference where eleazkowski is there but the Eiffel Towers dots or vice versa and if these are rare then basically all of the interference you get is alternating not simultaneous and language is just full of extremely rare features but tend not to occur at the same time um a paper that one of my mentees wears gunny works on called finding neurons in a haystack we try to look for empirical evidence of how models did to position and we found that one area with a use of the tongue with these detocolization neurons so or compound word detectors so the input to a model is the ideas like words or tokens like and but often words aren't the right unit of analysis the model won't attract compound words or it has a word that gets broken up into multiple tokens um like alpaca gets tokenized as like space Alp a c a and clearly you want to think of this as like a word and what we found is there are neurons that seem to do a Boolean and on like common sequences of tokens to like detonalize them to recognize them it's on things like prime factors or social security or blood pressure and an important property of these is that you can never get these occurring at the same time like it is literally impossible to be the pressure and blood pressure and the security and Social Security at the same time because a token just you just can't have different tokens at the same token it doesn't make any sense so what I'm saying the trivial statements that a token cannot both be the pressure and blood pressure and the security in Social Security and so when models want to do this algorithm of recognized sequences of tokens um they can never occur at the same time uh which means that this is like Prime real estate for super possession because it's just like I will never have simultaneous interference this is amazing I can just do lossless compression I can just have like a hundred Social Security neurons Each of which represents like another thousand compound words and it's so efficient I'm in love and in practice there seems to be what models do and I don't know I think this was like a really cool paper and where's did a fantastic job uh I also think it's like kind of embarrassing that this was like basically the first example of a real case study of superpositional language models and one thing I'm trying to work on at the moment is like getting more case studies of this because I think that one of the main leaders have progressed in the field of mechanistic interpretability is like good detailed case studies when I was like watching you walk through or like reading the the abstract there's something about only activating a certain part of the outputs and like masking the rest there's like some some like Factor k or something that that actually doesn't change yes so so the actual paper we're looking into this technique called sparse probing uh this is much more Wesley's influence than mine I'm much more interested in the case studies but so the idea of spouse proving is we used to think that individual neurons were the right unit of analysis but with superposition we now think that linear combinations of neurons so the right unit of analysis but our guess is that it's not the case that most neurons are used in most it's not the case that like every neuron is used to detect elegancekowski most neurons are off while some neurons are important here and so we ask ourselves the question um if we train a linear classifier to detect that eleazkowski is that um how sparse can it be but this time I mean a totally different notion of spasticity sorry for the notation confusion this time it's like how many neurons does it use and we were like okay it can use one neuron I find the neuron which is the most correlated with eleazkowski being there on not being and I see how good it predicted this is next I take the best pair of neurons and I use these well the best when twofold or deck two below here and you see how good you are detecting the thing the different numbers of neurons and you can use this to quantify how sparsely represented different things are there's like the more neurons you hire you have the more like accurate you become like how much you can have like a little bit of neuron and seal the ticket yeah it's so this turns out to be like quite a conceptually thoroughly thing because so let's take the Social Security example the model A Thing models are very good at is just storing in the residual stream information about the current token or recent tokens like it's a very very easy trainer Pro it says the current token is security or the previous token is social and if you just train a probe to detect Social Security the like easiest way for this to work is that it just like um the easiest way for this to work is that it just like detects current token is security that's a direction previous token is social that's a direction there's some of these two that's a Direction so in some sense you're like having those um like all these like duration for like each individual token um being in the right order and and the mix of them will be like the the entire like group like the linear combination that we're talking before yes and Blake this is boring like this is not the model detecting the compound word Social Security this is just a mathematical statement about detecting linear combinations of tokens um but models do not um and but in order to detect the multi-token phrase the model is going to intentionally have certain neurons that are activated for Social Security and which don't normally activate which they're not going to have to say social lap or social Johnson or something some like nonsense combination of words and a test you can do uh which would generally make it into the paper but probably should have is show that if you just want to detect known combinations of words it's a lot easier to do this than like random unknown combinations of words however do you let the model use every neuron it could detect random combinations of words very easily because of this current token is security previous token is social phenomena but because it's not intentionally specializing neurons for it it's much harder to train a sparse proof um it's also another thing about like the other sparsity where they do like these experiments where they like make um some features like it is as we're saying like more sports or less Parts I think maybe that's something you can explain it yeah so okay so the first thing to be clear about is there's actually two different kinds of superposition what I call computational and representational superposition so representational is when the model takes features it's computed in some high dimensional space and compresses them to a load management space in a way that they can be later recovered so for example models have a vocabulary of 50 000 tokens the residual stream is normally about a thousand um so you need to compress fifty thousand directions into a thousand dimensional space which are just like a pretty big lift like you need to do a lot of compression for this to work but you're not doing anything new your goal is just lose as little information as possible find some encoding that's just convenient and thoughtful and works and then computational is when you want to compute some new features but you know the current token Security in the previous token is social and you want to create a new feature that says they are social security that is their combination I can start thinking about welfare and government programs and politics and all of that stuff which is very dangerous um apologies I'll try not to give you cancer and uh no it's very dangerous if if models start to like understand all these things if they start like understanding politics and like every like very abstract Concepts that means that we're getting like close to like human level like I don't understand politics it just knows that if Social Security is there Trump and Obama and bison are more likely tokens to come next that's the politics feature it's a very boring feature I haven't actually checked if this exists but I'm like sure that exists So when you say computational future it means that they're like it is doing this to save computation um no what I mean is this algorithm learned it like it is useful for Downstream computation to know I am talking about social security right now and not say social media because both have the token social they're very different things that are very different concepts which have very different implications what should come in and the thing we looked at in finding neurons in a haystack is computational superposition uh we looked into how the model computes features like social social security detection we also looked into a bunch of other things like we found individual neurons that seem to detect French like this Texas and French or individual neurons that seems to um be detecting things like this is the end of a sentence and stuff like that uh or detecting facts the people that you were asking me about uh Toy models of super possession it's like really really good anthropic paper which is probably one of my all-time favorite paints they were mostly looking at representational superposition so the point of this paper was just we're going to look into toy models because we think we want to understand why neurons are polysomantic in a real model and we don't know why we're kind of confused about this um and um yeah we're kind of confused about this and the yeah um and um we think it's because they're doing super possession but no one's actually seen them doing a superposition can we build any setting at all but superposition is actually useful and use this to study as properties and honestly I would have probably predicted this would just not work because it's too divorced from Real Models and I do in fact think that using toy models cost them a lot in this area but they also just got so much done and like so many insights that like I was wrong and this was a great paper um so you know boots across better research than me um and so okay so what's going on here so what they found was they had this setup where they had an auto encoder it has a bunch of features as inputs that would like you each feature was like uniform between zero and one um but it was also off most of the time it was normally set to zero otherwise uniform between zero and one it had like 20 of these and then they made it compress it into a small dimensional bottleneck like five Dimensions linearly have a linear map back up and then they gave it a row on the end to do some clean and this is like an ideal setting to test for representational superposition because they trained it to see how well it could recover its input from its outputs while compressing it and decompressing it in this low dimensional bottleneck in the middle and they found all kinds of Wild results um notably they found that it could learn to use superposition and they would often learn these beautiful geometric configurations where there would be say it would learn to compress nine features to Six Dimensions that would spontaneously form three orthogonal subspaces of size 2 each Each of which contains three features that are compressed as an equilateral triangle or um it would have five dimensions and two of them would be an equilateral triangle where each feature gets two-thirds of a dimension and the other three would be a tetrahedron where there's three quarters of a dimension each and I personally would bet most of this doesn't happen to Real Models and it's just like too cute by half but it's also just like really cool and one Insight they found that I think does generalize is they found that it is a very the sparsity of these features how often they're zero like how rare the future is the model becomes a lot more willing to use superposition and the reason the reason this is like an intuitive thing is what I was saying earlier about alternating versus simultaneous interference if two if a dimension contains two features the two features a lot of fog law to each other then if both of them it's now quite hard to figure out what's going on because it looks like each feature is there on its own really strongly and models are kind of bad at dealing with this [ __ ] um and if exactly one of them is there then in the correct feature Direction it's big while in the incorrect feature Direction which is not orthogonal but it's also not the same it's small and the model can deal with that kind of stuff the use of the running one as output to clean up and so what's going on is that the model as you change the probability each thing is non-zero because it's sparse um because they're independent the probability that one is up there the other one isn't is 2p minus P Squared and the probability that both are there is P Squared so it was p is when p is Tiny the probability that both there is order P Squared well the probability one of them is there is order p and as P gets tiny P Squared gets smaller much faster than P does so the cost of simultaneous interference gets tricky right so the the cost of interference is is very small because of like this quadratic cost and like P is like smaller than one um and when something is like very close to zero so something like it because he appearing like one in a billion times it is like trivial to make like the neuron detect both suitcase key and and Neil man that because they've never had happen at the same time or at least maybe in this podcast maybe they happen all the time together is actually a pretty bad like any example I can think of is day factor a bad example but you can imagine like I don't know uh some niche some contestant on MasterChef uh season 17 in eleazkowski probably never gonna co-occur apart from literally that sentence and and so when um when the model has like a bunch of features to take into account and they're like all kind of aware it forms this like beautiful um like a geometric structures that are like not completely or trigonal but like more like um um you said like some like platonic shapes like yeah tetrahedron us Square anti-prisims where you have eight in three dimensions which is really cute I mean criticism is that this happens in most like toy models right a few a few layers MLPs or Transformers I don't know much taller than that just like linear map it's a small Dimension lay them up to Big Dimension single layer of rounds that is the model they studied and so the goal of your paper was was to have something it's like a different kind of reproducement but like in a bigger model right so like test it on like Real Models yeah I think I probably just like close by reiterating but I think this is just probably the biggest Frontier in macintuff right now we just don't know how superposition works and it's kind of embarrassing and it'll be pretty good if we understand it way better and I would love people to go do things like go and build all the work we did and the neurons in a haystack paper go and try to understand what superposition looks like in practice like can you erase the model's memory over the word Social Security how many neurons you have to delete to do that I don't know yeah and I think it's like a right moment to like maybe talk about doing research and preposition like as a whole because I think today you can like do maybe like work on your own work within Tropic or work with neonenda as as uh is like a new opportunity is like this like um Terry mats Syrian Mass color uh you can be one of them mentees that you have I think like yeah right now maybe like explain quickly what's like starting Madison why do you have seven people you Mentor thank you very much um yeah um because yeah it was I think Wes I met I met West working on this paper um in December on like another batch of of people working on with you and I think right now um I've met other people that you work with right now uh as part of another batch I think our church was in a new paper and I also talked to Bilal about a paper presented icml I think like one of the first like story math paper that is presented at some conferences that he also done with you as well so yeah maybe maybe talk about Sarah mass as a whole and how you do is yeah with your mentees actually clarify it all this paper was nothing to do with me and I can claim no credit but he is currently one of my math Scholars and I can totally clean credit for people we're about to put on it's gonna be great better than his previous one because it has a meal on it this time um but yeah so sorry Matt says this organization who were like hmm it's all seems like there's a ton of really talented people who want to do alignment work and a bunch of alignment people who like words and Mentor people if someone made them uh or someone was like here are 10 smart people to go mentor but well this isn't happening on its own uh I think Evan hubinger vixel Warlock and Oliver Zhang were some of the main people who like tried to make this happen initially with Evan as the original mental and Evan is a machine he was like yeah I can mentor like seven people this is fine I'll start my Fridays on it this is Chill and just like get shut down and this is now like one of the biggest programs for like alignment internships I think out there and to be clear for for people like we don't know what stream match stand for is like there's like Stanford Express organization called sarri then foreign yeah something like that and I know internal isn't really the right frame for it it's more like you'll go and do independent research under the guidance of a mentor and like the my system is I'm like a fairly but not incredibly a hands-off mentor I'm like excited about and invested in the researcher scholar produces and like have check-ins with them once a week and like generally try to be engaged in their projects like if they're blocked I try to help him get unblocked I try to help provide like concrete experiment ideas and motivation and like some amounts of like guidance and just try to make it less of a horrifying starcrafting experience than like doing independent research and one thing I've just been really pleasantly surprised by is how many great people that are out there who want to do macintuff research and how time efficient mentoring is where I don't know it just feels like great research happens with like two hours a week um for me per project and there's like a bunch of really competent people who will just mostly execute autonomously yes I'm actually adding significant value by providing like guidance and an outside perspective and mentorship and like connections and I think it's just like a really cool thing maths facilitates like having a PhD supervisor that like actually cares about you and is actually fun and actually like he's interested in your work I like to think that I am better than the average PhD of supervisor it's a low bar so I feel like I probably meet this but yeah one thing I didn't really expect to go into this is I think I've just been like really good for my career to do a lot of mentoring because I'm just letting really useful skills of like how to advise research how to lead a team how to generate a bunch of ideas how to like help other people be more effective rather than just doing this myself and one thing I'm currently trying to figure out is like taking more of a leading role on the deepmind mechanistic interpretability team and I think I'm just in a much much better position from having spent like the past I don't know coming on a year like doing a bunch of mentoring my spare time and also good papers happen it's just such a good deal I don't know why more people don't do it so I also have the hypothesis that I'm just like really extroversed in a way that gives me the superpower of I can just casually have like nine mentees in the evenings and just like chill um geek out about cool projects happening I think there's a superpower of like you you gain energy by talking to people right and so you enjoy you you're recharge into eating and some people have told me that like compared to like other other people or other mentors you can just like have these like one hour call with Neil Nanda at the end of the other day and he's like becomes like two hours because they talk like you you kind of enjoy doing this you're not even like it's not like a time commitment you just like actually enjoy helping people um one person that I think is kind of a good example of this is below also the the paper we've talked about um I think it's like toy models of universality and first certain Mass paper I wanted HTML in a why and and I record Bill alden's presentation and there was like the only logo that was on the paper was like Siri mats and then there was like Neil Nanda and uh Bilal that was like I think independent at the time uh maybe some other people I think he might have put UC Berkeley because he used to be a chai PhD student but it's now at ARK evals and used to be at Redwood and does all kinds of random shirts and apparently I guess like this idea for this paper came from like a SF already if I remember what you said on yeah on others so was it like main idea here um so the backstory of the paper is um so there was this paper called unblocking um this weird-ass phenomena where you train some tiny models on an algorithmic task like modular audition and you find that it initially just memorizes the training data but then if you keep training it for a really long time on the same data it will abruptly generalize or grock and go from can't do the task to candles and it's a very cool weird and this was like a really popular paper because people just like what the [ __ ] why does this happen like we know that things can memorize we know that they can generalize but normally it just does one and sticks there it doesn't switch what's going on and there was this great story that the reason they found it is they trained the model to do it it failed and then they just like left it training over the weekend and when they got back it had like figured it out and uh I don't know if this is true it's a great story so I sure hope it's true and one of them and the like the paper I discussed earlier about modular Edition progress measures for grocking via mechanistic interpretability the seat of this was I saw the paper and I was like these are tiny models on clean algorithm tasks if there was ever a mystery that someone made to be macintuffed it was this one and the algorithm I found generalized the fair bit it covers like modular subtraction and multiplication and division which are some of the other tasks in the paper but there was this whole other family of tasks about compositional permutations of sets of five elements composition the group S5 and this is completely different and I had no idea how this happened and I was at a party and I raised this to some people around me as a puzzle and two people there uh sandbox and Joe Benson were like entrusted um they first looked at the idea that representation theory was properly involved which is this branch of like 20th 19th 20th century mathematics about understanding groups in terms of understanding how they correspond to sets of linear Transformations and Vector spaces and uh after the party Sam actually sent me a less wrong message with like the first draft of the algorithm we ended up concluding the model learned though this has been some like further research that suggest me that algorithm would have been completely wrong I really know what's up with that I really don't allowed to go figure out whether this is legit and tell me about it because I haven't got around to actually reading to like claim to rebuttal yet but kind of embarrassing if we just like significantly misunderstood the algorithm science people falsify us progress Marches On um and so uh yeah all groups have these things called representations and um for example uh for the permutation group on five elements you can take the four-dimensional tetrahedron which has five vertices and any linear map that Maps the tetrahedral to itself like rotations and Reflections commutes the vertices and there's actually an exact correspondence to any permutation there's some linear map that does it and vice versa so you can actually think about the linear map you can actually think about group composition on these permutations of five elements as being linear transformations on this four-dimensional tetrahedron which is a 4x4 Matrix and what we seem to find is that the model would internally like represent these matrices so this is kind of awkward to talk about because apparently there was a fairly compelling rebuttal that I haven't engaged with yet so maybe we didn't show this who knows interpretability is hard man yeah I'm I'm sorry if if this is wrong and and we like expose you on a podcast about it but I guess like the the main idea is like somehow you can like map things between like how um like different groups in mathematics you can do like um like isomorphisms or those kind of things between like permutation and five elements or like the narrow um like um in their map between the different teacher head run and and this you kind of like found like an acute or like a nice way of looking at this and and this Maps exactly to modular Edition right yes with much of the rotation uh the representations are rotations of an inside shape where you can adding five mod seven is equivalent to rotating the seven-sided Thing by five times a seventh of a full term I'm kind of like curious about like the thing we discussed before with um like the Sinners and casinos and like all the like like mathematics that you decompose like what the model was doing is it's so it is the model doing some kind of computation it is like similar to like questions and centers and at the same time has this like different mapping with like the um group of implementation as well at the same time no so the Sines and cosines are the group representation um in the case of modular audition the group representation is rotations of an inside shape which is the same as rotations of the unit circle and the way you represent a rotation is with Sines and cosines and it turns out that the algorithm I found about composing rotations was actually an algorithm about composing group representations that just happens to also have this form of Fourier transforms and Trigon entities in the like simple case of modular audition which is like in some sense the simplest possible group right so this paper is most like a a more General framing than the like actual um decomposition into consensus and silence yes yeah we found a generalization of the algorithm which we thought we showed a little like maybe you were wrong who remembers um and and there's something else you say about this which is I think it's interesting is like we we tend to think about like oh the model needs to like learn sinus and cosines on like very like complex functions but actually it only needs to like learn the correct answer on a finite number of token inputs yeah so the model is so the input the model receives is just two inputs a plus b where each of them is a integer between 0 and 113 because I did addition mod 130 because that was a random number I picked that was Prime because Prime tonight and um this and so they need to know what sine of a times some frequency is but because a can only take on 113 values it only needs to memorize 113 values of the sine function and this is very easy models literally have a thing called the embedding that is just a lookup table for people who are familiar with the idea of one hot encodings um the ADR is like it's one I had encoded the multiplied by a matrix which is equivalent to a lookup table and it's not that like it knows what sign of like 0.13 is and that's different from sine of 0.14 it just knows its value on the 113 possible input it sees for a and the 113 possible inputs is used for B because it just cannot ever see anything else there's like an embedding was like 130 values and then uh whenever it needs to like look at this value it can just do like a one hot dot product or something like this yeah exactly and this is all baked into the model one way to think about it is that there is some underlying ground truth of like the represent of like the sine wave that is the region where the model performs the algorithm properly and we just give it at 113 data points and said smush them to be on this underlying sine wave um and the model does a bunch of trial and error until it's like okay this is like about the right point 113 times it's like learn to do the real valued computational for like actual wave which is a much harder task and people often are just like oh my God it launched it learned how to do trick and it's like no it memorized 113 numbers not that hard man to some extent is doing less reasoning than anything is it it's just like doing literally like interpolation on design curve or something yeah it's like one layer model like the impressive thing is that it realizes the sine curve is useful how did the capacity to learn it like they can learn arbitrary functions it's a lookup table it can do anything um but yeah and the like actual Narrative of girls paper the reason we called it a toy model of universality is so one really interesting thing happens once you have these representations because the representations you get um there's actually multiple equipment for each group that are qualitatively different like um you can get uh rotations of different frequencies which are just fundamentally different from each other no that's a terrible example uh so with uh the permutation group there's this like linear transformations of the four-dimensional tetrahedron but there's also a bunch of other stuff like I think there's a transformation of the main diagonals of a dodecahedron or an icosahedron for example and that's just like a totally different group and this is uh oh that might be also I don't know man geometries are weird um and these are just qualitatively different algorithms the model can learn and so an interesting question is which one does it learn and there's this hypothesis called universality vert says that there are like underlying true algorithms models learn that like or they will systematically converge and what we found here is that there's actually a ton of randomness like is he just very the random seed the model will learn it it will learn multiple of these representations but it will also learn different representations each time and this is just kind of weird um and uh we you would have guessed that it would learn the simple ones and naively you would think like oh uh things which are three-dimensional shapes like easiest to learn than links that are four-dimensional shapes so like obviously it will learn the 3D ones rather than the 40 ones and what we find is there's like a little bit of a bias towards lower dimensional stuff but it's like doesn't really correspond to our human intuitions and it's like kind of weird and we don't really we don't really have a good story for why but the importantly it's both not uniform like it prefers to learn certain representations than others but it's also not deterministic and this is like I don't know I don't think anyone actually believes this but there was this strong universality hypothesis that like models would always learn exactly the same algorithms and I think we've just clearly disproven this at least with these small models though the like exciting thing is that there's a finite set of algorithms that can be learned and you can imagine in real life like without having access to this Grand truth learning some like Periodic Table of algorithms where you like go and understand how a model learns something interpret five different versions of five different random scenes and learn like a complete set of all algorithms the model could learn so basically the model has like different algorithms that you can learn and it's not it doesn't learn always the same but there's like a set of things you can learn that is like able to learn uh is a hypothesis you could have we looked at the toy model this is like not that much evidence I think it'd be pretty cool if someone went and like did this properly the the Beloved paper about um the S5 group is is part of like the kind of the grucking research you've done and one paper I think is like the most one of the most famous favorite you've done is progress measures for gracking the mechanistic interpreality it also has a walkthrough on you on your on your channel yes highly recommend it's a great book for people who don't have four hours to to listen to it like there are people who don't have four hours what kind of viewership do you have man um yes I should clarify because Bella will kill me because paper was not just on S5 his paper was a grand ambitious paper about many groups which S5 was one particularly photogenic example but yes so what happens in this progress measures paper so this is the one where I reverse engineered modular audition which I've already somewhat discussed we found that you could just actually reverse engineer the algorithm the model Hood lens and it had learned to think about modular Edition in terms of rotations around the unit circle and in my opinion the like story of this paper and the reason it was a big deal or that I'm proud of it is sorry those are two different things the reason I'm proud of the paper is that lots of people think that real interpretability is [ __ ] they're just like ah you can't understand things it's all a discretible black box you have hubris for trying and should give up and go home and I don't know whatever random trap people talk about nowadays I try to start listening to the haters and um the thing neolanda August second don't listen to the haters um I'm per listening to good criticism of specific interpretability work to be clear uh criticism is great and also lots of interpretability work which is kind of bad so that I'm pretty in favor of uh but yeah um it's kind of like um I think I just like very rigorously reverse engineers and non-trivial algorithm I went in not knowing what the algorithm would be but I figured it out by messing around with the model splits and I think that is just like a really cool results that I'm really glad I did that I think is just a good proof of concept that the ambitious macintuff agenda is like even remotely possible the second thing was trying to use this understanding to explain why grocking happened and so as a reminder rocking is this phenomena where the model initially memorizes the training data and generalizes terribly but then when you keep training it on the same day sure again and again it abruptly generalizes and what I found is that grocking was actually like um an illusion it's not that the model like suddenly generalizes brocking actually splits into three to three phases that we call memorization circuit formation and cleanup in the first phase the model memorizes what it says on the 10.

but in the but then there's this this is not going to transfer well over audio whatever if you look at the grogging loss curve it's like train loss goes down and then stays down while test loss like uh is kind of goes up a bit it's worse than random and it remains up for a while and it's during the ceiling Plateau that I call circuit formation where it turns out the model is actually transitioning from the memorizing solution to the generalizing solution somehow keeping trade performance fixed throughout and it's kind of wild the models can do this the reason it does this um I don't claim this is like fully rigorously shared in the paper this is just my guess is there something weird going on where it's easier to get to the region of the Lost landscape where the model is doing the right thing um but it's it's easier to get to the region of the Lost landscape where the model is like doing the thing uh doing the thing by memorization the generalization so but training the model with weight Decay which creates an incentive to be simpler which creates an incentive to like do it yeah just like do it more simply and this means that the model is like um sorry this means that the model is like initially he starts memorizing because it's easier to get to but it wants to be generalizing and it turns out it is possible for it to transition between the two while preserving test performance which is kind of surprising every but like in hindsight it's not that crazy and then so why does Tesla's crash rather than going down gradually so this is the third stage called Cleve so during circuit formation the model is still mostly memorizing a memorization generalizes really badly out of distribution which means that the model just performs terribly on the Unseen data and it's only when it's got so good at generalizing there are no longer needs of the parameters that spending on memorizing that it can do cleanup and get rid of the parameters that spending memorizing and it's really when it's done that the model like performs better it's like actually able to perform well and later it hasn't seen yet which is this like sudden grocking crash or Spike and this is not sudden generalization it's gradual generalization followed by sudden cleaning do you have any evidence for this like um like circuit formation that happens gradually like have you have you tried to like look at the circuits and see like if they if they could like serve like simpler simpler tasks yeah so this is the point of our paper uh the book is compelling metric is what I call uh excluded loss where we um this is a special metric we designed using our understanding of the circuit where we delete the model's ability to use the rotation-based algorithm but we keep everything else the same and what we find is that early on in training excluded loss is like perfect it's about as good as trading was But as time goes on during circuit formation the excluded loss diverges until it's worse than random even though trim loss is like extremely good the whole way and so for people like want to like work with you on on like certain maths projects or collaborate and research or um want to learn more about making sure you have like any general direction you would like recommend people going through yeah so I have this blog post called getting started in mechanistic interpretability you can find it at Neil Nanda dot IO slash getting Dash started that's basically just a concrete guide on how to get started in the fields uh much of what I have people do during the first month of sorry mats is just going through that blog post and I think you can just get started now like there's a lot of pretty great resources on the internet at this point on how to get into mechanistic interpretability a little bit just because I was annoyed at how bad the resources were so I decided to make good ones but I think I succeeded you are welcome so send me emails complaining about how much my resources suck and how I shouldn't be false after physical podcasts and yeah I don't know how much longer I'm gonna continue having slack on the side of my job to like take on that Scholars I'm hoping to get at least another round of Matt scholarsen which I guess would be I know um they do like about two cohorts a year um I don't know exactly what the next one's going to be but just like pay attention for whatever maths makes advertisers um and yeah I guess like for your more like YouTube work because this is probably going to be on YouTube uh generally like video or like intro to recommend people watching or or like after this podcast watch where they start their binge on yeah so I think probably the most unique content I have on my channel is my research walkthroughs where I just like record myself doing research and upload it and I think I don't know I'm like very satisfied with this format I feel like it just works well it's kind of fun and motivating for me um and you just don't really normally see how the sausage gets made um like you see papers which are this like polished albeits often kind of garbage final products that's like um here is the like ends thing of the research but if you're getting into the fields the actual skill is how to do it and I think watching me in the decisions I make is like educational I've got pretty good feedback on them uh probably these are like yeah my second most popular video is like what also my second ever video all gone downhill since there man um which is just like a recording of myself doing that I as I mentioned have like 16 hours of additional recordings that I did my math Scholars that I'll be uploading over the next few weeks and there's a long Marathon one about looking into how gpdj loves you arithmetic that's a six billion premise language model yeah I'm I'm really excited to have this like GPT well through um as I think I mean like Daniel Fallon's uh house so I thought it was his Daniel felon question um the Daniel final question is what is a great question I haven't asked you yet uh or I forgot to ask him let's see when you haven't at all asked me why does my guitar matter from a lemon perspective pictures maybe as she might ask it and you have like a a short answer uh so I kind of want to give the counter-cultural answer of like I don't know man theories of change back chaining it's all really overrated you should just like do good science and assume good things will happen uh which I feel like is an underrated perspective at alignments uh so okay so my actual answer is like here's a bunch of different theories of change from like for interpretability at a very high level I don't know man we're trying to make killer black boxes so we don't understand they're going to take over the world it sure seems like if they weren't black boxes I'd feel better about this and it seems like one of the biggest advantages we have over like AI is from a stage of coup is we can just look inside their heads and just see what they're doing and be like the evil neuron is activating deactivate 11 salt and I don't actually think the world's gonna work but it just seems like if we could understand these systems it would be so much better um some specific angles that I think interpretability seems particularly important uh one I'm really excited about is auditing systems for deception so fundamentally alignments is a set of claims about the internal algorithms done implemented by a model like you need to be able to distinguish an aligned model that is doing the right thing for a model that is just learned to tell you what you want to hear but a visually capable model has an instrumental incentive to tell you what you want to hear in a way that looks exactly the same as an alignment and the only difference is about the internal algorithm so it seems to me like there will eventually be a point where the only way to tell if a system has learned a lot is by actually going and interpreting it and trying to understand what's going on inside that's like probably the angle I most version and the world where I most like man if we don't have in touch which is kind of screwed um there's a bunch of other angles um but like the deception angle I've had Colleen Burns uh in December on is like uh contrastive search and this deception work um not the sensitive word but like trying to detect deception and and basically by saying like oh man if if we had those like linear probes or whatever where whenever the model is lying uh it will say like oh I am I am lying did like I'm I'm trying to deceive you or something um or or or like the model is like fully honest about him being live like you can ask him like Hey are you lying right now you say like yes I am lying um like you're pretty bullish on like things going well at this point yeah and I I would put Colin's work in a fairly different category uh I kind of see macintuck in the really ambitious big if true category where we're pursuing this like really ambitious bet but it is possible to like really understand the system and like this is [ __ ] difficult and we're not very good at it and it might be completely impossible but we might need to settle for like much less compelling much weirder and jankier shits um and I put Colin's work in the category of like I don't know kind of dumb [ __ ] that you try because it'll be really embarrassing if this works you didn't do it uh where like you don't have this like really detailed story of how the Lydia probe you work you train like works or like how it tracks the thing that you're trying to track it's very far from foolproof but it like seems to genuinely tell you something and it seems better than nothing and it's like extremely easy and scalable and like effect efficient and to me these are just like conceptually quite different approaches for research I'm not trying to put a value judgment but I think this is a useful mental frame for a viewers to have one is big if it's true is like if it is on everything then we're like we're pretty much uh saved and one is like pretty useful and and easy to implement right now but maybe it has some some problems yeah I mean Magnum has problems it has so many problems like we suck at it and we don't know if it'll work but um yeah it's like aiming for like Rich detailed mechanistic understanding rather than is something which is kind of useful but I don't quite know how to interpret it I think I think we can say this about about the podcast as well seems quite useful but I'm not sure how to interpret it yeah I I don't have I don't have much more to say uh if you have any last message for the audience you can you can go for it but other thing I think it was great to have you yeah I think probably had what I like try to finish off the full song like why macintuck Matters from my perspective in the I think one of the big things to keep in mind is just there's so many ways that this stuff matters like you could build mechanistic interpretability tools to give to your human feedback riddance so they can give models feedback on whether it did the right thing for the right reasons or for the wrong reasons you could um create demos of misalignments if we're in a world and it's really hard it seems really useful to have scary demos misalignment we can show policy makers and other labs to be like this thing looks like but it's not this could be you be careful kids and don't do drugs and stuff like that um and it seems like um we it seems pretty useful for things like understanding whether a system has situational awareness um or like other kinds of alignment relevant capabilities um and I don't know for lots of these there's a lot you don't need like full ambitious mechanism and it's entirely plausible to me that if any given one of these was your true priority you would like not prioritize this like [ __ ] blue skies macintuff research but I think that it's like worth shots it seems like we're having traction I also think it's really hard we might just completely fail and anyone who's counting enough to solve it should like please not man but like like and no day to day I generally think about it as it would be such a big deal if we solved this I'm gonna focus more on the down-to-earth scientific problems of like superposition and causal interventions and how to do principal science here and accept that like things kind of suck um and except that like I didn't necessarily need to be thinking super hard about a detailed theory of change because getting better at understanding the killer black boxes just saw seems useful but for me this sounds like a a great conclusion I think like if we sort of make the ambitious version of making terrap we can like go to governments and like show exactly what happens we can like you know pin down exactly what we need to know to like align these models I think it's like a a very important step at least it would be so great if we solve the ambitious version of Macintosh like it's just feel so happy such a win if you want to make neolanda happy uh solve make a nerve um go check out his YouTube channel uh check out his exercise check out his papers uh figure out if the below paper is true or not yeah thank you yeah thank you very much it was great big home and yeah if people want to go check out one thing you should check out my guide on how to get started Neil nanta.io get slash getting Dash does it to get started check out get started it's 12 minutes man